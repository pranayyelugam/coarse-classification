id,sentence,coarse,fine,pol,asp
0,Summary.,Structuring,Structuring.Heading,,
1,The authors empirically investigate the influence of the architecture and the capacity of an NN-model on the transferability of adversarial examples.,Structuring,Structuring.Summary,,
2,They also study the influence of the smoothness.,Structuring,Structuring.Summary,,
3,"From the obtained results, they propose the smoothed gradient attack showing improvements on the transferability of adversarial examples.",Structuring,Structuring.Summary,,
4,Pros.,Structuring,Structuring.Heading,,
5,* Robustness of neural nets is a challenging problem of interest for ICLR,Evaluative,Evaluative,P-Positive,Motivation/Impact
6,* The paper is well written,Evaluative,Evaluative,P-Positive,Clarity
7,* The experimental study is convincing,Evaluative,Evaluative,P-Positive,Substance
8,* The experimental results for the smoothed gradient attacks are promising,Evaluative,Evaluative,P-Positive,Substance
9,Cons.,Structuring,Structuring.Heading,,
10,* The results of the experimental study are somehow expected,Evaluative,Evaluative,N-Negative,Substance
11,* the idea of smoothing gradients is not new,Evaluative,Evaluative,N-Negative,Originality
12,Evaluation.,Structuring,Structuring.Heading,,
13,The experimental study of the transferability of adversarial examples is well designed.,Evaluative,Evaluative,P-Positive,Substance
14,Experimental protocol is convincing.,Evaluative,Evaluative,P-Positive,Substance
15,The smoothed gradient attacks improve many previously proposed attacks.,Evaluative,Evaluative,P-Positive,Meaningful Comparison
16,"Therefore, my opinion is rather positive. But, as a non expert in the field, I am not completely convinced by the novelty of the approach.",Evaluative,Evaluative,N-Negative,Originality
17,Some details.,Structuring,Structuring.Heading,,
18,"Typos: That l8 abstract; systems l9 intro; and l2 related work; directly evaluation l2 Section4, must has l-10 p4;",Request,Request.Typo,N-Negative,Substance
19,* the choice \sigma = 15 in Section 6.2 should be justified by the following study,Request,Request.Clarification,N-Negative,Substance
20,* \sigma is not given in Figure 3(a),Request,Request.Explanation,N-Negative,Substance
21,The reinforcement learning tasks with sparse rewards are very important and challenging.,Fact,Fact,,
22,The main idea of this work is to encourage intra-life novelty.,Fact,Fact,,
23,The authors introduce the curiosity grid and the intrinsic reward term so that the agent can explore toward unvisited states at every episode.,Fact,Fact,,
24,"However, the results are not enough to be accepted to ICLR having a very high standard.",Evaluative,Evaluative,N-Negative,Substance
25,"In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
26,There are some RL algorithms reported to be better than A2C.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
27,"For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
28,=================================================================================================,,,,
29,I've read the rebuttal. I updated my score but still not vote for accept.,Structuring,Structuring.Heading,,
30,This paper is not my main research area.,Other,Other,,
31,"Very unfortunately, this paper was assigned to me.",Other,Other,,
32,The main issue of this paper is the fair comparisons with other works.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
33,"However, I don't have enough knowledges to judge this point.",Other,Other,,
34,So please assess this paper with other reviewers comments.,Other,Other,,
35,Summary:,Structuring,Structuring.Heading,,
36,This paper introduces a generative model for 3D point clouds.,Structuring,Structuring.Summary,,
37,"Authors aim at theoretically showing the difficulties of using existing generative models to learn distributions of point clouds, and propose a variant that supposedly solves the issues.",Structuring,Structuring.Summary,,
38,Pros:,Structuring,Structuring.Heading,,
39,+ The problem of designing generative models for 3D data is important.,Evaluative,Evaluative,P-Positive,Motivation/Impact
40,Cons:,Structuring,Structuring.Heading,,
41,"- Paper is often hard to follow, and contains a significant number of typos.",Evaluative,Evaluative,N-Negative,Clarity
42,"- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?",Request,Request.Explanation,N-Negative,Soundness/Correctness
43,"As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.",Evaluative,Evaluative,N-Negative,Clarity
44,"- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
45,- It is not clear why authors did not follow the evaluation protocol of [Achlioptas’17] or [Wu’16] more closely.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
46,"In particular, evaluation for the classification task should be compatible with the proposed model, which would give a much better picture of the learned representations.",Fact,Fact,,
47,This paper proposes use of intra-life coverage (an agent must visit all locations within each episode) for effective exploration in Atari games.,Structuring,Structuring.Summary,,
48,This is in contrast of approaches that use inter-life coverage or curiosity metrics to incentivize exploration.,Structuring,Structuring.Summary,,
49,"The paper shows detailed results and analysis on 2 Atari games: Montezuma’s Revenge and Seaquest, and reports results on other games as well.",Structuring,Structuring.Summary,,
50,Strengths,Structuring,Structuring.Heading,,
51,"1. Intuitively, the idea of intra-life curiosity is reasonable.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
52,The paper pursues this idea and provides experimental evidence towards it on 2 Atari games.,Structuring,Structuring.Summary,,
53,It is able to show compelling improvements on the challenging Montezuma’s Revenge game.,Evaluative,Evaluative,P-Positive,Substance
54,Weaknesses,Structuring,Structuring.Heading,,
55,1. The two primary comparison points are missing:,Structuring,Structuring.Heading,,
56,1a. Comparison to other exploration methods.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
57,"A number of methods that use state visitation counts (also referred to as diversity, eg. [A,B]), or prediction error (also referred to as curiosity, eg [C]) have been proposed in recent years.",Fact,Fact,,
58,It is important to place the contributions in this paper in context of these other works.,Request,Request.Experiment,N-Negative,Meaningful Comparison
59,A number of these references are missing and no experimental comparison to these methods has been made.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
60,1b. Comparison between inter and intra life curiosity.,Structuring,Structuring.Heading,,
61,"One of the central motivation is the utility of intra-life curiosity vs inter-life curiosity, yet no comparisons to this effect have been provided.",Evaluative,Evaluative,N-Negative,Substance
62,2.,Other,Other,,
63,"Additionally, the paper employs a custom way of computing coverage (or diversity).",Fact,Fact,,
64,"It is in terms of location of agent on the screen, as opposed to featurization of the full game screen as used in prior works.",Fact,Fact,,
65,It is possible that a large part of the gain comes from the clever design of the space for computing intrinsic exploration reward.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
66,"The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
67,"More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.",Request,Request.Experiment,N-Negative,Substance
68,"The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work.",Request,Request.Experiment,N-Negative,Substance
69,3. I will encourage investigation on a more varied set of tasks.,Request,Request.Experiment,N-Negative,Substance
70,"Perhaps, also using some MuJoCo environments, or 3D navigation environments.",Request,Request.Experiment,N-Negative,Substance
71,"Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
72,"Additionally, all of these are still on Atari.",Fact,Fact,,
73,"[A] Diversity is All You Need: Learning Skills without a Reward Function Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine",Other,Other,,
74,"[B] EX2: Exploration with Exemplar Models for Deep Reinforcement Learning Justin Fu, John D. Co-Reyes, Sergey Levine",Other,Other,,
75,"[C] Curiosity-driven Exploration by Self-supervised Prediction Deepak Pathak, Pulkit Agrawal, Alexei A. Efros and Trevor Darrell International Conference on Machine Learning (ICML), 2017",Other,Other,,
76,"Privacy concerns arise when data is shared with third parties, a common occurrence.",Fact,Fact,,
77,"This paper proposes a privacy-preserving classification framework that consists of an encoder that extracts features from data, a classifier that performs the actual classification, and a decoder that tries to reconstruct the original data.",Structuring,Structuring.Summary,,
78,"In a mobile computing setting, the encoder is deployed at the client side and the classification is performed on the server side which accesses only the output features of the encoder.",Structuring,Structuring.Summary,,
79,The adversarial training process guarantees good accuracy of the classifier while there is no decoder being able to reconstruct the original input sample accurately.,Structuring,Structuring.Summary,,
80,Experimental results are provided to confirm the usefulness of the algorithm.,Structuring,Structuring.Summary,,
81,"The problem of privacy-preserving learning is an important topic and the paper proposes an interesting framework for that. However, I think it needs to provide more solid evaluations of the proposed algorithm, and presentation also need to be improved a bit.",Evaluative,Evaluative,N-Negative|P-Positive,Motivation/Impact|Substance
82,Detailed comments:,Structuring,Structuring.Heading,,
83,I don’t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.,Evaluative,Evaluative,N-Negative,Substance
84,The decoder used to measure privacy is very important. Can you provide more detail about the decoders used in all the four cases?,Request,Request.Explanation,U-Neutral,Clarity
85,"If possible, evaluating the privacy with different decoders may provide a stronger evidence for the proposed method.",Request,Request.Experiment,U-Neutral,Substance
86,It seems that DNN(resized) is a generalization of DNN.,Fact,Fact,,
87,"If so, by changing the magnitude of noise and projection dimensions for PCA should give a DNN(resized) result (in Figure 3) that is close to DNN.",Fact,Fact,,
88,"If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
89,The abstract mentioned that the proposed algorithm works as an “implicit regularization leading to better classification accuracy than the original model which completely ignores privacy”. But I don’t see clearly from the experimental results how the accuracy compares to a non-private classifier.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
90,Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.,Request,Request.Experiment,N-Negative,Substance
91,I think it needs to be made clearer how reconstruction error works as a measure of privacy.,Request,Request.Edit,N-Negative,Clarity
92,"For example, an image which is totally unreadable for human eye might still leak sensitive information when fed into a machine learning model.",Fact,Fact,,
93,"In term of reference, it’s better to cite more articles with different kind of privacy attacks for how raw data can cause privacy risks.",Request,Request.Edit,U-Neutral,Meaningful Comparison
94,"For the “Noisy Data” method, it’s better to cite more articles on differential privacy and local differential privacy.",Request,Request.Edit,U-Neutral,Meaningful Comparison
95,"Some figures, like Figure 3 and 4, are hard to read.",Evaluative,Evaluative,N-Negative,Clarity
96,"The author may consider making the figures larger (maybe with a 2 by 2 layout), adjusting the position of the legend & scale of x-axis for Figure 3, and using markers with different colors for Figure 4.",Request,Request.Edit,U-Neutral,Clarity
97,"For the task of predicting interaction contact among atoms of protein complex consisting of two interacting proteins, the authors propose to train a Siamese convolutional neural network, noted as SASNet, and to use the contact map of two binding proteins’ native structure.",Structuring,Structuring.Summary,,
98,The authors claim that the proposed method outperforms methods that use hand crafted features; also the authors claim that the proposed method has better transferability.,Structuring,Structuring.Summary,,
99,"My overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
100,"Also, the compared methods don’t really use the validation set from the complex data for training at all.",Evaluative,Evaluative,N-Negative,Substance
101,Thus the experiment comparison is not really fair.,Evaluative,Evaluative,N-Negative,Substance
102,"2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
103,"Specifically, the testing dataset is fixed.",Fact,Fact,,
104,"A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.",Request,Request.Experiment,U-Neutral,Substance
105,"Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.",Request,Request.Experiment,N-Negative,Substance
106,"Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.",Request,Request.Experiment,N-Negative,Substance
107,"Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
108,"Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.",Evaluative,Evaluative,N-Negative,Substance
109,"Overall the paper is well written, and I do think the paper could be much stronger the issues above are addressed.",Evaluative,Evaluative,P-Positive,Clarity
110,Some minor issues:,Structuring,Structuring.Heading,,
111,"1)	on page 4, Section 3, the first paragraph, shouldn’t “C_p^{val} of 55” be “C_p^{test} of 55”?",Request,Request.Typo,U-Neutral,Clarity
112,2)	It is not clear what the “replicates” refer to in the experiments.,Request,Request.Clarification,U-Neutral,Substance
113,3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?,Request,Request.Clarification,U-Neutral,Substance
114,"This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent.",Structuring,Structuring.Summary,,
115,The approximated demonstrator agent is trained through standard imitation learning techniques and the learning or probing agent is trained using reinforcement learning.,Structuring,Structuring.Summary,,
116,The mind of the demonstrating agent is modeled as a latent space representation from a neural net.,Structuring,Structuring.Summary,,
117,This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior.,Structuring,Structuring.Summary,,
118,The authors test this in several gridworld environments as well as a sorting task and show that their method achieves superior performance and generalizes better to unseen states and task variations compared to several baseline methods.,Structuring,Structuring.Summary,,
119,"General comments, in no particular order:",Structuring,Structuring.Heading,,
120,1. The authors should provide more details on how the hand-crafted demonstrator agents were made.,Request,Request.Explanation,N-Negative,Replicability
121,"I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?",Request,Request.Explanation,N-Negative,Replicability
122,2. The demonstrated tasks are (gridworld and algorithmic) which are very simple RL taks with low-dimensional (non-visual) state-spaces,Fact,Fact,,
123,.,,,,
124,"It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
125,3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps.,Fact,Fact,,
126,"Because the latent vector is modeled as a non-linear function, distances between latent vector representations do not necessarily correspond to similar distances between behavior policies (for example, KL distances between two policy distributions).",Fact,Fact,,
127,"Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.",Evaluative,Evaluative,N-Negative,Substance
128,4. The biggest flaw that I see in this method is the practicality of it's use.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
129,This method relies on the ability to obtain or gain access to a demonstration agent to learn from.,Fact,Fact,,
130,"In very simple tasks, such as the one presented here, the authors were able to hard-code their own demonstration agent.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
131,"However, in harder tasks, this will not be feasible.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
132,"If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
133,"In reality, for sufficiently difficult tasks, a human would be the demonstration agent (as is done in most robotics tasks).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
134,"In practice, imitation learning from a human works well since the learning can be done offline (i.e., post-hoc after a set of demonstrations are collected from the human).",Fact,Fact,,
135,"However, this task requires the learning to be interactive and thus the demonstrator needs to be present during the learning.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
136,Interactively learning from a human becomes a problem if the learning takes tens of thousands of episodes of training since a human cannot reasonably be expected to be present for that amount of time.,Fact,Fact,,
137,"Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?",Request,Request.Explanation,N-Negative,Soundness/Correctness
138,5. My previous comment relates mainly to the application of improved imitation learning.,Fact,Fact,,
139,"However, I do think this is still very useful in the context of multi-agent reinforcement learning for collaborative and competitive tasks (sections 4.6 and 4.7).",Evaluative,Evaluative,P-Positive,Motivation/Impact
140,I think this method demonstrates a method for improved collaborative and/or competitive performances given the fact that you already have a single agent with a learned policy.,Fact,Fact,,
141,"Overall, I think the paper presents a really nice idea of how to improve modeling of agents.",Evaluative,Evaluative,P-Positive,Originality
142,"essentially, a learner agent learns how to probe a demonstrator agent to provide more information about what's being demonstrated and prevent over-fitting to a set of fixed demonstrations.",Fact,Fact,,
143,"This work sounds novel to me from a reinforcement learning perspective, however, I'm not well versed on theory of mind research.",Evaluative,Evaluative,P-Positive,Originality
144,"This paper proposes a Frank-Wolfe based method, called DFW, for training Deep Network.",Structuring,Structuring.Summary,,
145,"The DFW method linearizes the loss function into a smooth one, and also adopts Nesterov Momentum to accelerate the training.",Fact,Fact,,
146,Both techniques have been widely used in the literature for similar settings.,Fact,Fact,,
147,"This paper mainly focuses on the algorithm part, but only empirically demonstrate the convergence results.",Structuring,Structuring.Summary,,
148,"After reading the authors’ feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community.",Evaluative,Evaluative,P-Positive,Substance
149,"In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this?",Request,Request.Explanation,U-Neutral,Substance
150,This paper uses multi class hinge loss as an example for illustration.,Fact,Fact,,
151,"Can this approach be applied for structure prediction, for example, various ranking loss?",Request,Request.Experiment,U-Neutral,Replicability
152,* Strengths:,Structuring,Structuring.Heading,,
153,- The paper gives theoretical insight into why Batch Normalization is useful in making neural network training more robust and is therefore an important contribution to the literature.,Structuring,Structuring.Summary,,
154,"- While the actual arguments are somewhat technical as is expected from such a paper, the motivation and general strategy is very easy to follow and insightful.",Evaluative,Evaluative,P-Positive,Substance
155,* Weaknesses:,Structuring,Structuring.Heading,,
156,"- The bounds do not immediately apply in the batch normalization setting as used by neural network practitioners, however there are practical ways to link the two settings as pointed out in section 2.4",Evaluative,Evaluative,U-Neutral,Soundness/Correctness
157,"- As the authors point out, the idea of using a batch-normalization like strategy to set an adaptive learning rate has already been explored in the WNGrad paper.",Structuring,Structuring.Quote,,
158,However it is valuable to have a similar analysis closer to the batch normalization setting used by most practitioners.,Request,Request.Experiment,P-Positive,Meaningful Comparison
159,"- Currently there is no experimental evaluation of the claims, which would be valuable given that the setting doesn't immediately apply in the normal batch normalization setting.",Request,Request.Experiment,U-Neutral,Substance
160,I would like to see evidence that the main benefit from batch normalization indeed comes from picking a good adaptive learning rate.,Request,Request.Experiment,U-Neutral,Soundness/Correctness
161,"Overall I recommend publishing the paper as it is a well-written and insightful discussion of batch normalization. Be aware that I read the paper and wrote this review on short notice, so I didn't have time to go through all the arguments in detail.",Evaluative,Evaluative,P-Positive,Originality
162,1) Summary,Structuring,Structuring.Summary,,
163,This paper proposes a method for learning an agent by interacting and probing an expert agents behavior.,Structuring,Structuring.Summary,,
164,"This method is composed of a policy that learns to imitate an expert’s action, and a policy that challenges the expert in order to get it to take multiple possible routes to solve a task.",Structuring,Structuring.Summary,,
165,"The two policies share a “behavior tracker” that models the expert’s behavior, and communicates it to both policies being learned.",Structuring,Structuring.Summary,,
166,The probing policy is optimized using a curiosity-driven reward in order to get the expert take trajectories the probing policy has not seen before.,Structuring,Structuring.Summary,,
167,"In experiments, the authors perform experiments to show how the learned agent can generalize to unseen configurations in the corresponding environments in which the agents were trained, and also use the proposed technique in a sorting task in which the method generalizes to longer arrays to be sorted.",Structuring,Structuring.Summary,,
168,2) Pros:,Structuring,Structuring.Heading,,
169,+ Neat idea for exploring an experts behavior by changing the environment surrounding it (probing it).,Evaluative,Evaluative,P-Positive,Originality
170,+ Cool experiments for applicability.,Evaluative,Evaluative,P-Positive,Replicability
171,+ Well written paper and easy to understand.,Evaluative,Evaluative,P-Positive,Clarity
172,3 Comments:,Structuring,Structuring.Heading,,
173,- Equation 1 typo?:,Request,Request.Typo,U-Neutral,Substance
174,"To my understanding, in curiosity driven exploration, the exploration is driven based on how well the next state can be predicted by the agent.",Fact,Fact,,
175,"In equation 1, different time steps are being compared, m^t and m^{t-1}, but the comparison should be between the predicted time step t and real time step t. Can the authors clarify why different time steps are compared in the equation?",Request,Request.Clarification,U-Neutral,Substance
176,- Baseline missing: Random actions from expert,Request,Request.Experiment,N-Negative,Meaningful Comparison
177,A simple baseline to compare against could be to simply force the expert to take a few random actions during its trajectory and let the imitator learn from these.,Fact,Fact,,
178,Comparing against this baseline could serve as evidence that we need to actually learn the probing agent to acquire a more optimal policy.,Fact,Fact,,
179,- Baseline missing: Simple RNN policies that communicate hidden states.,Request,Request.Experiment,N-Negative,Meaningful Comparison
180,Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.,Fact,Fact,,
181,While optimizing the curiosity reward the hidden states could be used as well.,Other,Other,,
182,"If successful, this baseline can show that we actually need to model the “behavior” with a separate network.",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
183,- Ablation study for the importance of fusion:,Structuring,Structuring.Heading,,
184,The authors have a “fusion” layer within the imitator and probing policies.,Fact,Fact,,
185,An ablation study showing that this layer is actually necessary is missing from the paper.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
186,- Generalizability argument,Structuring,Structuring.Heading,,
187,"The authors claim that they show a single starting configuration for the agents during training, and different starting configurations during testing.",Fact,Fact,,
188,"While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
189,"It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
190,A more drastic change of the environment could make for a stronger argument.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
191,4) Conclusion:,Structuring,Structuring.Heading,,
192,"Overall, I like the idea of having a policy that tries to figure out the general behavior of a demonstrator by probing it.",Evaluative,Evaluative,P-Positive,Motivation/Impact
193,"Having said that, I feel this paper needs to improve in the aspects mentioned above.",Request,Request.Edit,U-Neutral,Substance
194,"If the authors present more convincing evidence that successfully address the comments above, I am willing to increase my score.",Request,Request.Experiment,P-Positive,Substance
195,The idea of image classification based on patch-level deep feature in the BoF model has been studied extensively.,Structuring,Structuring.Summary,,
196,Just list few of them:,Structuring,Structuring.Heading,,
197,"Wei et al. HCP: A Flexible CNN Framework for Multi-label Image Classification, IEEE TPAMI 2016",Other,Other,,
198,"Tang et al. Deep Patch Learning for Weakly Supervised Object Classification and Discovery, Pattern Recognition 2017",Other,Other,,
199,"Tang et al. Deep FisherNet for Object Classification, IEEE TNNLS",Other,Other,,
200,"Arandjelović et al. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition, CVPR 2016",Other,Other,,
201,The above papers are not cited in this paper.,Request,Request.Edit,N-Negative,Other
202,There are some unique points.,Evaluative,Evaluative,P-Positive,Originality
203,This work does not use RoIPooling layer and has results on ImageNet.,Fact,Fact,,
204,"But, the previous works use RoIPooling layer to save computations and works on scene understanding images, such as PASCAL.",Fact,Fact,,
205,"Besides, the paper uses the smallest patch among all the patch-based deep networks.",Fact,Fact,,
206,It is interesting.,Fact,Fact,,
207,I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.,Request,Request.Experiment,N-Negative,Substance
208,This paper displays an occurrence of density models assigning higher likelihood to out-of-distribution inputs compared to the training distribution.,Structuring,Structuring.Summary,,
209,"Specifically, density models trained on CIFAR10 have higher likelihood on SVHN than CIFAR10.",Fact,Fact,,
210,This is an interesting observation because the prevailing assumption is that density models can distinguish inliers from outliers.,Evaluative,Evaluative,P-Positive,Motivation/Impact
211,"However, this phenomenon is not encountered when comparing MNIST and NotMNIST.",Fact,Fact,,
212,The SVHN/CIFAR10 phenomenon has also been shown in concurrent work [1].,Other,Other,,
213,"Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?",Request,Request.Explanation,N-Negative,Soundness/Correctness
214,It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
215,"For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
216,"Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.",Request,Request.Experiment,N-Negative,Substance
217,"For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting.",Request,Request.Experiment,U-Neutral,Substance
218,The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
219,This paper is well written.,Evaluative,Evaluative,P-Positive,Clarity
220,I think the presentation of this density modelling shortcoming is a good contribution but leaves a bit to be desired.,Evaluative,Evaluative,U-Neutral,Motivation/Impact
221,"[1] Choi, H. and Jang, E. Generative Ensembles for Robust Anomaly Detection.",Other,Other,,
222,https://arxiv.org/abs/1810.01392,Other,Other,,
224,- Interesting observation of density modelling shortcoming,Evaluative,Evaluative,P-Positive,Motivation/Impact
225,- Clear presentation,Evaluative,Evaluative,P-Positive,Clarity
227,- Lack of a strong explanation for the results or a solution to the problem,Evaluative,Evaluative,N-Negative,Soundness/Correctness
228,- Lack of an extensive exploration of datasets,Evaluative,Evaluative,N-Negative,Substance
229,GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications.,Structuring,Structuring.Summary,,
230,"Unfortunately, GANs often show unstable behaviour during the training phase.",Structuring,Structuring.Summary,,
231,"The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.",Structuring,Structuring.Summary,,
232,"While the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:",Evaluative,Evaluative,N-Negative,Soundness/Correctness
233,"1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\theta_{old}}, ...",Evaluative,Evaluative,N-Negative,Soundness/Correctness
234,"2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
235,"3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.",Evaluative,Evaluative,N-Negative,Motivation/Impact
236,"4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability.",Fact,Fact,,
237,"It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
238,"While the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.",Fact,Fact,,
239,---,Structuring,Structuring.Heading,,
240,After paper revisions:,Structuring,Structuring.Heading,,
241,Thank you for the updates.,Social,Social,,
242,The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community.,Evaluative,Evaluative,P-Positive,Other
243,"This paper addresses the problem of adversarial transferability, i.e. the ability that an adversarial example generated by one model can successfully fool another model.",Structuring,Structuring.Summary,,
244,"There are numerous papers on this topic recently, such as Fawzi'15, Liu'17, Dong'18, Athalye'18...",Structuring,Structuring.Summary,,
245,The authors propose tot study two types of factors that might influence transferability: model-specific parameters and smoothness of loss surface for constructing adversarial examples.,Structuring,Structuring.Summary,,
246,Two experimental studies are made for each influence factor from existing architectures.,Structuring,Structuring.Summary,,
247,"Another attack strategy aiming at smoothing the loss surface is proposed, an experimental evaluation shows the effectiveness of the proposed method.",Structuring,Structuring.Summary,,
248,Pros,Structuring,Structuring.Heading,,
249,-the proposed experimental studies can be interesting to the community,Evaluative,Evaluative,P-Positive,Originality
250,-many interesting illustrations are provided.,Evaluative,Evaluative,P-Positive,Substance
251,Cons,Structuring,Structuring.Heading,,
252,"-The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better",Evaluative,Evaluative,U-Neutral,Originality
253,-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.,Evaluative,Evaluative,N-Negative,Substance
254,"-Only two influence factors are studied, again the paper would be more interesting with a more general study",Evaluative,Evaluative,N-Negative,Substance
255,The paper has an interesting potential but seems a bit limited in its present form.,Evaluative,Evaluative,N-Negative,Substance
256,Problem and contribution:,Structuring,Structuring.Heading,,
257,The paper studies if the Visual Question answering model “FILM” from Perez et al (2018) is able to decide if “most” of the objects have a certain attribute or color.,Structuring,Structuring.Summary,,
258,For this it tries to mimic the setup used to test human abilities in the study by Pietroski et al. (2009).,Structuring,Structuring.Summary,,
259,The main contribution of this is work is a discussion of how a model could solve the problem of deciding “most” and the study which shows that the studied model has some ability to do this.,Structuring,Structuring.Summary,,
260,From this the paper concludes that the model is likely to have some approximate number system.,Structuring,Structuring.Summary,,
261,Strengths:,Structuring,Structuring.Heading,,
262,"1.	The paper looks at a new angle to study and characterize CNN models in general, and VQA models in particular by looking into the psycholinguistic literature experimental setup studied with human subjects.",Evaluative,Evaluative,P-Positive,Originality
263,"2.	The paper studies different variants of controlling for different factors (e.g. pairing data points, area used, different training data and pre-trained vs. trained from scratch CNN models)",Evaluative,Evaluative,P-Positive,Substance
264,3.	It is interesting to see that the models performance reasonably aligns with the curve predicted by “Weber’s law”.,Evaluative,Evaluative,P-Positive,Substance
265,Weaknesses:,Structuring,Structuring.Heading,,
266,"4.	Number of objects vs. ratios is not disentangled: While the paper clarifies that not only a smaller number of objects are used, it would be interesting to understand if similar conclusions hold if only the same number or about the same number of total objects are used but the ratios change (at least for more extreme ratios, 1:2, this seems to be the case as they achieve 100% accuracy).",Evaluative,Evaluative,N-Negative,Substance
267,5.,Evaluative,Evaluative,N-Negative,Substance
268,"The paper only focusses on a single VQA model (FILM) which limits the understanding if this observation is specific to this model; what about other models such as the one from Hudson & Manning (2018), or Relation Networks (Santoro et al) or even simpler baselines: A system which two attention mechanisms (without normalizations) which are sum pooled and then compared would sort of explicitly encode the idea of the APN system.",,,,
269,It would be valuable to compare them to see how different systems (can) solve this task.,Request,Request.Experiment,N-Negative,Substance
270,I would expect that the architecture favors certain capabilities; e.g. Relation Networks might lead more to a paring-based strategy. Or Zhang et al. (2018) might be able to exploit explicit counting to solve the task.,Fact,Fact,,
271,6.	The “most” ability or APN ability seems to be highly related to accumulation in neural networks.,Fact,Fact,,
272,The paper FiLM uses global max-pooling and I am wondering if this affect this ability.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
273,7.	The study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
274,7.1.,Other,Other,,
275,"Maybe beyond the scope of this work, but it would be interesting to understand how much training data different models need to obtain this capability.",,,,
276,"8.	For evaluation: Are there distractors, i.e. elements which don’t belong to set A or B? If not, how would distractors affect it.",Request,Request.Clarification,N-Negative,Substance
277,9.	Clarity:,Structuring,Structuring.Summary,,
278,9.1.,Request,Request.Edit,N-Negative,Clarity
279,The equation between equation (1) and (2) misses a number [I will call it 1.5 for now],,,,
280,9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.,Request,Request.Edit,N-Negative,Clarity
281,Minor:,Structuring,Structuring.Heading,,
282,10.	The title suggests that the paper studies multiple VQA models but only a single model is studied.,Request,Request.Edit,N-Negative,Clarity
283,Conclusion:,Structuring,Structuring.Heading,,
284,"The paper looks into an interesting direction to study CNN models but has some limitations including studying only a single VQA model type, limited to artificially generated images.",Evaluative,Evaluative,N-Negative,Substance
286,- The finding that SVHN has larger likelihood than CIFAR according to networks is interesting.,Evaluative,Evaluative,P-Positive,Motivation/Impact
287,"- The empirical and theoretical analyses are clear, seem thorough, and make sense.",Evaluative,Evaluative,P-Positive,Clarity
288,- Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian).,Evaluative,Evaluative,P-Positive,Substance
290,"- The premises of the analyses are not very convincing, limiting the significance of the paper.",Evaluative,Evaluative,N-Negative,Replicability
291,"- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.",Evaluative,Evaluative,N-Negative,Replicability
292,"In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.",Evaluative,Evaluative,N-Negative,Replicability
293,- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
294,At least this limitation should be pointed out in the paper.,,,,
295,- Some parts of the paper feel long-winded and aimless.,Evaluative,Evaluative,N-Negative,Clarity
296,[Quality],Structuring,Structuring.Heading,,
297,See above pros and cons.,Structuring,Structuring.Heading,,
298,A few less important disagreement I have with the paper:,Structuring,Structuring.Heading,,
299,- I don't think Glow necessarily is encouraged to increase sensitivity to perturbations.,Evaluative,Evaluative,N-Negative,Substance
300,"The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.",Evaluative,Evaluative,N-Negative,Substance
301,"- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.",Evaluative,Evaluative,N-Negative,Substance
302,[Clarity],Structuring,Structuring.Heading,,
303,"In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.",Evaluative,Evaluative,N-Negative|P-Positive,Clarity|Substance
304,Section 2 background takes too much space.,Evaluative,Evaluative,N-Negative,Substance
305,"Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.",Evaluative,Evaluative,N-Negative,Substance
306,"Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.",Evaluative,Evaluative,N-Negative,Substance
307,A few editorial issues:,Structuring,Structuring.Heading,,
308,"- On page 4 footnote 2, as far as I know the paper did not define BPD.",Request,Request.Explanation,N-Negative,Substance
309,"- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.",Request,Request.Typo,N-Negative,Clarity
310,[Originality],Structuring,Structuring.Heading,,
311,"I am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel.",Evaluative,Evaluative,P-Positive,Originality
312,"However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:",Evaluative,Evaluative,N-Negative,Other
313,Vít Škvára et al. Are generative deep models for novelty detection truly better?,Other,Other,,
314,"^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.",,,,
315,A part of the paper's contribution (section 5 conclusion) seem to overlap with others' work.,,,,
316,"The section concludes that if the second dataset has small variances, it will get higher likelihood.",,,,
317,But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).,,,,
318,[Significance],Structuring,Structuring.Heading,,
319,The paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly.,Evaluative,Evaluative,P-Positive,Motivation/Impact
320,"However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.",Evaluative,Evaluative,N-Negative,Replicability|Substance
321,"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?",Request,Request.Experiment,N-Negative,Substance
322,"Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.",Evaluative,Evaluative,N-Negative,Substance
323,"In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.",Structuring,Structuring.Summary,,
324,The paper is interesting and well written.,Evaluative,Evaluative,P-Positive,Clarity
325,"However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.",Request,Request.Experiment,U-Neutral,Substance
326,"The authors propose three improvements to the DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links --- and show that this allows the model to solve synthetic memory tasks faster and with better precision.",Structuring,Structuring.Summary,,
327,They also show the model performs better on average on bAbI than the original DNC.,Structuring,Structuring.Summary,,
328,"The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.",Evaluative,Evaluative,N-Negative,Originality
329,"I think the paper is particularly clearly written, and I would vote for it being accepted as it has implications beyond the DNC.",Evaluative,Evaluative,P-Positive,Motivation/Impact
330,The fact that masked attention works so much better than the standard cosine-weighted content-based attention is pretty interesting in itself.,Fact,Fact,,
331,"The insights (e.g. Figure 5) are interesting and show the study is not just trying to be a benchmark paper for some top-level results, but actually cares about understanding a problem and fixing it.",Evaluative,Evaluative,P-Positive,Motivation/Impact
332,"Although most recent memory architectures do not seem to have incorporated the DNC's slightly complex memory de-allocation scheme, any resurgent work in this area would benefit from this study.",Fact,Fact,,
333,"This is an novel, interesting paper on an important topic: semi-supervised learning.",Evaluative,Evaluative,P-Positive,Originality
334,"Even though the proposed approach seems to have significant potential, the experimental",Evaluative,Evaluative,N-Negative,Originality
335,"is somewhat disorganized,",,,,
336,and it also includes some weak claims that should be removed.,Request,Request.Edit,N-Negative,Substance
337,"For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).",Request,Request.Edit,N-Negative,Substance
338,"In this reviewer's opinion, it would be a lot more reasonable to have instead a learning curve showing the results for, say, 100, 500, 1K, 5K, and 10K labeled examples for all three domains.",Request,Request.Experiment,U-Neutral,Substance
339,"In 4.1, you are using different epsilon policies for synthetic vs organic datasets; why?",Request,Request.Explanation,U-Neutral,Substance
340,"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.",Request,Request.Explanation,N-Negative,Substance
341,You should also show the performance of regular SSL methods in the setup on Table 4.,Request,Request.Experiment,U-Neutral,Substance
342,"Last but not least, you have repeatedly made the claim combining SST and other SSL may further improve the performance;",Fact,Fact,,
343,"however, you do not provide any evidence for it, so you should avoid making such claims.",Request,Request.Edit,N-Negative,Substance
344,Other comments:,Structuring,Structuring.Heading,,
345,"- on page 2, the two terms classification & selection network appear ""out of the blue;"" it would be quite helpful to make it clear from the abstract that the proposed implementation is for neural networks.",Request,Request.Clarification,U-Neutral,Substance
346,- figures 2 & 3 should be a lot larger in order to be readable,Request,Request.Edit,N-Negative,Clarity
347,"- 4.1.2 top of page 7: claims such as ""SST could have obtained better performance"" have no place in such a paper; you could instead make a note about the method being ""prohibitively CPU intensive for the time being""",Request,Request.Edit,U-Neutral,Substance
348,"- lower on the same page you say: ""SST may get better performance"" - see above",Request,Request.Edit,U-Neutral,Substance
349,"A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude.",Structuring,Structuring.Summary,,
350,They used different model compression techniques in this framework to show the effectiveness of the proposed method.,Structuring,Structuring.Summary,,
351,This paper proposes a framework intending to use fewer hardware resources without compromising the model accuracy.,Structuring,Structuring.Summary,,
352,"However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
353,"Therefore, it is not clear how the proposed framework is helping the model compression techniques.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
354,"I really enjoyed reading the paper! The exposition is clear with interesting observations, and most importantly, the authors walk the extra mile in doing a theoretical analysis of the observed phenomena.",Evaluative,Evaluative,P-Positive,Clarity|Substance
355,Questions for the authors:,Structuring,Structuring.Heading,,
356,1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled “Generative Ensembles for Robust Anomaly Detection” makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.,Other,Other,,
357,Their criteria also accounts for the variance in model log-likelihoods and is hence slightly different.,Other,Other,,
358,"2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test.",Fact,Fact,,
359,"If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?",Request,Request.Explanation,,
360,3. Why does the constant image (all zeros) in Figure 9 (appendix) have such a high likelihood? It’s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn’t seem applicable.,Request,Request.Explanation,,
361,4. How much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images.,Request,Request.Experiment,,
362,5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.,Fact,Fact,,
363,This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.,Request,Request.Explanation,,
364,Minor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :),Request,Request.Typo,,
366,This paper is built on the top of DNC model.,Structuring,Structuring.Summary,,
367,"Authors observe a list of issues with the DNC model: issues with deallocation scheme, issues with the blurring of forward and backward addressing, and issues in content-based addressing.",Structuring,Structuring.Summary,,
368,Authors propose changes in the network architecture to solve all these three issues.,Structuring,Structuring.Summary,,
369,"With toy experiments, authors demonstrate the usefulness of the proposed modifications to DNC.",Structuring,Structuring.Summary,,
370,The improvements are also seen in more realistic bAbI tasks.,Structuring,Structuring.Summary,,
371,Major Comments:,Structuring,Structuring.Heading,,
372,The paper is well written and easy to follow.,Evaluative,Evaluative,P-Positive,Clarity
373,The proposed improvements seem to result in very clear improvements.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
374,The proposed improvements also improve the convergence of the model.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
375,I do not have any major concerns about the paper.,Social,Social,,
376,I think that contributions of the paper are good enough to accept the paper.,Social,Social,,
377,I also appreciate that the authors have submitted the code to reproduce the results.,Evaluative,Evaluative,P-Positive,Replicability
378,I am curious to know if authors observe similar convergence gains in bAbI tasks as well. Can you please provide the mean learning curve for bAbI task for DNC vs proposed modifications?,Request,Request.Explanation,U-Neutral,Substance
379,"This is a paper of the verification of neural networks, i.e. check their robustness,",Structuring,Structuring.Summary,,
380,and the main contribution here is to tackle it as a statistical problem adressed with,,,,
381,multi-level splitting Monte Carlo approach.,,,,
382,"I found the paper well motivated and original,",Evaluative,Evaluative,P-Positive,Originality
383,resulting in a publishable piece of research up to a few necessary adjustments.,,,,
384,These,Evaluative,Evaluative,N-Negative,Clarity
385,concern principally notation issues and some potential improvements in the writing.,,,,
386,"Let me list below some main remarks along the text, including also some typos.",Structuring,Structuring.Heading,,
387,"* In the introduction, ""the classical approach"" is mentioned but to be the latter is",Evaluative,Evaluative,N-Negative,Substance
388,insufficiently covered. Some more detail would be welcome.,,,,
389,"* page 2, ""predict the probability"": rather employ ""estimate"" in such context?",Request,Request.Edit,U-Neutral,Soundness/Correctness
390,"* ""linear piecewise"": ""piecewise linear""?",Request,Request.Typo,U-Neutral,Soundness/Correctness
391,"* what is ""an exact upper bound""?",Request,Request.Clarification,U-Neutral,Clarity
392,"* In related work, no reference to previous work on ""statistical"" approaches to NN",Evaluative,Evaluative,N-Negative,Meaningful Comparison
393,verification. Is it actually the case that this angle has never been explored so far?,,,,
394,"* I am not an expert but to me ""the density of adversarial examples"" calls for further",Request,Request.Edit,U-Neutral,Soundness/Correctness
395,explanation.,,,,
396,* From page 3 onwards: I was truly confused by the use of [x] throughought the text,Evaluative,Evaluative,N-Negative,Soundness/Correctness
397,(e.g. in Equation (4)),,,,
399,"x is already present within the indicator, no need to add yet",Evaluative,Evaluative,N-Negative,Soundness/Correctness
400,another instance of it. Here and later I suffered from what seems to be like an awkward,,,,
401,attempts to stress dependency on variables that already appear or should otherwise,,,,
402,appear in a less convoluted way.,,,,
403,"* In Section 4, it took me some time to understand that the considered metrics do not",Evaluative,Evaluative,N-Negative,Clarity
404,require actual observations but rather concern coherence properties of the NN per se.,,,,
405,"While this follows from the current framework, the paper might benefit from some more",Request,Request.Edit,U-Neutral,Soundness/Correctness
406,explanation in words regarding this important aspect.,,,,
407,"* In page 6, what is meant by ""more perceptually similar to the datapoint""?",Request,Request.Clarification,U-Neutral,Clarity
408,"* In the discussion: is it really ""a new measure"" that is introduced here?",Request,Request.Clarification,U-Neutral,Soundness/Correctness
409,*,Evaluative,Evaluative,N-Negative,Soundness/Correctness
410,"In the appendix: the MH acronym should better be introduced, as should the notation",,,,
411,"g(x,|x')",,,,
412,if not done elsewhere (in which case a cross-reference would be welcome).,,,,
413,"Besides this, writing ""the last samples"" requires disambiguation (using ""respective""?).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
414,This paper suggests a novel and compact neural network architecture which uses the information within bag-of-words features.,Structuring,Structuring.Summary,,
415,The proposed algorithm only uses the patch information independently and performs majority voting using independently classified patches.,Structuring,Structuring.Heading,,
416,"The proposed method provides the state-of-the-art prediction accuracy unexpectedly, and several additional experiments show the state-of-the-art neural networks mainly learn without association between information in different patches.",Structuring,Structuring.Summary,,
417,"The proposed algorithm is simple and does not provide completely new idea, but this paper has a clear contribution connecting the previous main idea of feature extraction, bag-of-words, and the prevailing blackbox algorithm, CNN.",Evaluative,Evaluative,P-Positive,Originality
418,The results in the paper are worth to be shared in the community and need further investigated.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
419,"The presented experiments look fair and reasonable to show the importance of the independent patch information (without association between them), and the presented experimental results show some state-of-the-art methods also perform with independent patch information.",Evaluative,Evaluative,P-Positive,Substance
420,Comparison with attention models is necessary to compare the important patches obtained from conventional networks.,Request,Request.Experiment,U-Neutral,Substance
421,"The paper presents a method of learning representations that is based on minimizing ""deficiency"" rather than optimizing for information sufficiency.",Structuring,Structuring.Summary,,
422,"While perfect optimization of the sufficiency term in IB is equivalent to minimizing deficiency, the thesis of the paper is that the variational upper bound on deficiency is easier to optimize, and when optimized produces",Structuring,Structuring.Summary,,
423,"better (more compressed representations), while performing equally on test accuracy.",,,,
424,The paper is well written and easy to read.,Evaluative,Evaluative,P-Positive,Clarity
425,"The idea behind the paper (optimizing for minimizing deficiency instead of sufficiency in IB) is interesting, especially because the variational formulation of DB is a generalization of VIB (in that VIB reduces to VDB for M=1).",Evaluative,Evaluative,P-Positive,Motivation/Impact
426,"What takes away from the paper is that while perfect optimization of IB/sufficiency is equivalent to perfect optimization of DB, it is not clear what happens when perfection is not achieved.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
427,"Further, the authors claim that DB is able to obtain more compressed representations (But is the goal a compressed representation, or an informative one?).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
428,"The paper would also benefit from evaluation of the representation itself, and comparison to other non-information bottleneck based algorithms.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
430,The paper proposes to add to the original GAN (2014) loss a zero-centered gradient penalty as the one defined in the WGAN-GP paper.,Structuring,Structuring.Summary,,
431,It also provides an analysis on the mode collapse and lack of stability of classical GANs.,Structuring,Structuring.Summary,,
432,The authors compare results using their penalty on a few synthetic examples and on image net dogs generations to results using the classical GAN loss with or without gradient penalties.,Structuring,Structuring.Summary,,
433,Positive points:,Structuring,Structuring.Heading,,
434,The paper is interesting to read and well illustrated.,Evaluative,Evaluative,P-Positive,Substance
435,An experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.,Evaluative,Evaluative,P-Positive,Substance
436,Points to improve:,Structuring,Structuring.Heading,,
437,"If I understood correctly, the main contribution resides in the application of the GP proposed by WGAN-GP to the original setting.",Fact,Fact,,
438,"Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
439,"WGAN-GP, VEEGAN, or Lucas et al arXiv:1806.07185, ICML 2018",Other,Other,,
440,to name only a few.,,,,
441,"The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction.",Evaluative,Evaluative,N-Negative,Clarity
442,The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments,Evaluative,Evaluative,N-Negative,Substance
443,"The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
444,The imagenet experiment lacks details.,Evaluative,Evaluative,N-Negative,Clarity
445,This paper proposes an additional loss term to use when training an LSTM LM.,Structuring,Structuring.Summary,,
446,"The authors argue that, intuitively, we want the output distribution to retain some information about the context, or ""past"".",Structuring,Structuring.Summary,,
447,"Given this, they use the output distribution as input to a one layer network that must predict the current token.",Structuring,Structuring.Summary,,
448,The loss for this network is incorporated as an additional term used when training the LM.,Structuring,Structuring.Summary,,
449,The authors show that by adding this loss term they can achieve SOTA (for single softmax model) perplexity on a number of LM benchmarks.,Structuring,Structuring.Quote,,
450,The technical contribution is proposing a new loss term to use when training a language model.,Structuring,Structuring.Summary,,
451,"The idea is clear, simple, and well explained, and it seems to be effective in practice.",Evaluative,Evaluative,P-Positive,Motivation/Impact
452,One drawback is that it is highly specific to language models.,Evaluative,Evaluative,N-Negative,Motivation/Impact
453,"Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.",Fact,Fact,,
454,"In addition, there is not much theoretical justification for it, it seems like a one-off trick.",Evaluative,Evaluative,N-Negative,Originality
455,"The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?",Request,Request.Clarification,N-Negative,Soundness/Correctness
456,"Although it is specific to language models, there are a few reasons it might be of broader significance:",Structuring,Structuring.Heading,,
457,- It falls in the recent line of work in incorporating auxiliary losses for various tasks.,Evaluative,Evaluative,P-Positive,Substance
458,This idea has touched many problems and seen success in practice.,Evaluative,Evaluative,P-Positive,Substance
459,- Perhaps it can be applied to other sequence models.,Request,Request.Experiment,P-Positive,Meaningful Comparison
460,"For example in encoder-decoder models, the decoder can be thought of as a conditional LM.",Fact,Fact,,
461,Experiments are comprehensive and rigorous.,Evaluative,Evaluative,P-Positive,Substance
462,They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.,Request,Request.Experiment,N-Negative,Substance
464,- New SOTA for single softmax model on LM benchmarks.,Evaluative,Evaluative,P-Positive,Substance
465,"- Simple, clearly explained idea.",Evaluative,Evaluative,P-Positive,Originality
466,- Demonstrates effectiveness of auxiliary losses.,Evaluative,Evaluative,P-Positive,Originality
467,- Rigorous experiments.,Evaluative,Evaluative,P-Positive,Substance
469,- Trick is specific to LM.,Evaluative,Evaluative,N-Negative,Replicability
470,- No large corpus results.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
471,"Dual Block-Coordinate Frank-Wolfe (Dual-BCFW) has been widely used in the literature of non-smooth and strongly-convex stochastic optimization problems, such as (structural) Support Vector Machine.",Fact,Fact,,
472,"To my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network, which employs a proximal-point method that linearizes not the whole loss function but only the DNN (up to the logits) to form a convex subproblem and then deal with the loss part in the dual.",Evaluative,Evaluative,P-Positive,Originality
473,"The attempt is not perfect (actually with a couple of issues detailed below), but the proposed approach is inspiring and I personally would love it published to encourage more development along this thread.",Evaluative,Evaluative,P-Positive,Motivation/Impact
474,The following points out a couple of items that could probably help further improve the paper.,Structuring,Structuring.Heading,,
475,*FW vs BCFW*,Structuring,Structuring.Heading,,
476,"The algorithm employed in the paper is actually not Frank-Wolfe (FW) but Block-Coordinate Frank-Wolfe (BCFW), as it minimizes w.r.t. a block of dual variables belonging to the min-batch of samples.",Fact,Fact,,
477,*Batch Size*,Structuring,Structuring.Summary,,
478,"Though the algorithm can be easily extended to the min-batch case, the author should discuss more how the batch size is interpreted in this case (i.e. minimizing w.r.t. a larger block of dual variables belonging to the batch of samples) and the algorithmic block (Algorithm 1) should be presented in a way reflecting the batch size since this is the way people use an algorithm in practice (to improve the utilization rate of a GPU).",Request,Request.Edit,P-Positive,Substance
479,*Convex-Conjugate Loss*,Structuring,Structuring.Heading,,
480,The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).,Fact,Fact,,
481,All convex loss function can derive a dual formulation based on its convex-conjugate.,Fact,Fact,,
482,"See [1,2] for examples.",Structuring,Structuring.Heading,,
483,It would be more insightful to compare SGD vs dual-BCFW when both of them are optimizing the same loss functions (either hinge loss or cross-entropy loss) in the experimental comparison.,Request,Request.Experiment,P-Positive,Substance
484,"[1] Shalev-Shwartz, Shai, and Tong Zhang. ""Stochastic dual coordinate ascent methods for regularized loss minimization."" JMLR (2013)",Social,Social,,
485,"[2] Tomioka, Ryota, Taiji Suzuki, and Masashi Sugiyama. ""Super-linear convergence of dual augmented Lagrangian algorithm for sparsity regularized estimation."" JMLR (2011).",Social,Social,,
486,*BCFW vs BCD*,Structuring,Structuring.Heading,,
487,"Actually, (Lacoste-Julien, S. et al., 2013) proposes Dual-BCFW to optimize structural SVM because the problem contains exponentially many number of dual variables.",Fact,Fact,,
488,For typical multiclass hinge loss problem the Dual Block-Coordinate Descent that minimizes w.r.t. all dual variables of a sample in a closed-form update converges faster without extra computational cost.,Fact,Fact,,
489,"See the details in, for example, [3, appendix for the multiclass hinge loss case].",Structuring,Structuring.Heading,,
490,"[3] Fan, Rong-En, et al. ""LIBLINEAR: A library for large linear classification."" JMLR (2008).",Social,Social,,
491,*Hyper-Parameter*,Structuring,Structuring.Heading,,
492,"The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.",Evaluative,Evaluative,N-Negative,Substance
493,"This paper proposes a pair of LSTM networks, one of which estimates the current strategy at iteration t+1 and the other estimates the average strategy after t iterations.",Structuring,Structuring.Summary,,
494,"By using these networks within a CFR framework, the authors manage to avoid huge memory requirements traditionally needed to save cumulative regret and average strategy values for all information sets across many iterations.",Structuring,Structuring.Summary,,
495,"The neural networks are trained via a novel sampling method with lower variance/memory-requirements that outcome/external sampling, and are amendable to continual improvement by warm-starting the networks based on cloned tabular regret values.",Structuring,Structuring.Summary,,
496,"Overall, the paper is well-written with clear definitions/explanations plus  comprehensive ablation-analyses throughout, and thus constitutes a nice addition to the recent literature on leveraging neural networks for IIG.",Evaluative,Evaluative,P-Positive,Clarity|Substance
497,"I did not find many flaws to point out, except I believe the paper could benefit from more extensive  comparisons in Figure 4A against other IIG methods such as Deep Stack, as well as comparing on much larger IIG settings with many more states to see how the neural CFR methods hold up in the regime where they are most needed.",Request,Request.Experiment,N-Negative,Substance
498,"Typo:  ""care algorithm design"" -> ""careful algorithm design""",Request,Request.Typo,N-Negative,Clarity
499,"The paper is well written and the main contribution, a methodology to find “blind-spot attacks” well motivated and differences to prior work stated clearly.",Evaluative,Evaluative,P-Positive,Motivation/Impact|Clarity
500,The empirical results presented in Figure 1 and 2 are very convincing.,Evaluative,Evaluative,P-Positive,Substance
501,"The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.",Evaluative,Evaluative,P-Positive,Substance
502,"Why for example not using a simple score based on the histogram, or even the mean distance?",Request,Request.Explanation,U-Neutral,Meaningful Comparison
503,Of course providing a single measure would allow to leverage that information during training.,Fact,Fact,,
504,"However, in its current form this seems rather complicated and computationally expensive (KL-based).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
505,As stated later in the paper the histograms themselves are not informative enough to detect such blind-spot transformation.,Fact,Fact,,
506,Intuitively this makes a lot of sense given that the distance is based on the network embedding and is therefore also susceptible to this kind of data.,Evaluative,Evaluative,P-Positive,Motivation/Impact
507,"However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
508,— Summary,Structuring,Structuring.Heading,,
509,"The method extends [21], which proposes an unordered set prediction model for multi-class classification.",Structuring,Structuring.Summary,,
510,"For that problem, [21] can assume logistic outputs for all distinct classes.",Structuring,Structuring.Summary,,
511,"This work extends set prediction to the object detection task, where box identity is not distinct — this is handled by an additional model output that reasons about the most likely object permutations.",Structuring,Structuring.Summary,,
512,"The permutation predictions are used during training, but are not needed at inference time — as shown in Fig1 and Eq 7.",Structuring,Structuring.Summary,,
513,Results are on detection of overlapping objects and a CAPTCHA toy summation example.,Structuring,Structuring.Summary,,
514,— Clarity,Structuring,Structuring.Heading,,
515,The exposition is not particularly clear in several places:,Evaluative,Evaluative,N-Negative,Clarity
516,- U^m in Eq 1 is undefined and un-discussed.,Evaluative,Evaluative,N-Negative,Clarity
517,"What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.",Request,Request.Clarification,N-Negative,Clarity
518,- The term p(w) disappears on the left hand side of Eq 2.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
519,"- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.",Evaluative,Evaluative,N-Negative,Clarity
520,"Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
521,"Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
522,"If so, why does term f2 in Eq 5 contain the permutation head output O2 and how do the two relate?",Request,Request.Clarification,N-Negative,Soundness/Correctness
523,"- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.",Evaluative,Evaluative,N-Negative,Replicability
524,The dimensions of the convolutional feature map matter (probably need to be kept tractable).,Fact,Fact,,
525,— Significance,Structuring,Structuring.Heading,,
526,"Key aspects of the model are not particularly clear, specifically about how the permutation prediction ( the key novelty here) is used to benefit training.",Evaluative,Evaluative,N-Negative,Clarity
527,"— Term f2 in Eq5 uses w~ estimates, which appeared to be based on statistics from past SGD runs, yet also depends on the output of the permutation head O2. Am I misinterpreting the method?",Request,Request.Clarification,N-Negative,Soundness/Correctness
528,"— In the paragraph right after Eq5, it’s claimed that “Empirically, in our applications, we found out that estimation of the permutations from just f1 [in Eq5] is sufficient to train properly … by using the Hungarian algorithm”. So then f2 term is not even used in. Eq5? If so, what is the significance of the permutation head other than adding an auxiliary loss?",Request,Request.Clarification,N-Negative,Soundness/Correctness
529,"Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?",Request,Request.Explanation,N-Negative,Soundness/Correctness
530,"While the failure of NMS to detect overlapping objects is expected, the experiments showing that perm-set prediction handles them well is interesting and promising.",Evaluative,Evaluative,P-Positive,Substance
531,"Solving the general case with larger images and many instances would increase the impact significantly — and likely require a combination of perm-set prediction and image tiling, although this is just a hypothesis.",Fact,Fact,,
532,"The Captcha toy example also shows some interesting behavior emerging — without digit-specific annotations (otherwise it would be multi-class classification setup from [21]), the model can handle the majority of summations correctly.",Evaluative,Evaluative,P-Positive,Substance
533,— Experimental results,Structuring,Structuring.Heading,,
534,The results are interesting proofs-of-concept but a few more experiments/answers would be helpful:,Request,Request.Experiment,U-Neutral,Substance
535,- It still appears that PR curve in the high-precision regime (fig 3b) has lower precision than FRCNN/YOLO.,Structuring,Structuring.Quote,,
536,Any idea as to why?,Request,Request.Clarification,U-Neutral,Soundness/Correctness
537,"- Ablation results on the effect of the permutation predictions vs Hungarian algorithm, etc would be helpful, as discussed above.",Request,Request.Experiment,U-Neutral,Meaningful Comparison
538,"- How sensitive is the method to seeing a certain cardinality? What if it never sees 3 pedestrians in an image, but only 1,2,4 will it fail to predict 3? Or alternatively, if we train a model that can handle up to 5-6 entities with examples than have <=4? What is the right way of data augmentation for this model (was there any and should there be?)",Request,Request.Explanation,U-Neutral,Clarity
539,"- Given that values for U differ across applications, how sensitive is the output / how much sweeping did you have to do?",Request,Request.Explanation,U-Neutral,Clarity
540,-- Related work,Structuring,Structuring.Heading,,
541,To the best of my knowledge it's representative.,Fact,Fact,,
542,It would help to cite more recent work that decreases detector dependence on NMS.,Request,Request.Edit,U-Neutral,Meaningful Comparison
543,"For example, ""Learning Non-Maximum Suppression"", Hosang, Benenson, Schiele, CVPR 2017 or ""Relation Networks for Object Detection"", by Hu et al, CVPR 2018 and references therein.",Other,Other,,
579,This paper proposes an extension to the continual learning framework using existing variational continual learning (VCL) as the base method.,Structuring,Structuring.Summary,,
580,"In particular, it proposes to use the weight of evidence (WE) (from Zintgraf et al 2017) for each task.",Structuring,Structuring.Summary,,
581,"Firstly, this WE can be used to visualize the learned model (as used in Zintgraf et. al. 2017).",Structuring,Structuring.Summary,,
582,The novelty of this paper is:,Structuring,Structuring.Heading,,
583,1. to use this WE from the current task to generate a silence map (by smoothing the WE) for the next task.,Structuring,Structuring.Summary,,
584,This is interpreted the learned the learned attention region.,Structuring,Structuring.Summary,,
585,Such an approach is named Interpretable COntinual Learning (ICL),Structuring,Structuring.Summary,,
586,2. The paper proposes a metric for the saliency map naming FSM which is an extension of existing metric SSR.,Structuring,Structuring.Summary,,
587,"The extension is to take pixel count to compute the area instead of using rectangular region area, as well as taking the distance between pixels into account.",Structuring,Structuring.Summary,,
588,This metric can be used to evaluate the level of catastrophic forgetting.,Structuring,Structuring.Summary,,
589,Pro:,Structuring,Structuring.Heading,,
590,"In general, the idea is very intuitive and make sense.",Evaluative,Evaluative,P-Positive,Motivation/Impact
591,The paper also demonstrates superior performance with the proposed method on continual learning on all classic tasks comparing with VCL and EWC.,Evaluative,Evaluative,P-Positive,Substance
592,The presentation is very easy to follow.,Evaluative,Evaluative,P-Positive,Clarity
593,It seems like a valid and flexible extension that can be used in other continual learning frameworks.,Evaluative,Evaluative,P-Positive,Motivation/Impact
595,The theoretical contribution is very limited.,Evaluative,Evaluative,N-Negative,Originality
596,The work is rather incremental from current state-of-the-art methods.,Evaluative,Evaluative,N-Negative,Originality
597,There should be a better discussion of related work on the topic.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
598,"The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
599,A general overview of related work in these directions are needed.,Request,Request.Edit,U-Neutral,Clarity
600,Other:,Structuring,Structuring.Heading,,
601,1. The paper should also consider more recently proposed evaluation metrics such as discussed in https://arxiv.org/pdf/1805.09733.pdf,Request,Request.Experiment,U-Neutral,Replicability
602,2. The author should try to avoid using yellow color in plots.,Request,Request.Edit,U-Neutral,Clarity
603,"The primary innovation of this paper seems focused towards increasing the generalization of GANs, while also maintaining convergence and preventing mode collapse.",Structuring,Structuring.Summary,,
604,"The authors first discuss common pitfalls concerning the generalization capability of discriminators, providing analytical underpinnings for their later experimental results.",Structuring,Structuring.Summary,,
605,"Specifically, they address the problem of gradient explosion in discriminators.",Structuring,Structuring.Summary,,
606,The authors then suggest that a zero-centered gradient penalty (0-GP) can be helpful in addressing this issue.,Structuring,Structuring.Summary,,
607,"0-GPs are regularly used in GANs, but the authors point out that the purpose is usually to  provide convergence, not to increase generalizability.",Structuring,Structuring.Summary,,
608,"Non-zero centered penalties can give a convergence guarantee but, the authors, assert, can allow overfitting.",Structuring,Structuring.Summary,,
609,A 0-GP can give the same guarantees but without allowing overfitting to occur.,Fact,Fact,,
610,"The authors then verify these assertions through experimentation on synthetic data, as well as MNIST and ImageNet.",Structuring,Structuring.Summary,,
611,My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?,Request,Request.Experiment,N-Negative,Substance
612,Some portion? It is not clear from reading. This would be a serious impediment to reproducibility.,Evaluative,Evaluative,N-Negative,Clarity
613,"All in all, however, the authors provide a convincing  combination of analysis and experimentation.",Evaluative,Evaluative,P-Positive,Substance
614,I believe this paper should be accepted into ICLR.,Fact,Fact,,
615,"Note: there is an error on page 9, in Figure 3.",Request,Request.Typo,U-Neutral,Substance
616,The paragraph explanation should list that the authors' 0-GP is figure 3(e).,,,,
617,They list (d) twice.,,,,
618,The authors proposed an AutoLoss controller that can learn to take actions of updating different parameters and using different loss functions.,Structuring,Structuring.Summary,,
620,1. Propose a unified framework for different loss objectives and parameters.,Evaluative,Evaluative,P-Positive,Originality
621,2. An interesting idea in meta learning for learning loss objectives/schedule.,Evaluative,Evaluative,P-Positive,Originality
623,"1. The formulation uses REINFORCE, which is often known with high variance.",Evaluative,Evaluative,N-Negative,Substance
624,Are the results averaged across different runs? Can you show the variance?,Request,Request.Edit,U-Neutral,Substance
625,It is hard to understand the results without discussing it.,Request,Request.Clarification,N-Negative,Clarity
626,The sample complexity should be also higher than traditional approaches.,Evaluative,Evaluative,U-Neutral,Soundness/Correctness
627,2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?,Request,Request.Experiment,N-Negative,Substance
628,3. Why do you set S=1 in the experiments? What’s the importance of S?,Request,Request.Explanation,U-Neutral,Substance
629,4. I think it is quite surprising the AutoLoss can resolve mode collapse in GANs.,Evaluative,Evaluative,U-Neutral,Soundness/Correctness
630,I think more analysis is needed to support this claim.,Request,Request.Explanation,U-Neutral,Substance
631,"5. The evaluation metric of multi-task MT is quite weird. Normally people report BLEU, whereas the authors use PPL.",Evaluative,Evaluative,N-Negative,Substance
632,"6. According to https://github.com/pfnet-research/chainer-gan-lib, I think the bested reported DCGAN results is not 6.16 on CIFAR-10 and people still found other tricks such as spectral-norm is needed to prevent mode-collapse.",Fact,Fact,,
634,1. The usage of footnote 2 is incorrect.,Request,Request.Typo,N-Negative,Clarity
635,"2. In references, some words should be capitalized properly such as gan->GAN.",Request,Request.Typo,U-Neutral,Clarity
636,This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function.,Structuring,Structuring.Summary,,
637,"They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost.",Structuring,Structuring.Summary,,
638,Their experimental results showed competitive performance to SGD/Adam on the same network architectures.,Evaluative,Evaluative,P-Positive,Substance
639,1.,Structuring,Structuring.Heading,,
640,Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function.,Evaluative,Evaluative,P-Positive,Substance
641,"While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
642,An appendix with more numerical comparisons on other loss functions might also be insightful.,Evaluative,Evaluative,N-Negative,Clarity
643,2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2).,Fact,Fact,,
644,In Table 2 I saw some optimizers end up with much lower test accuracy.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
645,Can the authors show the convergence plots of these methods (similar to Figure 2)?,Request,Request.Experiment,U-Neutral,Substance
646,"After reading the authors' response, I'm revising my score upwards from 5 to 6.",Other,Other,,
647,"The authors propose a defense against adversarial examples, that is inspired by ""non local means filtering"".",Structuring,Structuring.Summary,,
648,"The underlying assumption seems to be that, at feature level, adversarial examples manifest as IID noise in feature maps, which can be ""filtered away"" by using features from other images.",Structuring,Structuring.Summary,,
649,"While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.",Evaluative,Evaluative,N-Negative,Substance
650,Some examples of verifying this are:,Structuring,Structuring.Heading,,
651,1. How does varying the number of nearest neighbors change the network behavior?,Request,Request.Explanation,U-Neutral,Soundness/Correctness
652,"2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?",Request,Request.Explanation,U-Neutral,Soundness/Correctness
653,"3. Does just simple filtering of the feature map, say, by local averaging, perform equally well?",Request,Request.Clarification,U-Neutral,Soundness/Correctness
654,4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?,Request,Request.Explanation,U-Neutral,Soundness/Correctness
655,"Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training.",Fact,Fact,,
656,It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.,Fact,Fact,,
666,"The authors extend an existing approach to adaptive softmax classifiers used for the output component of neural language models into the input component, once again allowing tying between the embedding and softmax.",Structuring,Structuring.Quote,,
667,"This fills a significant gap in the language modeling architecture space, and the perplexity results bear out the advantages of combining adaptively-sized representations with weight tying.",Fact,Fact,,
668,"While the advance is in some sense fairly incremental, the centrality of unsupervised language modeling to modern deep NLP (ELMo, BERT, etc.) implies that perplexity improvements as large as this one may have meaningful downstream effects on performance on other tasks.",Fact,Fact,,
669,Some things I noticed:,Structuring,Structuring.Heading,,
670,"- One comparison that I believe is missing (I could be misreading the tables) is comparing directly to Merity et al.'s approach (adaptive softmax but fixed embedding/softmax dimension among the bands). Presumably you're faster, but is there a perplexity trade-off?",Evaluative,Evaluative,N-Negative,Meaningful Comparison
671,- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.,Request,Request.Edit,N-Negative,Clarity
672,- The loss by frequency-bin plots are really fantastic.,Evaluative,Evaluative,P-Positive,Substance
673,You could also try a scatterplot of log freq vs. average loss by individual word/BPE token.,Request,Request.Experiment,U-Neutral,Substance
674,- Do you have thoughts as to why full-softmax BPE is worse than adaptive softmax word level?,Request,Request.Explanation,N-Negative,Meaningful Comparison
675,That goes against the current (industry) conventional wisdom in machine translation and large-scale language modeling that BPE is solidly better than word-level approaches because it tackles the softmax bottleneck while also sharing morphological information between words.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
676,This paper studies the characteristics of representations and their roles in neural network expressiveness.,Structuring,Structuring.Summary,,
677,The results  are overall not very impressive.,Evaluative,Evaluative,N-Negative,Motivation/Impact
678,"1. There are many characteristics of representations such as scaling, permutation, covariance, correlation, sparsity, dead units, rank.",Fact,Fact,,
679,"The papers discusses some (not surprising) theoretical properties relating to scaling, permutation, covariance, correlation, while making less efforts on the more interesting characteristics  sparsity, dead units, rank, mutual information.",Evaluative,Evaluative,N-Negative,Substance
680,Only some heuristic results are obtained for them without rigorous theory.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
681,It would be better if these heuristic arguments can be formed as theorems as well.,Request,Request.Experiment,U-Neutral,Soundness/Correctness
682,"2. Probably the most interesting experimental finding of this paper is that the mutual information between z and output is constant, while the one between z and input strongly depends on the regularizers.",Structuring,Structuring.Summary,,
683,"That is, the dependence between z and y does not vary with regularizers but the one between z and x does.",Fact,Fact,,
684,Is this a coincidence or a general phenomenon? Is there a theoretical explanation?,Request,Request.Explanation,U-Neutral,Soundness/Correctness
685,3.,Structuring,Structuring.Heading,,
687,The role of auxiliary tasks is to improve the generalization performance of the principal task of interest.,Structuring,Structuring.Summary,,
688,"So far, hand-crafted auxiliary tasks are generated, tailored for a problem of interest.",Structuring,Structuring.Summary,,
689,"The current work addresses a meta-learning approach to automatically generate auxiliary tasks suited to the principal task, without human knowledge.",Structuring,Structuring.Summary,,
690,The key components of the method are: (1) meta-generator; (2) multi-task evaluator.,Structuring,Structuring.Summary,,
691,"These two models are trained using the gradient-based meta-learning technique (for instance, MAML).",Structuring,Structuring.Summary,,
692,"The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.",Evaluative,Evaluative,N-Negative,Replicability
694,"- To my best knowledge, the idea of applying the meta-learning to the automatic generation of auxiliary tasks is novel.",Evaluative,Evaluative,P-Positive,Originality
695,- The paper is well written and easy to read.,Evaluative,Evaluative,P-Positive,Clarity
696,"- The method nicely blends a few components such as self-supervised learning, meta-learning, auxiliary tasks into a single model to tackle the meta auxiliary learning.",Evaluative,Evaluative,P-Positive,Substance
697,Weakness:,Structuring,Structuring.Heading,,
698,- The performance gain is not substantial in experiments.,Evaluative,Evaluative,N-Negative,Substance
699,I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.,Request,Request.Experiment,N-Negative,Substance
700,You can refer to the state-of-the-arts performance on CIFAR.,,,,
701,"- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.",Evaluative,Evaluative,N-Negative,Substance
712,The privacy definition employed in this work is problematic.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
713,"The authors claim that ""Privacy can be quantified by the difficulty of reconstructing raw data via a generative model"".",Evaluative,Evaluative,N-Negative,Soundness/Correctness
714,This is not justified sufficiently.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
715,Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.,Evaluative,Evaluative,N-Negative,Substance
716,The proposed method is not appropriately compared with the other methods in experiments.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
717,In Fig. 3 the author claim that the proposed method dominates the other methods in terms of privacy and utility but this is not correct.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
718,"At the specific point that the proposed method is evaluated with MNIST and Sound, it achieves better utility and better ""privacy"".",Evaluative,Evaluative,N-Negative,Soundness/Correctness
719,"However, the Pareto front of the proposed method is concentrated on a specific point.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
720,"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
721,"In this sense, the proposed method is not comparable with ""noisy"".",Evaluative,Evaluative,N-Negative,Meaningful Comparison
722,"In my understanding, this concentration occurs because the range of \lambda is inappropriately set.",Evaluative,Evaluative,N-Negative,Substance
723,This kind of regularization parameter should be exponentially varied so that the privacy-utility Pareto front covers a wide range.,Evaluative,Evaluative,N-Negative,Substance
724,--,Structuring,Structuring.Heading,,
726,"In Eq. 1, the utility is evaluated as the probability Yi=Yi'.",Evaluative,Evaluative,N-Negative,Substance
727,What randomness is considered in this probability?,Request,Request.Clarification,N-Negative,Substance
728,"In Eq 2, privacy is defined as maxmin of |Ii - Ii'|.",Evaluative,Evaluative,N-Negative,Substance
729,Do you mean privacy guaranteed by the proposed method is different for each data? This should be defined as expectation over T or max over T.,Request,Request.Clarification,N-Negative,Substance
730,"In page 4. ""The reason we choose this specific architecture is that an exactly reversed mode is intuitively the mode powerful adversarial against the Encoder."" I could not find any justification for this setting. Why ""exactly reversed mode"" can be the most powerful adversary? What is an exactly reversed mode?",Request,Request.Explanation,N-Negative,Substance
731,Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.,Evaluative,Evaluative,N-Negative,Substance
732,The resulting model would thus be highly affected by the setting of n and k.,Evaluative,Evaluative,N-Negative,Substance
733,How can you choose k and n?,Request,Request.Explanation,N-Negative,Substance
734,"This paper proposed a general framework, DeepTwist, for model compression.",Structuring,Structuring.Heading,,
735,The so-called weight distortion procedure is added into the training every several epochs.,Structuring,Structuring.Heading,,
736,Three applications are shown to demonstrate the usage of the proposed approach.,Structuring,Structuring.Heading,,
737,"Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.",Evaluative,Evaluative,N-Negative,Originality
738,See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.,Evaluative,Evaluative,N-Negative,Originality
739,"Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient.",Fact,Fact,,
740,"Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function.",Fact,Fact,,
741,"Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework.",Fact,Fact,,
742,Then proximal function can be applied directly after Distortion Step to project the solutions.,Fact,Fact,,
743,"In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.",Evaluative,Evaluative,N-Negative,Originality
744,"Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.",Fact,Fact,,
745,"PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
746,"The paper proposes a neural net implementation of counterfactual regret minimization where 2 networks are learnt, one for estimating the cumulative regret (used to derive the immediate policy) and the other one for estimating a cumulative mixture policy.",Structuring,Structuring.Summary,,
747,In addition the authors also propose an original MC sampling strategy which generalize outcome and external sampling strategies.,Structuring,Structuring.Summary,,
748,The paper is interesting and easy to read. My main concern is about the feasibility of using a neural networks to learn cumulative quantities.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
749,The problem of learning cumulative quantities in a neural net is that we need two types of samples:,Fact,Fact,,
750,"- the positive examples: samples from which we train our network to predict its own value plus the new quantity,",Fact,Fact,,
751,but also:,Fact,Fact,,
752,"- the negative examples: samples from which we should train the network to predict 0, or any desired initial value.",Fact,Fact,,
753,"However in the approach proposed here, the negative examples are missing.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
754,So the network is not trained to predict 0 (or any initial values) for a newly encountered state.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
755,"And since neural networks generalize (very well...) to states that have not been sampled yet, the network would predict an arbitrary values in states that are visited for the first time.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
756,"For example the network predicting the cumulative regret may generalize to large values at newly visited states, instead of predicting a value close to 0.",Evaluative,Evaluative,N-Negative,Replicability
757,"The resulting policy can be arbitrarily different from an exploratory (close to uniform) policy, which would be required to minimize regret from a newly visited state.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
758,"Then, even if that state is visited frequently in the future, this error in prediction will never be corrected because the target cumulative regret depends on the previous prediction.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
759,So there is no guarantee this algorithm will minimise the overall regret.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
760,"This is a well known problem for exploration (regret minimization) in reinforcement learning as well (see e.g. the work on pseudo-counts [Bellemare et al., 2016, Unifying Count-Based Exploration and Intrinsic Motivation] as one possible approach based on learning a density model).",Fact,Fact,,
761,"Here, maybe a way to alleviate this problem would be to generate negative samples (where the network would be trained to predict low cumulative values) by following a different (possibly more exploratory) policy.",Fact,Fact,,
763,- It does not seem necessary to predict cumulative mixture policies (ASN network).,Evaluative,Evaluative,N-Negative,Soundness/Correctness
764,One could train a mixture policy network to directly predict the current policy along trajectories generated by MC.,Fact,Fact,,
765,"Since the samples would be generated according to the current policy \sigma_t, any information nodes I_i would be sampled proportionally to \pi^{\sigma^t}_i(I_i), which is the same probability as in the definition of the mixture policy (4).",Fact,Fact,,
766,This would remove the need to learn a cumulative quantity.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
767,"- It would help to have a discussion about how to implement (7), for example do you use a target network to keep the target value R_t+r_t fixed for several steps?",Request,Request.Explanation,N-Negative,Replicability
768,- It is not clear how the initialisation (10) is implemented.,Evaluative,Evaluative,N-Negative,Replicability
769,"Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?",Request,Request.Explanation,N-Negative,Replicability
770,Summary,Structuring,Structuring.Heading,,
771,-------,Structuring,Structuring.Heading,,
772,This paper describes a model for musical timbre transfer which builds on recent developments in domain- and style transfer.,Structuring,Structuring.Summary,,
773,"The proposed method is designed to be many-to-many, and uses a single pair of encoders and decoders with additional conditioning inputs to select the source and target domains (timbres).",Structuring,Structuring.Summary,,
774,"The method is evaluated on a collection of individual note-level recordings from 12 instruments, grouped into four families which are used as domains.",Structuring,Structuring.Summary,,
775,"The method is compared against the UNIT model under a variety of training conditions, and evaluated for within-domain reconstruction and transfer accuracy as measured by maximum mean discrepancy.",Structuring,Structuring.Summary,,
776,"The proposed model seems to improve on the transfer accuracy, with a slight hit to reconstruction accuracy.",Structuring,Structuring.Summary,,
777,Qualitative investigation demonstrates that the learned representation can approximate several coarse spectral descriptors of the target domains.,Structuring,Structuring.Summary,,
778,High-level comments,Structuring,Structuring.Heading,,
779,-------------------,Structuring,Structuring.Heading,,
780,"Overall, this paper is well written, and the various design choices seem well-motivated.",Evaluative,Evaluative,P-Positive,Substance
781,"The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
782,"Specifically, the authors introduced an extra input (control) to encode the pitch class and octave information during encoding.",Structuring,Structuring.Heading,,
783,"I infer that this was necessary to achieve good performance, but it would be instructive to see the results without this additional input, since it does in a sense constitute a form of supervision, and therefore limits the types of training data which can be used.",Request,Request.Experiment,P-Positive,Substance
784,"While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
785,Some of this comes down to incomplete definition of the metrics (see detailed comments below).,Structuring,Structuring.Heading,,
786,"However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
787,"The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?",Request,Request.Clarification,N-Negative,Soundness/Correctness
788,What is the criteria for bolding here?,Request,Request.Clarification,U-Neutral,Soundness/Correctness
789,"It would be helpful if these scores could be calibrated in some way, e.g., with reference to",Request,Request.Experiment,U-Neutral,Soundness/Correctness
790,MMD/KNN scores of random partitions of the target domain samples.,,,,
791,"Since the authors do additional information here for each sample (notes), it would be possible to pair generated and real examples by instrument and note, rather than (in addition to) unsupervised, feature-space pairing by MMD.",Request,Request.Experiment,P-Positive,Soundness/Correctness
792,"This could provide a slightly stronger version of the comparison in Figure 3, which shows that the overall distribution of spectral centroids is approximated by transfer, but does not demonstrate per-sample correspondence.",Request,Request.Experiment,P-Positive,Meaningful Comparison
793,Detailed comments,Structuring,Structuring.Heading,,
794,-----------------,Structuring,Structuring.Heading,,
795,"At several points in the manuscript, the authors refer to ""invertible"" representations (e.g., page 4, just after eq. 1), but it seems like what they mean is approximately invertible or decodable.",Request,Request.Edit,U-Neutral,Clarity
796,It would be better if the authors were a little more careful in their use of terminology here.,Request,Request.Edit,N-Negative,Clarity
797,"In the definition of the RBF kernel (page 4), why is there a summation?",Request,Request.Clarification,U-Neutral,Clarity
798,What does this index? How are the kernel bandwidths defined?,Request,Request.Clarification,U-Neutral,Clarity
799,"How exactly are reconstruction errors calculated: using the NSGT magnitude representation, or after resynthesis in the time domain?",Request,Request.Explanation,U-Neutral,Substance
800,"The paper explores how the architecture, smoothness of the decision boundary and test accuracy of a model impacts the transferability of examples produced from it.",Structuring,Structuring.Summary,,
801,"The paper provides a couple of novel insights, such as the asymmetry when transferring adversarial examples from one model to another.",Structuring,Structuring.Summary,,
802,"In addition, a novel method is proposed to enhance the transferability of adversarial examples from any model, through using smoothed gradients.",Structuring,Structuring.Summary,,
803,"The experiments seem to show that the effect is rather large, and also makes the examples more robust to other transformations such as JPEG compression.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
804,"Overall, these are interesting insights that could lead to further developments in making models more robust to adversarial examples.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
805,"In particular, deriving adversarial examples that are both transferable and resilient to certain usual image transformations shows that the scope of the issue with adversarial examples may be even greater than what is understood today.",Evaluative,Evaluative,P-Positive,Substance
806,The paper is rather clear.,Evaluative,Evaluative,P-Positive,Clarity
807,"Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.",Evaluative,Evaluative,N-Negative,Clarity
808,Some examples (there are way too many to report them all):,Structuring,Structuring.Heading,,
809,"- ""Transfer-based attackS ... since they ...*",Request,Request.Typo,U-Neutral,Clarity
810,"- ""of adversarial exampleS ...""",Request,Request.Typo,U-Neutral,Clarity
811,"- ""from model A can transfer to model B""",Request,Request.Typo,U-Neutral,Clarity
812,"- ""less transferable than *those from* a shallow model""?",Request,Request.Typo,U-Neutral,Clarity
813,"- ""investigations, We "": don't capitalize",Request,Request.Typo,U-Neutral,Clarity
814,"- ""the averaging *has* a smoothing effect""",Request,Request.Typo,U-Neutral,Clarity
815,"- ""our motivation are""",Request,Request.Typo,U-Neutral,Clarity
816,"- ""contributed it to""",Request,Request.Typo,U-Neutral,Clarity
817,"- ""available *to the* adversary""",Request,Request.Typo,U-Neutral,Clarity
818,"- ""crafting adversarial perturbationS""",Request,Request.Typo,U-Neutral,Clarity
819,"- ""directly evaluation""",Request,Request.Typo,U-Neutral,Clarity
820,"- ""be fixed 100""",Request,Request.Typo,U-Neutral,Clarity
822,- Transferability and robustness of adversarial examples is a very important problem,Evaluative,Evaluative,P-Positive,Motivation/Impact
823,"- Interesting insights, esp. the construction and evaluation of examples that are more resilient to certain image transformations",Evaluative,Evaluative,P-Positive,Substance
824,- Experimental results are convincing,Evaluative,Evaluative,P-Positive,Soundness/Correctness
826,- Contribution overall may be a bit limited,Evaluative,Evaluative,N-Negative,Motivation/Impact
827,- Grammatical errors and odd formulations all over the place,Evaluative,Evaluative,N-Negative,Motivation/Impact
828,* Summary,Structuring,Structuring.Summary,,
829,The paper proposes an improved method for computing derivatives of the expectation.,Structuring,Structuring.Summary,,
830,Such problems arises with many probabilistic models with noises or latent variables.,Structuring,Structuring.Heading,,
831,"The paper proposes a new gradient estimator of low variance applicable in certain scenarios, in particular it allows training of generative models in which observations and/or latent variables are discrete.",Structuring,Structuring.Summary,,
832,"The submission clearly improves the state-of-the-art, experimentally demonstrates the method on several problems comparing with the alternative techniques.",Evaluative,Evaluative,P-Positive,Substance
833,"In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.",Evaluative,Evaluative,N-Negative,Substance
834,The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.,Evaluative,Evaluative,N-Negative,Clarity
835,"In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.",Evaluative,Evaluative,N-Negative,Motivation/Impact
836,"It also contains lots of additional technical details and experiments in the appendix, which I unfortunately did not review.",Evaluative,Evaluative,U-Neutral,Other
838,Clarity,Structuring,Structuring.Heading,,
839,In the abstract the paper promises more than it delivers.,Evaluative,Evaluative,N-Negative,Substance
840,Many problems can be cast as optimizing an expectation-based objective.,Evaluative,Evaluative,N-Negative,Substance
841,The result does not at all apply to all of them.,Evaluative,Evaluative,N-Negative,Motivation/Impact
842,"The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
843,Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).,Evaluative,Evaluative,N-Negative,Soundness/Correctness
844,This very much limits the utility of the method.,Evaluative,Evaluative,N-Negative,Motivation/Impact
845,"In particular it is not applicable to learning e.g. sigmoid belief networks [Neal, 92] (with conditional Bernoulli units) and many other problems.",Evaluative,Evaluative,N-Negative,Motivation/Impact
846,“reparametrizable distributions”,Other,Other,,
847,"A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
848,Because of the above many discussions about discrete vs. continuous variables are missleading.,Evaluative,Evaluative,N-Negative,Clarity
849,Section 2.,Other,Other,,
850,The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
851,I find the background on ELBO and GANs unnecessary occluding the clarity at this point.,Evaluative,Evaluative,N-Negative,Clarity
852,"For the purpose of introduction, it might be better to give examples of expectation objectives such as:",Request,Request.Explanation,U-Neutral,Motivation/Impact
853,"- dropout: q is the distribution of NN outputs given the input image and integrating out latent dropout noises, gamma are parameters of this NN.",Request,Request.Explanation,U-Neutral,Motivation/Impact
854,"- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.",Request,Request.Explanation,N-Negative,Motivation/Impact
855,- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.,Request,Request.Explanation,N-Negative,Motivation/Impact
856,"Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).",Request,Request.Explanation,N-Negative,Motivation/Impact
857,Section 3.,Other,Other,,
858,"Contrary to the discussion, there are examples of non-negative distributions to which the reparameterization trick can be applied, including log-Normal and Gamma distributions.",Evaluative,Evaluative,U-Neutral,Motivation/Impact
859,Method:,Other,Other,,
860,"In the case when Rep trick is applicable, is it identical to GO?",Evaluative,Evaluative,U-Neutral,Substance
861,The difference seems to be only in that the mapping tau may be different from Q^-1.,Evaluative,Evaluative,U-Neutral,Soundness/Correctness
862,"However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.",Evaluative,Evaluative,N-Negative,Substance
863,"Yet, in Fig.1 some difference is observed between the methods, why is that so?",Evaluative,Evaluative,N-Negative,Substance
864,Sec 7.1,Other,Other,,
865,“We adopt the sticking approach hereafter”. Does it mean it is applied with all experiments with GO?,Evaluative,Evaluative,U-Neutral,Substance
866,* Related Work,Other,Other,,
867,The state of the art allows combining differentiable and non-differentiable pieces of computation:,Other,Other,,
868,"[Schulman, J., Heess, N., Weber, T., Abbeel, P.: Gradient estimation using stochastic computation graphs.]",Other,Other,,
869,I believe it should be discussed in related work.,Request,Request.Edit,U-Neutral,Substance
870,Limitations / where the proposed method brings an improvement should be highlighted.,Request,Request.Experiment,U-Neutral,Motivation/Impact
871,* Technical Correctness,Other,Other,,
872,"Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.",Request,Request.Edit,N-Negative,Soundness/Correctness
873,Equation (7) (integration by parts) holds only with some additional requires on f.,Evaluative,Evaluative,U-Neutral,Soundness/Correctness
874,Theorem 1 does not take account for the above conditions.,Evaluative,Evaluative,N-Negative,Substance
875,"This paper combines the global and local stability prediction and tries to get interpretable results using the stethoscope design, which is actually a weighted subbranch for the main branch.",Structuring,Structuring.Summary,,
876,There are several concerns regarding the proposed framework.,Structuring,Structuring.Heading,,
877,1) How to choose \lambda?,Request,Request.Clarification,U-Neutral,Replicability
878,A better design could be a learnable \lambda.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
879,"Instead of just one scalar value, it could be better to learn a map of \lambdas, which indicates the distribution of local stability and how it is related to global stability.",Request,Request.Experiment,U-Neutral,Substance
880,The visualization of the \lambda map might be more interpretable for understanding the stability prediction.,Request,Request.Experiment,U-Neutral,Substance
881,"2) The global stability prediction does not have a consistent correlation with the local stability prediction, as shown by the easy and hard examples.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
882,This complex relationship will confuse the network during the training.,Fact,Fact,,
883,"That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
884,This is hard to provide a meaningful interpretation of the task.,Evaluative,Evaluative,N-Negative,Motivation/Impact
885,"This paper presents a novel approach to bundle adjustment, where traditional geometric optimization is paired with deep learning.",Structuring,Structuring.Summary,,
886,"Specifically, a CNN computes both a multi-scale feature pyramid and a depth prediction, expressed as a linear combination of ""depth bases"".",Structuring,Structuring.Summary,,
887,"These values are used to define a dense re-projection error over the images, akin to that of dense or semi-dense methods.",Structuring,Structuring.Summary,,
888,"Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM).",Structuring,Structuring.Summary,,
889,"By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.",Structuring,Structuring.Summary,,
890,"The paper is clear, well organized, well written and easy to follow.",Evaluative,Evaluative,P-Positive,Clarity
891,"Even if the idea of joining BA / SfM and deep learning is not new, the authors propose an interesting novel formulation.",Evaluative,Evaluative,P-Positive,Originality
892,"In particular, being able to train the CNN with a supervision signal coming directly from the same geometric optimization process that will be used at test time allows it to produce features that  will make the optimization smoother and the convergence easier.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
893,The experiments are quite convincing and seem to clearly support the efficacy of the proposed method.,Evaluative,Evaluative,P-Positive,Substance
894,"I don't really have any major criticism, but I would like to hear the authors' opinions on the following two points:",Structuring,Structuring.Heading,,
895,"1) In page 5, the authors write ""learns to predict a better damping factor lambda, which gaurantees that the optimziation will converged to a better solution within limited iterations"".",Fact,Fact,,
896,I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.,Request,Request.Explanation,N-Negative,Replicability
897,"The word ""guarantee"" usually implies that the effect can be somehow mathematically proved, which is not done in the paper.",Request,Request.Explanation,N-Negative,Replicability
898,"2) As far as I can understand, once the networks are learned, possibly on pairs of images due to GPU memory limitations, the proposed approach can be easily applied to sets of images of any size, as the features and depth predictions can be pre-computed and stored in main system memory.",Fact,Fact,,
899,"Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.",Request,Request.Explanation,N-Negative,Substance
900,The paper suggests a new regularization technique which can be added on top of those used in AWD-LSTM of Merity et al. (2017) with little overhead.,Structuring,Structuring.Summary,,
901,This is a well-written paper with a clear structure.,Evaluative,Evaluative,P-Positive,Clarity
902,"The experiments are presented in a clear and understandable fashion, and the evaluation seems thorough.",Evaluative,Evaluative,P-Positive,Substance
903,"The methodology seems sound, and the authors present the reader with all the information needed to replicate the experiments.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
904,I would only suggest evaluating this technique on AWD-LSTM-MoS of Yang et al. (2017) to get a more complete picture.,Request,Request.Experiment,P-Positive,Meaningful Comparison
905,References,Other,Other,,
906,"- Merity, S., Keskar, N.S. and Socher, R., 2017. Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182.",,,,
907,"- Yang, Z., Dai, Z., Salakhutdinov, R. and Cohen, W.W., 2017. Breaking the softmax bottleneck: A high-rank RNN language model.",,,,
908,arXiv preprint arXiv:1711.03953.,,,,
909,This paper used the concept based on channel deficiency to derive a variational bound similar to variational information bottleneck.,Structuring,Structuring.Summary,,
910,Theoretical analysis shows that this bound is an lower bound on the VIB objective.,Fact,Fact,,
911,The empirical analysis shows it outperforms VIB in some sense.,Fact,Fact,,
912,I think this paper's contribution is rather theoretical than practical.,Evaluative,Evaluative,N-Negative,Substance
913,The experiments section can be improved in the following aspect:,Structuring,Structuring.Heading,,
914,-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines,Request,Request.Experiment,N-Negative,Clarity
915,- I(Z;Y) vs I(Z;X) graph is typically used in a VIB setting.,Fact,Fact,,
916,"In the paper's variational deficiency setting, although plotting I(Z;Y) vs I(Z;X) is necessary, it would be also helpful for the authors' to plot Deficiency vs I(Z;X), because this is what new objective is trading-off.",Request,Request.Experiment,P-Positive,Substance
917,"- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.",Evaluative,Evaluative,N-Negative,Clarity
918,- How do the paper estimate I(Z;Y) and I(Z;X) for plotting these figures? Does the paper use lower bound or some estimators? It should be made clear in the paper since these are non-trivial estimations.,Request,Request.Clarification,N-Negative,Substance
919,"Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:",Evaluative,Evaluative,N-Negative,Originality
920,"- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016",Social,Social,,
921,It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.,Evaluative,Evaluative,N-Negative,Other
922,It would also be helpful for the authors to do a comparison or connection section with this paper.,Request,Request.Experiment,P-Positive,Substance
923,"I like the paper in general, but given it still has some space for improvement, I would keep my decision as boarder line for now.",Evaluative,Evaluative,U-Neutral,Substance
924,The paper focuses on the stability prediction task on the ShapeStacks dataset.,Structuring,Structuring.Summary,,
925,"Specifically, the paper creates a new extension to the dataset, and it proposes the use of ""Neural Stethoscopes"" framework to analyze deep neural nets' physical reasoning of local stability v.s. global stability.",Structuring,Structuring.Summary,,
926,It is shown in the paper neural nets tend to be misled by local stability when the task is to predict global stability.,Structuring,Structuring.Summary,,
927,Then the paper utilizes the proposed framework to de-bias the misleading correlation to achieve a state-of-the-art on the dataset.,Evaluative,Evaluative,P-Positive,Substance
928,The paper is very well-written and easy to follow.,Evaluative,Evaluative,P-Positive,Clarity
929,The main idea is simple and the experiments are detailed.,Evaluative,Evaluative,P-Positive,Substance
930,"Specifically on the task of stability prediction, it is quite interesting to know that neural nets can be misled by visual cues (local stability).",Evaluative,Evaluative,P-Positive,Motivation/Impact
931,"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.",Evaluative,Evaluative,N-Negative,Motivation/Impact
954,This paper provides some insights on influence of data distribution on robustness of adversarial training.,Structuring,Structuring.Summary,,
955,The paper demonstrates through a number of analysis that the distance between the training an test data sets plays an important role on the effectiveness of adversarial training.,Structuring,Structuring.Summary,,
956,"To show the latter, the paper proposes an approach to measure the distance between the two data sets using combination of nonlinear projection (e.g. t-SNE), KDE, and K-L divergence.",Structuring,Structuring.Summary,,
957,"The paper also shows that under simple transformation to the test dataset (e.g. scaling), performance of adversarial training reduces significantly due to the large gap between training and test data set.",Structuring,Structuring.Summary,,
958,This tends to impact high dimensional data sets more than low dimensional data sets since it is much harder to cover the whole ground truth data distribution in the training dataset.,Structuring,Structuring.Summary,,
960,- Provides insights on why adversarial training is less effective on some datasets.,Evaluative,Evaluative,P-Positive,Motivation/Impact
961,- Proposes a metric that seems to strongly correlate with the effectiveness of adversarial training.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
963,- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.,Evaluative,Evaluative,N-Negative,Substance
964,"- The marketing phrase ""the blind-spot attach"" falls short in delivering what one may expect from the paper after reading it.",Evaluative,Evaluative,N-Negative,Clarity
965,The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.,Evaluative,Evaluative,N-Negative,Substance
966,"For some dataset, this is beyond a spot, it could actually be huge portion of the input space!",Fact,Fact,,
967,Minor comments:,Structuring,Structuring.Heading,,
968,- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
969,"Though the paper is not suggesting that, it would help to clarify it in the paper.",Request,Request.Edit,U-Neutral,Clarity
970,"Furthermore, it would help if the paper elaborates why the distance between the test and training dataset is smaller in an adversarially trained network compared to a naturally trained network.",Request,Request.Edit,U-Neutral,Clarity
971,- Are the results in Table 1 for an adversarially trained network or a naturally trained network?,Request,Request.Clarification,U-Neutral,Clarity
972,"Either way, it could be also interesting to see the average K-L divergence between an adversarially and a naturally trained network on the same dataset.",Fact,Fact,,
973,- Please provide more visualization similarly to those shown in Fig 4.,Request,Request.Edit,U-Neutral,Clarity
974,"The authors consider the scenario of two agents, a demonstrator acting in an environment to achieve a goal, and a learner, which can also interact with the environment, but whose goal is to learn the demonstrator’s policy by carrying out actions eliciting strong changes in the demonstrator’s trajectory.",Structuring,Structuring.Summary,,
975,"The former is implemented as imitation learning, i.e. policy learning, the latter as curiosity driven RL.",Structuring,Structuring.Summary,,
976,"The authors are encouraged to review some of the related literature on optimal teaching, which also has developed a rich set of approaches to agent modeling, e.g. the work by Patrick Shafto.",Request,Request.Edit,U-Neutral,Meaningful Comparison
977,It may also be relevant to think about the relationship to active learning in IRL.,Evaluative,Evaluative,U-Neutral,Meaningful Comparison
978,I am not sure whether I would be able to implement and reproduce the presented work on the basis of the current manuscript including the appendix.,Evaluative,Evaluative,N-Negative,Replicability
979,It would be very helpful for the community to be able to do so.,Evaluative,Evaluative,P-Positive,Replicability
980,"E.g., details on the the training of the demonstrators, their reward functions, and the behavior tracker.",Evaluative,Evaluative,U-Neutral,Replicability
981,"Particularly the ""fusion"" module remains extremely unclear.",Evaluative,Evaluative,N-Negative,Replicability
982,"Overall, this is a nice paper, despite the fact that the example domains and problems considered are engineered strongly to allow for the proposed algorithm to be useful.",Evaluative,Evaluative,U-Neutral,Substance
983,"Particularly for the claim of generalization to different environments, the details are all in the engineering of the particular grid world tasks, how they relate to each other and the sate representation used for the demonstrator s_d.",Evaluative,Evaluative,U-Neutral,Substance
984,"I am not sure why it was submitted to ICLR and not the Annual Meeting of the Cognitive Science Society, though.",Other,Other,,
985,Minor points:,Structuring,Structuring.Heading,,
986,“differs from this in two folds”,Request,Request.Typo,U-Neutral,Clarity
987,“by generate queries”,Request,Request.Typo,U-Neutral,Clarity
988,This manuscript applies transfer learning for protein surface prediction.,Structuring,Structuring.Summary,,
989,The problem is important and  the idea is novel and interesting.,Evaluative,Evaluative,P-Positive,Originality
990,"However, the  transfer learning model is unclear.",Evaluative,Evaluative,N-Negative,Clarity
991,Pros:  interesting and novel idea,Evaluative,Evaluative,P-Positive,Originality
992,"Cons:  unclear transfer learning model, insufficient experiments.",Request,Request.Experiment,N-Negative,Clarity|Substance
993,"Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.",Request,Request.Clarification,N-Negative,Substance
994,It is unknown the used model is a new model or existing model.,Evaluative,Evaluative,N-Negative,Clarity
995,"Besides, in the experiments, the proposed method is not compared to other transfer learning methods.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
996,"Thus, the evidence of the experiments is not enough.",Request,Request.Experiment,N-Negative,Substance
998,This paper uses siamese networks to define a discriminative function for predicting protein-protein interaction interfaces.,Structuring,Structuring.Summary,,
999,They show improvements in predictive performance over some other recent deep learning methods.,Fact,Fact,,
1000,"The work is more suitable for a bioinformatics audience though, as the bigger contribution is on the particular application, rather than the model / method itself.",Evaluative,Evaluative,U-Neutral,Other
1001,Novelty:,Structuring,Structuring.Heading,,
1002,The main contribution of this paper is the representation of the protein interaction data in the input layer of the CNN,Evaluative,Evaluative,U-Neutral,Originality
1003,Clarity:,Structuring,Structuring.Heading,,
1004,"- The paper is well written, with ample background into the problem.",Evaluative,Evaluative,P-Positive,Clarity
1005,Significance:,Structuring,Structuring.Heading,,
1006,- Their method improves over prior deep learning approaches to this problem.,Evaluative,Evaluative,P-Positive,Substance
1007,"However, the results are a bit misleading in their reporting of the std error.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1008,They should try different train/test splits and report the performance.,Request,Request.Experiment,U-Neutral,Substance
1009,- This is an interesting application paper and would be of interest to computational biologists and potentially some other members of the ICLR community,Evaluative,Evaluative,U-Neutral,Motivation/Impact
1010,- Protein conformation information is not required by their method,Evaluative,Evaluative,N-Negative,Substance
1011,Comments:,Structuring,Structuring.Heading,,
1012,"- The authors should include citations and motivation for some of their choices (what sequence identity is used, what cut-offs are used etc)",Request,Request.Edit,U-Neutral,Meaningful Comparison
1013,-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred,Request,Request.Edit,U-Neutral,Meaningful Comparison
1014,- The authors use a balanced ratio of positive and negative examples.,Fact,Fact,,
1015,The true distribution of interacting residues is not balanced -- there are several orders of magnitude more non-interacting residues than interacting ones.,Evaluative,Evaluative,N-Negative,Substance
1016,Can they show performance at various ratios of positive:negative examples?,Request,Request.Experiment,U-Neutral,Substance
1017,"In case there is a consistent improvement over prior methods, then this would be a clear winner",Evaluative,Evaluative,P-Positive,Originality
1046,This paper proposes a self-auxiliary-training method that aims to improve the generalization performance of simple supervised learning.,Structuring,Structuring.Summary,,
1047,"The basic idea is to train the classification network to predict fine-level auxiliary labels in addition to the ground-truth coarse label, where the auxiliary labels used in training is generated by a generator network.",Structuring,Structuring.Summary,,
1048,"During training, the classification network and the generator network are alternatively updated, and the update of the latter aims to maximize the improvement of the former after using the generated auxiliary label for training.",Structuring,Structuring.Summary,,
1049,The method requires a class hierarchy in advance to define the binary mask applied to the output layer for auxiliary class prediction.,Structuring,Structuring.Summary,,
1050,A KL divergence term is attached to the optimization objective to avoid generating trivial and collapsing auxiliary classes.,Structuring,Structuring.Summary,,
1052,1) The main idea is simple and easy to understand.,Evaluative,Evaluative,P-Positive,Clarity
1053,"2) It discusses the class collapsing problem in generating pseudo (auxiliary) labels and provides a reasonable solution, i.e., using KL divergence as regularization.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
1054,3) Uses several visualizations to show experimental results.,Evaluative,Evaluative,P-Positive,Substance
1056,"1) The problem it aims to solve is neither multi-task learning nor meta-learning: it tries to solve a supervised classification problem defined on principle classes, with the help of simultaneously predicting/generating auxiliary class labels.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1057,"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.",Evaluative,Evaluative,N-Negative,Substance
1058,"This is not correct: in meta-learning, each task is a subset of classes drawn from a ground set of classes, and different tasks are independently sampled.",,,,
1059,"In addition, the classification models for different tasks are independent, though their training might be related by a meta-learner.",Fact,Fact,,
1060,"Hence, the claims in multiple places of this paper and the names for the two networks are misleading.",Evaluative,Evaluative,N-Negative,Clarity
1061,"2) At the end of Page 4, the authors show that the update of the generator only depends on the improvement of the classifier after using the auxiliary label for training.",Fact,Fact,,
1062,"In fact, the optimal auxiliary labels minimizing the objective is the ground truth label for principle classes.",Fact,Fact,,
1063,This results in the class collapsing problem observed by the authors.,Fact,Fact,,
1064,"The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.",Evaluative,Evaluative,N-Negative,Substance
1065,"In other words, the auxiliary labels for a specific principle class are very possible to be multiple noisy copies of the principal label with random perturbations.",,,,
1066,So it is not convincing to me that the auxiliary labels generated by the generator can be really helpful.,,,,
1067,"My conjecture is that the observed improvements are mainly due to the softness of the auxiliary labels, which has been proved by model compression/knowledge distillation and recent ""born-again neural networks"".",Fact,Fact,,
1068,"To verify this, the authors might need to compare the results with those methods (which use the generated soft probability of ground truth classes for training), and the ""random-noisy copies of soft principle label"" mentioned above.",Request,Request.Experiment,U-Neutral,Substance
1069,"3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).",Evaluative,Evaluative,N-Negative,Meaningful Comparison
1070,"A successful idea of self-supervised learning is to use the output feature map of the trained classification network to generate auxiliary training signals, since it provides extra information about the learned distance beyond the ground-truth labels.",Fact,Fact,,
1071,"The authors might want to compare to ""Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.",Request,Request.Experiment,U-Neutral,Meaningful Comparison
1072,"Deep Clustering for Unsupervised Learning of Visual Features. ECCV 2018."" and ""Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. ICCV 2017."" Moreover, since the method is not a meta-learning approach for few-shot learning, it is not fair and also not appropriate to compare with Prototypical Network.",Other,Other,,
1073,"4) Although the paper claims that the ground truth fine labels are not required, it requires a class hierarchy, which in the experiments are provided by the dataset and defined between true coarse and fine classes.",Fact,Fact,,
1074,"In practice, such hierarchy might be much harder to achieve than the primary (coarse) labels, and might be as costly to obtain as the true fine-class labels.",Evaluative,Evaluative,N-Negative,Substance
1075,This weakens the feasibility of the proposed method.,,,,
1076,"5) The experiments only test the proposed method on CIFAR100 and CIFAR10, which has at most 100 fine classes.",Fact,Fact,,
1077,"It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.",Evaluative,Evaluative,N-Negative,Replicability
1079,Some important equations in the paper should be numbered.,Request,Request.Typo,U-Neutral,Substance
1080,This paper introduces deficiency bottleneck for learning a data representation and represent  complicated channels using simpler ones.,Structuring,Structuring.Summary,,
1081,This problem has a natural variational form that can be easily implemented from VIB.,Structuring,Structuring.Summary,,
1082,Experiments show good performance comparing to VIB.,Evaluative,Evaluative,P-Positive,Substance
1083,This paper is well-written and easy to read.,Evaluative,Evaluative,P-Positive,Clarity
1084,The idea using KL divergence creating a deficiency channel to learn data representation is very natural.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1085,"It is interesting that this formulation could be understood as minimizing a regularized risk gap of statistical decision problems, which justifies the usage of deficiency bottleneck (eq.9).",Structuring,Structuring.Quote,,
1086,"My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
1087,"However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
1088,"For example, how does the method compare with (variants of) Variational Autoencoder?",Request,Request.Explanation,U-Neutral,Meaningful Comparison
1089,A discussion on this or some empirical evaluations would be nice.,Request,Request.Experiment,U-Neutral,Meaningful Comparison
1090,This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.,Structuring,Structuring.Summary,,
1091,Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances.,Structuring,Structuring.Summary,,
1092,This is an interesting work.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1093,The merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning.,Evaluative,Evaluative,P-Positive,Originality
1094,(2) The proposed approach produced effective empirical results.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
1095,The drawbacks  of the work include the following: (1) There is not much technical contribution.,Evaluative,Evaluative,N-Negative,Substance
1096,It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning.,Fact,Fact,,
1097,"Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1098,"(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.",Structuring,Structuring.Quote,,
1099,"Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.",Evaluative,Evaluative,N-Negative,Substance
1100,"For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.",Fact,Fact,,
1101,This is a major concern.,Fact,Fact,,
1110,I believe that the authors have a solid contribution that can be interesting for the ICLR community.,Evaluative,Evaluative,P-Positive,Other
1111,"Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).",Evaluative,Evaluative,N-Negative,Clarity
1113,The authors propose a new method called BA-Net to solve the SfM problem by explicitly incorporating geometry priors into a machine learning task.,Structuring,Structuring.Summary,,
1114,The authors focus on the Bundle Adjustment process.,Structuring,Structuring.Summary,,
1115,"Given several successive frames of a video sequence (2 frames but can be extended up to 5), BA-Net jointly estimates the depth of the first frame and the relative camera motion (between the first frame and the next one).",Structuring,Structuring.Summary,,
1116,The method is based on a convolutional neural network which extracts the features of the different pyramid levels of the two images and in parallel computes the depth map of the first frame.,Structuring,Structuring.Summary,,
1117,"The proposed network is based on the DRN-54 (Yu et al., 2017) as a feature extractor.",Structuring,Structuring.Summary,,
1118,This is complemented by the linear combination of depth bases obtained from the first image.,Structuring,Structuring.Summary,,
1119,The features and the initial depth then passed to the optimization layer called BA-layer where the feature re-projection error is minimized by the modified LM algorithm.,Structuring,Structuring.Summary,,
1120,The authors adapt the standard multi-view geometry constraints by a new concept of feature re-projection error in the BA framework (BA-layer) which they made differentiable.,Structuring,Structuring.Summary,,
1121,Differentiable optimization of camera motion and image depth via LM algorithm is now possible and can be used in various other DL architectures (ex. MVS-Net can probably benefit from BA-layer).,Structuring,Structuring.Summary,,
1122,"The authors also propose a novel depth parametrization in the form of linear combination of depth bases which reduces the number of parameters for the learning task,",Structuring,Structuring.Summary,,
1123,enables integration into the same backbone net as used or feature pyramids and makes it possible to jointly train the depth generator and the BA-layer.,Structuring,Structuring.Summary,,
1124,Originally the proposed approach depicts the network operating in the two-view settings.,Structuring,Structuring.Summary,,
1125,"The extensibility to more views is also possible and, as shown by authors, proved to improve performance.",Structuring,Structuring.Summary,,
1126,"It is, however, limited by the GPU capacity.",Structuring,Structuring.Summary,,
1127,"Overall, the authors came up with an interesting approach to the standard BA problem.",Structuring,Structuring.Summary,,
1128,They have managed to inject the multi-view geometry priors and BA into the DL architecture.,Structuring,Structuring.Summary,,
1129,Major comments regarding the paper:,Structuring,Structuring.Heading,,
1130,It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.,Request,Request.Explanation,N-Negative,Replicability
1131,Minor comments regarding the paper:,Structuring,Structuring.Heading,,
1132,-	The spacing between sections is not consistent.,Evaluative,Evaluative,N-Negative,Clarity
1133,-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture.,Evaluative,Evaluative,N-Negative,Clarity
1134,It would be nice to see more details on the subnet for depth estimator and output of the net.,Request,Request.Experiment,N-Negative,Substance
1135,Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.,Request,Request.Edit,N-Negative,Replicability
1136,-,Evaluative,Evaluative,N-Negative,Clarity
1137,Talking about proposed formulation of BA use either of the following and be consistent across the paper:,,,,
1138,Featuremetric BA / Feature-metric BA / Featuremetric BA / ‘Feature-metric BA’,Evaluative,Evaluative,N-Negative,Clarity
1139,-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.,Evaluative,Evaluative,N-Negative,Clarity
1140,-	Attention should be given to the notation in formulas (3) and (4).,Request,Request.Edit,N-Negative,Clarity
1141,The projection function there is no longer accepts a 3D point parametrized by 3 variables.,Evaluative,Evaluative,N-Negative,Clarity
1142,Instead only depth is provided.,Evaluative,Evaluative,N-Negative,Clarity
1143,"In addition, the subindex ‘1’ of the point ‘q’ is not explained.",Evaluative,Evaluative,N-Negative,Clarity
1144,-	More attention should be given to the evaluation section.,Evaluative,Evaluative,N-Negative,Substance
1145,Specifically to the tables (1 and 2) with quantitative results showing the comparison to other methods.,Evaluative,Evaluative,U-Neutral,Substance
1146,It is not clear how the depth error is measured and it would be nicer to have the other errors explained exactly as they referred in the tables (e.g. ATE?).,Evaluative,Evaluative,N-Negative,Replicability
1147,-	How the first camera pose is initialized?,Request,Request.Explanation,N-Negative,Replicability
1148,-	In Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?,Request,Request.Explanation,N-Negative,Soundness/Correctness
1149,"-	Attention should be given to the grammar, formatting in particular the bibliography.",Request,Request.Edit,N-Negative,Clarity
1170,The paper is well written and easy to follow. The topic is apt.,Evaluative,Evaluative,P-Positive,Clarity
1171,I don’t have any comments except the following ones.,Structuring,Structuring.Heading,,
1172,"Lemma 2.4, Point 1: The proof is confusing.",Evaluative,Evaluative,N-Negative,Substance
1173,Consider the one variable vector case.,Fact,Fact,,
1174,"Assuming that there is only one variable w, then \nabla L(w) is not perpendicular to w in general.",Fact,Fact,,
1175,"The Rayleigh quotient example L(w)  = w’*A*w/ (w’*w) for a symmetric matrix A, then \nabla L(w) = (2/w’*w)(Aw - L(w)*w), which is not perpendicular to w.",Fact,Fact,,
1176,Even if we constrain ||w ||_2 = 1,Fact,Fact,,
1177,", then also  \nabla L(w)  is not perpendicular to w.",,,,
1178,Am I missing something?,Request,Request.Clarification,U-Neutral,Soundness/Correctness
1179,What is G_t in Theorem 2.5. It should be defined in the theorem itself.,Request,Request.Edit,N-Negative,Substance
1180,There is another symbol G_g which is a constant.,Structuring,Structuring.Quote,,
1194,This paper attempts to mitigate catastrophic problem in continual learning.,Structuring,Structuring.Summary,,
1195,"Different from the previous works where episodic memory is used, this work adopts the generative replay strategy and improve the work in (Serra et al., 2018) by extending the output neurons of generative network when facing the significant domain shift between tasks.",Evaluative,Evaluative,P-Positive,Originality
1196,Here are my detailed comments:,Structuring,Structuring.Heading,,
1197,"Catastrophic problem is the most severe problem in continual learning since when learning more and more new tasks, the classifier will forget what they learned before, which will be no longer an effective continual learning model.",Fact,Fact,,
1198,"Considering that episodic memory will cost too much space, this work adopts the generative replay strategy where old representative data are generated by a generative model.",Evaluative,Evaluative,P-Positive,Motivation/Impact
1199,"Thus, at every time step, the model will receive data from every task so that its performance on old tasks will retain.",Fact,Fact,,
1200,"However, if the differences between tasks are significant, the generator cannot reserve vacant neurons for new tasks or in other words, the generator will forget the old information from old tasks when overwritten by information from new tasks.",Fact,Fact,,
1201,"Therefore, this work tries to tackle this problem by extending the output neurons of the generator to keep vacant neurons to retain receive new information.",Structuring,Structuring.Quote,,
1202,"As far as I am concerned, this is the main contribution of this work.",Structuring,Structuring.Summary,,
1203,"Nevertheless, I think there are some deficiencies in this work.",Structuring,Structuring.Heading,,
1204,"First, this paper is not easy to follow.",Evaluative,Evaluative,N-Negative,Clarity
1205,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.",Evaluative,Evaluative,N-Negative,Originality
1206,"For example, in Section 4.1, I am not sure the equation (3), (4), (5), (6) are the contributions of this paper or not since a large number of citations appear.",Evaluative,Evaluative,N-Negative,Clarity
1207,"Second, the authors mention that to avoid storing previous data, they adopt generative replay and continuously enlarge the generator to tackle the significant domain shift between tasks.",Structuring,Structuring.Quote,,
1208,"However, in this way, when more and more tasks come, the generator will become larger and larger.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1209,The storing problem still exists.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1210,Generative replay also brings the time complexity problem since it is time consuming to generate previous data.,Evaluative,Evaluative,N-Negative,Substance
1211,"Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.",Request,Request.Experiment,N-Negative,Meaningful Comparison
1212,"Third, the datasets used in this paper are rather limited.",Evaluative,Evaluative,N-Negative,Substance
1213,Three datasets cannot make the experiments convincing.,Evaluative,Evaluative,N-Negative,Substance
1214,"In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10.",Request,Request.Explanation,N-Negative,Meaningful Comparison
1215,I hope the author could explain this phenomenon.,Request,Request.Explanation,N-Negative,Meaningful Comparison
1216,"Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.",Request,Request.Edit,N-Negative,Clarity
1217,"Fourth, there are some grammar mistakes and typos.",Request,Request.Typo,N-Negative,Clarity
1218,"For example, there are two ""the"" in the end of the third paragraph in Related Work.",Request,Request.Typo,N-Negative,Clarity
1219,"In the last paragraph in Related Work, ""provide"" should be ""provides"".",Request,Request.Typo,N-Negative,Clarity
1220,"In page 8, the double quotation marks of ""short-term"" are not correct.",Request,Request.Typo,N-Negative,Clarity
1221,"Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1222,The proposed method is also heuristic and lacks promising guarantee.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1223,This paper proposed a new training framework to disentangle global structures from local structures,Fact,Fact,,
1224,based on Variational Autoencoders (VAEs). They first generate a transformed image by shuffling the,,,,
1225,patches of the original image to destroy the global structures.,,,,
1226,The training task forces the model to,Fact,Fact,,
1227,"reconstruct the original image and shuffled images from different latent variables, thus separating",,,,
1228,global long-range structural correlations and local patch-wise correlations .,,,,
1229,"Instead of adjusting the objective function or model structure, the paper proposed a new and simple",Evaluative,Evaluative,P-Positive,Originality
1230,"training framework to disentangle the global and local structures, which is novel.",,,,
1231,The experiment results are good on SVHN.,Evaluative,Evaluative,P-Positive,Substance
1232,Some visual inspection experiments on CIFAR10 are performed.,Fact,Fact,,
1233,The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.,Evaluative,Evaluative,N-Negative,Clarity
1234,The rest experiments,Evaluative,Evaluative,N-Negative,Substance
1235,"are all based on SVHN, which is too simple.",,,,
1236,More experiments based on other types of data sets with clear global structures such as faces or stop signs will,Request,Request.Experiment,N-Negative,Substance
1237,be more convincing.,,,,
1238,"In the digit dataset, the local and global structures are relatively easy to separate.",Fact,Fact,,
1239,However,Evaluative,Evaluative,N-Negative,Substance
1240,", in Table 1, the",,,,
1241,performance of VAE+Auxiliary is not better than two of the other methods.,,,,
1242,The idea in this paper is novel but experiments do not seem to be enough.,Evaluative,Evaluative,N-Negative,Substance
1243,More experiments on datasets,Evaluative,Evaluative,N-Negative,Substance
1244,with clear global and local structure separations with careful analyses are required to make the paper stronger.,,,,
1245,The submission proposes a new method for agent design to learn about the behaviour of other fixed agents inhabiting the same environment.,Structuring,Structuring.Summary,,
1246,The method builds on imitation learning (behavioural cloning) to model the agent’s behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours.,Structuring,Structuring.Summary,,
1247,"Overall, the approach falls into the field of intrinsic motivation / curiosity-like reward generation procedures but with respect to target agent behaviour instead of the agent’s environment.",Structuring,Structuring.Summary,,
1248,"While learning to model the target agent’s inner state, the RL reward is generated based on the difference of the target agent’s inner state between consecutive time steps.",Structuring,Structuring.Summary,,
1249,The approach is evaluated against a small set of baselines in various toy grid-world scenarios and a sorting task and overall performs commensurate or better than the investigated baselines.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
1250,"Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1251,It would be highly beneficial to evaluate these aspects.,Request,Request.Experiment,P-Positive,Soundness/Correctness
1252,"Furthermore, it would be beneficial to provide more information about the baselines; in particular the type of count-based exploration.",Request,Request.Clarification,U-Neutral,Substance
1253,"For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs to not only evaluate performance but also robustness.",Request,Request.Edit,U-Neutral,Substance
1254,"Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1255,"One additional aspect pointing towards the necessity of further evaluation is the strong dependence of performance on the dimensionality of the latent, internal state (Fig.4).",Evaluative,Evaluative,U-Neutral,Substance
1256,Minor issues:,Structuring,Structuring.Heading,,
1257,- Reward formulations for the baselines as part of the appendix.,Request,Request.Edit,U-Neutral,Substance
1258,- Same scale for the y-axes across figures,Request,Request.Typo,U-Neutral,Substance
1259,Regularizing RKHS norm is a classic way to prevent overfitting.,Structuring,Structuring.Summary,,
1260,The authors,Structuring,Structuring.Summary,,
1261,note the connections between RKHS norm and several common regularization and,,,,
1262,"robustness enhancement techniques, including gradient penalty, robust",,,,
1263,optimization via PGD and spectral norm normalization.,,,,
1264,They can be seen as upper,Structuring,Structuring.Summary,,
1265,or lower bounds of the RKHS norm.,,,,
1266,"There are some interesting findings in the experiments. For example, for",Evaluative,Evaluative,P-Positive,Substance
1267,"improving generalization, using the gradient penalty based method seems to work",,,,
1268,best.,,,,
1269,"For improving robustness, adversarial training with PGD has the best",Fact,Fact,,
1270,results (which matches the conclusions by Madry et al.); but as shown in Figure,,,,
1271,"2,",,,,
1272,"because adversarial training only decreases a lower bound of RKHS norm, it",,,,
1273,does not necessarily decrease the upper bound (the product of spectral norms).,,,,
1274,This can be shown as a weakness of adversarial training if the authors explore,Fact,Fact,,
1275,further and deeper in this direction.,,,,
1276,"Overall, this paper has many interesting results, but its contribution is",Evaluative,Evaluative,N-Negative,Originality
1277,limited because:,,,,
1278,1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has,Evaluative,Evaluative,N-Negative,Originality
1279,been well studied by previous literature.,,,,
1280,This paper simply applies these,,,,
1281,"results to deep neural networks, by treating the neural network as a big",,,,
1282,black-box function f(x),,,,
1284,Many of the results have been already presented in,Evaluative,Evaluative,N-Negative,Originality
1285,previous works like Bietti & Mairal (2018).,,,,
1286,"2. In experiments, the authors explored many existing methods on improving",Evaluative,Evaluative,N-Negative,Originality
1287,generalization and robustness. However all these methods are known and not new.,,,,
1288,"Ideally, the authors can go further and propose a new regularization method",Other,Other,,
1289,"based on the connection between neural networks and RKHS, and conduct",,,,
1290,experiments to show its effectiveness.,,,,
1291,"The paper is overall well written, and the introductions to RKHS and each",Evaluative,Evaluative,P-Positive,Clarity
1292,regularization techniques are very clear.,,,,
1293,The provided experiments also include,Evaluative,Evaluative,P-Positive,Substance
1294,some interesting findings.,,,,
1295,My major concern is the lack of novel contributions,Evaluative,Evaluative,N-Negative,Originality
1296,in this paper.,,,,
1318,"Given a network and input model for generating adversarial examples, this paper presents an idea to quantitatively evaluate the robustness of the network to these adversarial perturbations.",Structuring,Structuring.Summary,,
1319,"Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks.",Request,Request.Experiment,U-Neutral,Substance
1320,Detailed review below:,Structuring,Structuring.Heading,,
1321,"- How does the performance of the proposed method scale wrt scalability? It will be useful to do an ablation study, i.e. keep the input model fixed and slowly increase the dimension.",Evaluative,Evaluative,U-Neutral,Motivation/Impact
1322,- Did you experiment with other MH proposal beyond a random walk proposal? Is it possible to measure the diversity of the samples using techniques such as the effective sample size (ESS) from the SMC literature?,Evaluative,Evaluative,U-Neutral,Meaningful Comparison
1323,"- What is the performance of the proposed method against ""universal adversarial examples""?",Evaluative,Evaluative,U-Neutral,Substance
1324,- The most interesting question is whether this method gives reasonable robustness estimates even for large networks such as AlexNet?,Evaluative,Evaluative,U-Neutral,Motivation/Impact
1325,"- Please provide some intuition for this line in Figure 3: ""while the robustness to perturbations of size  = 0:3 actually starts to decrease after around 20 epochs.""",Request,Request.Edit,U-Neutral,Soundness/Correctness
1326,- A number of attack and defense strategies have been proposed in the literature.,Evaluative,Evaluative,U-Neutral,Substance
1327,"Isn't it possible to use the proposed method to quantify the increase in the robustness towards an attack model using a particular defense strategy? If it is possible to show that the results of the proposed method match the conclusions from these papers, then this will be an important contribution.",Evaluative,Evaluative,U-Neutral,Substance
1328,Summary: The paper studies the problem of training deep neural networks in the distributes setting while ensuring privacy.,Structuring,Structuring.Summary,,
1329,"Each data sample is held by one individual (e.g., on a cell phone), and a central algorithm trains a learning model on top of this data.",Structuring,Structuring.Summary,,
1330,"In order to protect the privacy of the individuals, the paper proposes the use of multi-layer encoders (E) over the raw data, and then send them across the server.",Structuring,Structuring.Summary,,
1331,"The privacy is ensured by exemplifying the inability to reconstruct the original data from the encoded features, via running a reverse deep model (X).",Structuring,Structuring.Summary,,
1332,"The notion of privacy is quantified by the Euclidian distance between the reconstructed vector via the best X and the original feature vector, maximized over E. The overall framework resembles a GAN, and the paper calls it RAN (Reconstructive Adversarial Network).",Structuring,Structuring.Summary,,
1333,Positive aspects: The problem of training privacy preserving deep models over distributed data has been a significant and important challenge.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1334,"The current solutions that adhere to differential privacy based approaches are not yet practical. In my view, it is a very important research question.",Evaluative,Evaluative,P-Positive,Motivation/Impact
1335,Negative aspects: One major concern I have with the paper is the notion of privacy considered.,Evaluative,Evaluative,N-Negative,Substance
1336,The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.,Evaluative,Evaluative,N-Negative,Substance
1337,"There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.",Evaluative,Evaluative,N-Negative,Substance
1338,"I do not see the GAN style approach taken by the paper, ensures this.",Evaluative,Evaluative,N-Negative,Substance
1339,"Authors provide a variant of WGAN, called PC-GAN, to generate 3D point clouds.",Structuring,Structuring.Summary,,
1340,The drawback of a vanilla GAN with a DeepSet classifier is analyzed.,Structuring,Structuring.Summary,,
1341,The rationality that decoupling the point generator with the object generator is also discussed.,Structuring,Structuring.Summary,,
1342,A sandwiching objective function is proposed to achieve a better estimation of Wasserstein distance.,Structuring,Structuring.Summary,,
1343,"Compared with AAE and the simplified variants of the proposed PC-GAN, the proposed PC-GAN achieves incremental results on point cloud generation.",Structuring,Structuring.Summary,,
1345,1. Authors calculate W_U in a primal form via solving an assignment programming problem.,Structuring,Structuring.Quote,,
1346,Have authors ever tried Sinkhorn iteration?,Request,Request.Clarification,U-Neutral,Substance
1347,"To my knowledge, sinkhorn iteration is a very popular method to solve OT problem effectively.",Fact,Fact,,
1348,It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U.,Request,Request.Explanation,U-Neutral,Replicability
1350,"Authors proved that the sandwiching object W_s is closer to the real Wasserstein distance, but it increases the variance of the loss function.",Structuring,Structuring.Quote,,
1351,"Specifically, the dynamics of W_U, and W_L, according to lemma1, is (epsilon2-epsilon1)*w(P, G) while the dynamics of W_s is 2*epsilon1 * w(P, G), and 2epsilon1 > epsilon2 - epsilon1 (according to the assumption in lemma 1).",Structuring,Structuring.Quote,,
1352,Does it mean that the W_s is not as stable as W_L or W_U during training?,Request,Request.Clarification,U-Neutral,Soundness/Correctness
1353,"Additionally, authors combined W_U with W_L with a mixture 20:1, i.e., the s in Eqs(6, 13, 14) is smaller than 0.05.",Structuring,Structuring.Quote,,
1354,"In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U?",Request,Request.Clarification,U-Neutral,Soundness/Correctness
1355,Authors should analyze the stability of their method in details.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1356,"Essentially, the proposed method is a variant of WGAN, which estimates Wasserstein distance with lower bias but may suffer from worse stability.",Fact,Fact,,
1357,"In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants.",Evaluative,Evaluative,N-Negative,Originality
1358,Typos:,Structuring,Structuring.Heading,,
1359,"- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}",Request,Request.Typo,N-Negative,Clarity
1360,- The 3rd line of lemma 1: epsilon1 -> epsilon_1,Request,Request.Typo,N-Negative,Clarity
1361,"- Page 14, Eq(14), \lambda should be s",Request,Request.Typo,N-Negative,Clarity
1362,"- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.",Request,Request.Typo,N-Negative,Clarity
1363,"This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations.",Structuring,Structuring.Summary,,
1364,"The article is well written and I find the contribution simple, but interesting.",Evaluative,Evaluative,P-Positive,Clarity
1365,It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017).,Evaluative,Evaluative,P-Positive,Motivation/Impact
1366,My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.,Evaluative,Evaluative,N-Negative,Substance
1367,"I understand that for two matrices A and B we have rank(AB) <= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but",Evaluative,Evaluative,U-Neutral,Substance
1368,have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?,Request,Request.Experiment,N-Negative,Substance
1370,"Joulin, A., Cissé, M., Grangier, D. and Jégou, H., 2017, July. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp. 1302-1310).",Other,Other,,
1372,The authors look at the problem of exploration in deep RL.,Structuring,Structuring.Summary,,
1373,They propose a “curiosity grid” which is a virtual grid laid out on top of the current level/area that an Atari agent is in.,Structuring,Structuring.Summary,,
1374,"Once an agent enters a new cell of the grid, it obtains a small reward, encouraging the agent to explore all parts of the game.",Structuring,Structuring.Summary,,
1375,The grid is reset (meaning new rewards can be obtained) after every roll out (meaning the Atari agent has used up all its lives and the game restarts).,Structuring,Structuring.Summary,,
1376,The authors argue that this method enables better exploration and they obtain an impressive score on Montezuma’s Revenge (MR).,Structuring,Structuring.Summary,,
1377,Review:,Structuring,Structuring.Heading,,
1378,"The paper contains an extensive introduction with many references to prior work, and a sensible lead up to the introduced algorithm.",Evaluative,Evaluative,P-Positive,Motivation/Impact
1379,The algorithm itself seems to work well and some of the results are convincing.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
1380,I am a bit worried about the fact that the agents have access to their history of locations (“the grid”).,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1381,The authors mention that none of the methods they compare against has this advantage and it seems that in a game that rewards exploration directly (MR) this is a large advantage.,Fact,Fact,,
1382,The authors comment on this advantage in section 3 and found that removing intrinsic rewards hurt performance significantly.,Fact,Fact,,
1383,Only removing the grid access made results on MR very unstable.,Fact,Fact,,
1384,"However in order to compute the intrinsic rewards, it still seems necessary to access the location of the agent, meaning that implicitly the advantage of the method is still there.",Fact,Fact,,
1385,"I was wondering if the authors find that the agents are forcibly exploring the entire environment during each rollout? Even if the agent knows what/where the actual goal is. There is a hint to this behaviour in section 4, on exploration in sparse domains.",Request,Request.Explanation,U-Neutral,Soundness/Correctness
1386,"The future work section mentions some interesting improvements, where the agent position is learned from data. That seems like a promising direction that would generalise beyond Atari games and avoids the advantage.",Evaluative,Evaluative,P-Positive,Substance
1387,Nits/writing feedback:,Structuring,Structuring.Heading,,
1388,- There is no need for such repetitive citing (esp paragraph 2 on page 2).,Evaluative,Evaluative,N-Negative,Clarity
1389,Sometimes the same paper is cited 4 times within a few lines.,Evaluative,Evaluative,N-Negative,Clarity
1390,"While it’s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.",Evaluative,Evaluative,N-Negative,Clarity
1391,- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1392,##,Structuring,Structuring.Heading,,
1394,Revision:,Structuring,Structuring.Heading,,
1395,The rebuttal does little to clarify open questions:,Evaluative,Evaluative,N-Negative,Other
1396,1. Both reviewer 2 and I commented on the ablation study regarding the grid but received no reply.,Evaluative,Evaluative,N-Negative,Substance
1397,"2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.",Evaluative,Evaluative,N-Negative,Originality
1398,"3. The authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm.",Evaluative,Evaluative,N-Negative,Originality
1399,This seems contradictory.,Evaluative,Evaluative,N-Negative,Originality
1400,The paper is really interesting.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1401,Set prediction problem has lots of applications in AI applications and the problem has not been conquered by deep networks.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1402,The paper proposes a formulation to learn the distribution over unobservable permutation variables based on deep networks and uses a MAP  estimator for inference.,Structuring,Structuring.Summary,,
1403,It has object detection applications.,Structuring,Structuring.Summary,,
1404,The results show that it can outperform YOLOv2 and Faster R-CNN in a small pedestrian detection dataset which contains heavy occlusions.,Structuring,Structuring.Summary,,
1405,The limitation is clearly stated in the last part of the paper that the number of possible permutations exponentially grows with the maximum set size (cardinality).,Structuring,Structuring.Quote,,
1406,"In the author response period, I would like the author give more details about the pedestrian detection experiments, such as how many dense layers are used after ResNet-101, what are the training and inference time, is it possible to report results on PASCAL VOC (only the person class).",Request,Request.Edit,U-Neutral,Replicability
1407,The method is exciting for object detection funs.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1408,I would like to encourage the authors to release the code and let the whole object detection community overcome the limitation in the paper.,Request,Request.Experiment,U-Neutral,Replicability
1435,"The proposed method tackles class-incremental continual learning, where new categories are incrementally exposed to the network but a classifier across all categories must be learned.",Fact,Fact,,
1436,"The proposed method seems to be essentially a combination of generative replay (e.g. Deep Generative Replay) with AC-GAN as the model and attention (HAT), along with a growing mechanism to support saturating capacity.",Fact,Fact,,
1437,Quantitative results are shown on MNIST and SVHN while some analysis is provided on CIFAR.,Fact,Fact,,
1439,"+ The method combines the existing works in a way that makes sense, specifically AC-GAN to support a single generator network with attention-based methods to prevent forgetting in the generator.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
1440,"+ The method results in good performance, although see caveats below.",Evaluative,Evaluative,P-Positive,Substance
1441,+ Analysis of the evolution of mask values over time is interesting.,Evaluative,Evaluative,P-Positive,Substance
1443,- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.,Evaluative,Evaluative,N-Negative,Clarity
1444,"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.",Evaluative,Evaluative,N-Negative,Clarity
1445,"Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.",Evaluative,Evaluative,N-Negative,Originality
1446,The authors should on the claimed contributions.,Request,Request.Edit,N-Negative,Originality
1447,Is it a combination of DGR and HAT with some capacity expansion?,Request,Request.Explanation,N-Negative,Originality
1448,- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1449,"Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?",Request,Request.Explanation,N-Negative,Substance
1450,- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.,Evaluative,Evaluative,N-Negative,Replicability
1451,It also adds capacity and it is not at all made clear whether the comparison is fair since no analysis on number of parameters are shown.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
1452,"- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.",Request,Request.Experiment,N-Negative,Meaningful Comparison
1453,"As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1454,"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1455,"Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.",Evaluative,Evaluative,N-Negative,Substance
1456,Otherwise you are just defining the problem in a way that excludes other simple approaches which work.,Evaluative,Evaluative,N-Negative,Originality
1457,"- There are several methodological issues: Why are CIFAR results not shown in a table as is done for the other dataset? How many times were the experiments run and what were the variances? How many parameters are used (since capacity can increase?) It is for example not clear that the comparison to joint training is fair, when stating: ""Interestingly, DGM outperforms joint training on the MNIST dataset using the same architecture.",Request,Request.EditExperiment,N-Negative,Replicability|Meaningful Comparison
1458,This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations,Request,Request.Explanation,N-Negative,Substance
1459,"compared to what it would learn given all the data."" Doesn't DGM grow the capacity, and therefore this isn't that surprising?",,,,
1460,"This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
1461,Some other minor issues in the writing includes:,Structuring,Structuring.Heading,,
1462,"1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).",Request,Request.Edit,N-Negative,Originality
1463,"The initial narrative mixes prior works' contributions and this paper's contributions; the contributions of the paper itself should be made clear,",Request,Request.Edit,N-Negative,Originality
1464,"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,",Request,Request.Edit,N-Negative,Clarity
1465,3) There is no legend for CIFAR; what do the colors represent?,Request,Request.Clarification,N-Negative,Substance
1466,"4) There are several typos/grammar issues e.g. ""believed to occurs"", ""important parameters sections"", ""capacity that if efficiently allocated"", etc.).",Request,Request.Typo,N-Negative,Other
1467,"In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
1468,More rigorous experiments and analysis is needed to make this a good ICLR paper.,Evaluative,Evaluative,N-Negative,Substance
1469,This paper studies how the FiLM visual question answering (VQA) model answer questions involving the quantifier ‘most’.,Structuring,Structuring.Summary,,
1470,"This quantifier is chosen for study because it cannot be expressed in first order logic (i.e., high-order logic is required), and secondly because there are two different algorithmic approaches to answering questions involving ‘most’ (cardinality-based strategy and pairing-based strategy).",Structuring,Structuring.Summary,,
1471,"Experiments are performed by designing abstract visual scenes with controlled numerosity and spatial layouts, and applying methodologies from pyscholinguistics.",Structuring,Structuring.Summary,,
1472,"The paper concludes that the model learns an approximate number system (ANS), consistent with the cardinality-based strategy, with implications for understanding the conditions under which existing VQA models should perform well or badly (and possibly for improving VQA models).",Structuring,Structuring.Summary,,
1474,"- The research question is clear and well-conceived. In general, it seems there are significant opportunities for better collaboration between the experimental psychology and machine learning communities, and this is a good example of the benefits.",Evaluative,Evaluative,P-Positive,Motivation/Impact
1475,"- The paper is clear, highly-focused, and well-written.",Evaluative,Evaluative,P-Positive,Clarity
1477,- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.,Evaluative,Evaluative,N-Negative,Substance
1478,"For example, the section on “Ratios andWeber fraction” argues that “these curves align well with the trend predicted by Weber’s law”, but does not explain how the experimental data would present if the alternative hypothesis (pairing-based strategy) was being used.",Evaluative,Evaluative,N-Negative,Substance
1479,What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?,Request,Request.Explanation,N-Negative,Substance
1480,"- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.",Evaluative,Evaluative,N-Negative,Originality
1481,"While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.",Request,Request.Experiment,N-Negative,Originality
1482,"- In some ways it is not that surprising that the CNN more easily learns an approximate number system rather than a pairing-based algorithm, as the later would presumably need to learn a different convolutional filter for every possible spatial arrangement of the pairs (which would be very sample inefficient).",Fact,Fact,,
1483,"Therefore, it might be interesting to consider, are there any circumstances under which the CNN would learn a pairing based algorithm?",Request,Request.Experiment,U-Neutral,Substance
1484,"For example, what if the spatial configuration of the pairs was simplified, so they were always side-by-side at a fixed distance? If pairing-based algorithms emerged under simplified scenarios, this might have implications for the design of CNN filters (if we want models that are capable of learning these types of functions).",Request,Request.Experiment,U-Neutral,Substance
1486,"I regard this as a good paper, with a couple of weakness that could be addressed as indicated.",Evaluative,Evaluative,P-Positive,Other
1496,This paper introduced a new architecture for input embeddings of neural language models: adaptive input representation (ADP).,Structuring,Structuring.Summary,,
1497,ADP allowed a model builder to define a set of bands of input words with different frequency where frequent words have larger embedding size than the others.,Structuring,Structuring.Summary,,
1498,The embeddings of each band are then projected into the same size.,Structuring,Structuring.Summary,,
1499,This resulted in lowering the number of parameters.,Structuring,Structuring.Summary,,
1500,Extensive experiments with the Transformer LM on WikiText-103 and Billion Word corpus showed that ADP achieved competitive perplexities.,Structuring,Structuring.Summary,,
1501,"While tying weight with the output did not benefit the perplexity, it lowered the runtime significantly on Billion Word corpus.",Structuring,Structuring.Summary,,
1502,Further analyses showed that ADP gained performance across all word frequency ranges.,Structuring,Structuring.Summary,,
1503,"Overall, the paper was well-written and the experiments supported the claim.",Evaluative,Evaluative,P-Positive,Clarity
1504,The paper was very clear on its contribution.,Evaluative,Evaluative,P-Positive,Clarity
1505,The variable-size input of this paper was novel as far as I know.,Evaluative,Evaluative,P-Positive,Originality
1506,"However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1507,The weight sharing was also needed further investigation and experimental data on sharing different parts.,Evaluative,Evaluative,U-Neutral,Substance
1508,"The experiments compared several models with different input levels (characters, BPE, and words).",Structuring,Structuring.Heading,,
1509,The perplexities of the proposed approach were competitive with the character model with an advantage on the training time.,Evaluative,Evaluative,P-Positive,Substance
1510,"However, the runtimes were a bit strange.",Evaluative,Evaluative,N-Negative,Substance
1511,"For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).",Structuring,Structuring.Quote,,
1512,The runtime of ADP seemed to lose in term of scaling as well to BPE.,Structuring,Structuring.Quote,,
1513,"Perhaps, the training time was an artifact of multi-GPU training.",Fact,Fact,,
1514,Questions:,Other,Other,,
1515,1. I am curious about what would you get if you use ADP on BPE vocab set?,Request,Request.Experiment,U-Neutral,Substance
1516,2. How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?,Request,Request.Explanation,U-Neutral,Substance
1546,This work proposes an approach for explicitly placing information in a subset of the latent variables.,Fact,Fact,,
1547,"The approach is to construct an auxiliary generative model that takes as input the set of latent variables subtracted from the target subset, which is used to model modified data samples that do not contain the desired information.",Fact,Fact,,
1548,Experiments focus on learning global information.,Fact,Fact,,
1549,The auxiliary model is then given data that have their global information destroyed via random shuffling of image patches.,Fact,Fact,,
1550,# Approach seems limited.,Structuring,Structuring.Heading,,
1551,"- This approach seems very limited, as there must exist a known transformation that removes the desired information.",Evaluative,Evaluative,N-Negative,Substance
1552,"Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)",Request,Request.Experiment,N-Negative,Clarity
1553,- Can this approach learn multiple factors as opposed to just two?,Request,Request.Experiment,N-Negative,Substance
1554,- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.),Request,Request.Experiment,N-Negative,Substance
1555,# More ablations or experiments with comparable settings would be desirable.,Structuring,Structuring.Heading,,
1556,- What is the choice of beta in the beta-VAE training objective?,Request,Request.Clarification,N-Negative,Clarity
1557,"Apart from 1.2, this isn't mentioned.",,,,
1558,My concern here is that beta might be affecting the result more than the proposed training algorithm.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1559,Can the proposed approach perform just as well without a modified objective?,Request,Request.Experiment,N-Negative,Substance
1560,Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.,Request,Request.Experiment,N-Negative,Substance
1561,"(e.g. this approach with normal VAE objective, and normal VAE objective without auxiliary task for the clustering experiment.)",,,,
1562,"- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?",Request,Request.Explanation,N-Negative,Clarity
1563,# Related work.,Structuring,Structuring.Heading,,
1564,"There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:",Evaluative,Evaluative,N-Negative,Other
1565,"- Tranforming autoencoders [1] also apply a transformation to the image, but the goal is to learn the factor corresponding to the transformation, rather than the complement as in this work.",Other,Other,,
1566,- An opposing approach for explicit information placement with a modified training procedure (where the target information is directly placed in the target subset and can handle multiple factors) is DC-IGN [2].,Other,Other,,
1567,"I believe the DC-IGN approach is more general and can handle a superset of the tasks of this approach, without requiring an auxiliary decoder.",Fact,Fact,,
1568,"Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?",Request,Request.Experiment,N-Negative,Substance
1569,"[1] Hinton, Geoffrey E., Alex Krizhevsky, and Sida D. Wang. ""Transforming auto-encoders."" International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2011.",Other,Other,,
1570,"[2] Kulkarni, Tejas D., et al. ""Deep convolutional inverse graphics network."" Advances in neural information processing systems. 2015.",Other,Other,,
1571,---- Update since rebuttal ----,Structuring,Structuring.Heading,,
1572,I thank the authors for clarifying how this work fits in with related works and clarifying the hyperparameters.,Social,Social,,
1573,I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.,Evaluative,Evaluative,N-Negative,Substance
1574,More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.,Evaluative,Evaluative,N-Negative,Substance
1575,"The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.",Request,Request.Experiment,N-Negative,Substance
1576,"This paper looks to predict ""unstructured"" set output data.",Structuring,Structuring.Summary,,
1577,It extends Rezatofighi et al 2018 by modeling a latent permutation.,Structuring,Structuring.Summary,,
1578,"Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1579,"1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1580,"The outputs are either unordered sets, where you would have a permutation invariant (or exchangeable) likelihood, or they are ordered sequence where the order of the outputs does matter, as some are more likely than others.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1581,2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1582,"The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1583,"Conditioned on the permutation of the set, the points are exchangeable.",Fact,Fact,,
1584,"Let's just consider a 2 element ""set"" at the moment Y = (y_1, y_2).",Fact,Fact,,
1585,"Order matters, so either this is being observed as pi=(1, 2) or pi=(2, 1), both of which depend on the input x. However, the likelihood of the points does not actually depend on the order in any traditional sense of the word.",Fact,Fact,,
1586,we have:,Structuring,Structuring.Heading,,
1587,"p_\pi((1, 2) | x, w) p_y(y_1 |  x, w, (1, 2)) p_y(y_2 |  x, w, (1, 2)) + p_\pi((2, 1) | x, w) p_y(y_1 |  x, w, (2, 1)) p_y(y_2 |  x, w, (2, 1))",Fact,Fact,,
1588,"*Note that in here (as in eq. 2) the output distribution p_y does not know what the index is of what it is outputting, since it is iid.* So what does this mean? It means that the order (permutation) can only affect the distribution in an iid (exchangeable, order invariant) way.",Fact,Fact,,
1589,Essentially the paper has just written a mixture model for the output points where there are as many components as permutations.,Fact,Fact,,
1590,"I don't think this makes much sense, and if it was an intentional choice, the paper did a poor job of indicating it.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1591,"3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1592,It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1593,This needs to be justified.,Request,Request.Explanation,N-Negative,Soundness/Correctness
1594,There are some stylistic shortcomings as well.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1595,"For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs)",Evaluative,Evaluative,N-Negative,Clarity
1597,"Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1598,But these and other points are minor.,Fact,Fact,,
1599,"The paper should not be published until it can resolve or make sense of the methodological discrepancies between what it says it looks to do and what it actually does as described in points 1), 2), and 3) above.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1601,"In the semi-supervised self-training setting, this paper proposes to select a certain subset of unlabelled data for training rather than all unlabelled data, where the ensemble of confidence scores of the trained model in iterations is used to guide the selection.",Structuring,Structuring.Summary,,
1602,Strong points:,Structuring,Structuring.Heading,,
1603,"It is a good idea to conduct an ensemble based on the confidence scores of trained models in iterations, although the authors did not mention any theoretical explanation or guarantee behind this.",Evaluative,Evaluative,P-Positive,Motivation/Impact
1604,Weak points:,Structuring,Structuring.Heading,,
1605,"1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.",Evaluative,Evaluative,N-Negative,Originality
1606,"Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter.",Fact,Fact,,
1607,"Therefore, the technical contribution of this paper is moderate.",Evaluative,Evaluative,N-Negative,Motivation/Impact
1608,"Zhu, Xiaojin. ""Semi-supervised learning literature survey."" Computer Science, University of Wisconsin-Madison 2.3 (2006): 4.",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
1609,2) The writing is poor and hard to follow.,Evaluative,Evaluative,N-Negative,Clarity
1610,"First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.",Request,Request.Explanation,N-Negative,Clarity
1611,"For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?",Request,Request.Experiment,N-Negative,Substance
1612,"From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations.",Evaluative,Evaluative,N-Negative,Substance
1613,"The descriptions of the datasets used are not clear, e.g., the number of classes for each data.",Request,Request.Clarification,N-Negative,Substance
1614,"Second, many typos and grammar errors need to fix, e.g., ""the proposed SST is suitable for lifelong learning which make use..."", ""the error 21.44% was lower than"" 18.97?",Request,Request.Typo,N-Negative,Substance
1615,3) The overall performance of the proposed SST in the experiments is not convincing and not promising.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
1616,"First, the labeled data portion is fixed and is relatively high",Evaluative,Evaluative,N-Negative,Substance
1617,compared to most standard semi-supervised learning settings,,,,
1619,"Second, SST itself is only comparable with or even worse than the state-of-art methods.",Evaluative,Evaluative,N-Negative,Substance
1620,Combining SST with other existing techniques can help.,Request,Request.Edit,U-Neutral,Substance
1621,"However, the additional cost is expensive.",Fact,Fact,,
1622,Further demonstrations are necessary for the proposed SST method.,Request,Request.Experiment,U-Neutral,Substance
1623,"This paper presents a gradient estimator for expectation-based objectives, which is called Go-gradient.",Structuring,Structuring.Heading,,
1624,"This estimator is unbiased, has low variance and, in contrast to other previous approaches, applies to either continuous and discrete random variables.",Evaluative,Evaluative,P-Positive,Substance
1625,"They also extend this estimator to problems where the gradient should be ""backpropagated"" through a nested combination of random variables and a (non-linear) functions.",Evaluative,Evaluative,P-Positive,Motivation/Impact
1626,Authors present an extensive experimental evaluation of the estimator on different challenging machine learning problems.,Evaluative,Evaluative,P-Positive,Clarity
1627,"The paper addresses a relevant problem which appears in many machine learning settings, as it is the problem of estimating the gradient of an expectation-based objective.",Evaluative,Evaluative,P-Positive,Motivation/Impact
1628,"In general, the paper is well written and easy to follow. And the experimental evaluation is extensive and compares with relevant state-of-the-art methods.",Evaluative,Evaluative,P-Positive,Substance
1629,The main problem with this paper is that it is difficult to identify its main and novel contributions.,Evaluative,Evaluative,N-Negative,Substance
1630,"1. In the case of continuous random variables, Go-gradient is equal to Implicit Rep gradients (Figurnov et al. 2018) and pathwise gradients (Jankowiack & Obermeyer,2018).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1631,"Furthermore, for the Gaussian case, Implicit Rep gradients (and Go-gradient too) are equal to the standard reparametrization trick estimator (Kingma & Welling, 2014).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1632,This should be made crystal-clear in the paper.,Request,Request.Experiment,U-Neutral,Clarity
1633,What happens is that the authors arrive at this solution using a different approach.,Evaluative,Evaluative,U-Neutral,Substance
1634,"In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.",Request,Request.Experiment,N-Negative,Substance
1635,"Moreover, I don't think some of the presented experiments are necessary.",Evaluative,Evaluative,U-Neutral,Other
1636,Simply because for continuous variables similar experiments have been reported before,Evaluative,Evaluative,N-Negative,Originality
1637,"(Figurnov et al. 2018, Jankowiack & Obermeyer,2018).",Other,Other,,
1638,"2. It seems that the main novel contribution of the paper is to extend the ideas of (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) to discrete variables. And this is a relevant contribution.",Evaluative,Evaluative,U-Neutral,Motivation/Impact
1639,And the experimental evaluations of this part are convincing and compare favourably with other state-of-the-art methods.,Evaluative,Evaluative,U-Neutral,Motivation/Impact
1640,3. Authors should be much more clear about which is their original contribution to the problems stated in Section 4 and Section 5.,Request,Request.ExperimentResult,N-Negative,Clarity|Originality
1641,As authors acknowledge in Section 6,Request,Request.Experiment,U-Neutral,Substance
1643,"<<Stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015), focusing mainly on re-parameterizable Gaussian random variables and deep latent Gaussian models, exploits the product rule for an integral to derive gradient backpropagation through several continuous random variables.",Request,Request.Experiment,U-Neutral,Substance
1644,>>,Other,Other,,
1645,This is exactly what authors do in these sections.,Request,Request.Experiment,U-Neutral,Substance
1646,"Again it seems that the real contribution of this paper here is to extend this stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) ideas to discrete variables.",Evaluative,Evaluative,U-Neutral,Motivation/Impact
1647,Although this extension seems to be easily derived using the contributions made at point 2.,Evaluative,Evaluative,U-Neutral,Motivation/Impact
1648,"Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.",Structuring,Structuring.Summary,,
1649,- Summary,Structuring,Structuring.Heading,,
1650,This paper proposes a multi-objective evolutionary algorithm for the neural architecture search.,Structuring,Structuring.Summary,,
1651,"Specifically, this paper employs a Lamarckian inheritance mechanism based on network morphism operations for speeding up the architecture search.",Structuring,Structuring.Summary,,
1652,The proposed method is evaluated on CIFAR-10 and ImageNet (64*64) datasets and compared with recent neural architecture search methods.,Structuring,Structuring.Summary,,
1653,"In this paper, the proposed method aims at solving the multi-objective problem: validation error rate as a first objective and the number of parameters in a network as a second objective.",Structuring,Structuring.Summary,,
1654,- Pros,Structuring,Structuring.Heading,,
1655,- The proposed method does not require to be initialized with well-performing architectures.,Evaluative,Evaluative,P-Positive,Substance
1656,"- This paper proposes the approximate network morphisms to reduce the capacity of a network (e.g., removing a layer), which is reasonable property to control the size of a network for multi-objective problems.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
1657,- Cons,Structuring,Structuring.Heading,,
1658,"- Judging from Table 1, the proposed method does not seem to provide a large contribution.",Evaluative,Evaluative,N-Negative,Substance
1659,"For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.",,,,
1660,"- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.",Evaluative,Evaluative,N-Negative,Clarity
1661,"- In the case of the search space II, how many GPU days does the proposed method require?",Request,Request.Explanation,N-Negative,Substance
1662,"- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.",Request,Request.Explanation,N-Negative,Substance
1664,This paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.,Structuring,Structuring.Summary,,
1665,The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results.,Structuring,Structuring.Summary,,
1667,-This paper is well-motivated.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1668,Studying label propagation in the meta-learning setting is interesting and novel.,Evaluative,Evaluative,P-Positive,Originality
1669,"Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low.",Fact,Fact,,
1670,"-The empirical results show improvement over the baselines, which are expected.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
1672,-Some technical details  are missing.,Evaluative,Evaluative,N-Negative,Clarity
1673,"In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.",Evaluative,Evaluative,N-Negative,Replicability
1674,Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?,Request,Request.Explanation,U-Neutral,Soundness/Correctness
1675,-Does episode training help label propagation? How about the results of label propagation without the episode training?,Request,Request.Explanation,U-Neutral,Clarity
1676,This paper presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models.,Structuring,Structuring.Summary,,
1677,"It consists of math problems in various categories such as algebra, arithmetic, calculus, etc.",Structuring,Structuring.Summary,,
1678,The dataset is designed carefully so that it is very unlikely there will be any duplicate between train/test split and the difficulty can be controlled.,Structuring,Structuring.Summary,,
1679,"Several models including LSTM, LSTM + Attention, Transformer are evaluated on the proposed dataset.",Structuring,Structuring.Summary,,
1680,The result showed some interesting insights about the evaluated models.,Structuring,Structuring.Summary,,
1681,The evaluation of mathematical reasoning ability is an interesting perspective.,Structuring,Structuring.Summary,,
1682,"However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
1683,"The paper is relatively well-written, although the description of the neural models can be improved.",Evaluative,Evaluative,N-Negative,Clarity
1684,The generation process of the dataset is well thought out.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
1685,"The insights from the analysis of the failure cases are intriguing, but it also points out that the neural networks models are not really performing mathematical reasoning since the generalization is very limited.",Fact,Fact,,
1686,"One suggestion is that it might be useful to also release the structured (parsed) form besides the freeform inputs and outputs, for analysis and for evaluating structured neural network models like the graph networks.",Request,Request.Clarification,U-Neutral,Replicability
1687,My main concerns are about the evaluation and comparison of standard neural models.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
1688,"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1689,"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1690,I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
1691,"With some improvements in the evaluation and comparison, I believe this paper will be more complete and much stronger.",Other,Other,,
1692,typo:,Structuring,Structuring.Heading,,
1693,page 3: “freefrom inputs and outputs” -> “freeform inputs and outputs”,Request,Request.Typo,U-Neutral,Clarity
1702,This paper makes concerted efforts to examine the existing beliefs about the significance of various statistical characteristics of hidden layer activations (or representations) in a DNN.,Structuring,Structuring.Summary,,
1703,"In the past, many works have argued for encouraging the certain statistical behavior of these representations (e.g., sparsity, low correlation etc) in order to have better classification accuracy.",Structuring,Structuring.Summary,,
1704,"However, this paper tries to argue that such efforts are not very useful as these statistical characteristics don't provide any systematic explanation for the performance of DNNs across different settings.",Structuring,Structuring.Summary,,
1705,"First, the paper argues that given a DNN, it's possible to construct either an identical output network or a comparable network that can have very different behavior for some of the statistical characteristics.",Structuring,Structuring.Summary,,
1706,This casts doubt on the usefulness of these characteristics in explaining the performance of the network.,Structuring,Structuring.Summary,,
1707,"The paper conducts experiments with different regularizers associated with some of the standard statistical characteristics using the MNIST, CIFAR-10, and CIFAR-100 datasets.",Structuring,Structuring.Summary,,
1708,The paper claims that for each dataset the best performing network cannot be attributed to any single regularizer.,Structuring,Structuring.Summary,,
1709,"For the same set of regularizers and the MNIST dataset, the paper then explores the mutual information between the inputs and the hidden layer activations.",Structuring,Structuring.Summary,,
1710,The paper observes that the best performing regularizer is the one which minimizes this mutual information.,Structuring,Structuring.Summary,,
1711,"Therefore, it is plausible that the mutual information regularization can consistently explain the performance of an NN.",Structuring,Structuring.Summary,,
1712,The paper addresses an interesting problem and makes some good contributions.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1713,"However, the reviewer feels that the brief treatment of mutual information regularizer leaves something to be desired.",Fact,Fact,,
1714,"Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.",Request,Request.Explanation,N-Negative,Substance
1715,"In these tables, how do the authors decide which hidden layer representations should be explored for their statistical characteristics?",Request,Request.Clarification,U-Neutral,Soundness/Correctness
1716,"The reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?",Request,Request.Clarification,U-Neutral,Soundness/Correctness
1717,Summary: This paper proposes a meta-learning solution for problems involving optimizing multiple loss values.,Structuring,Structuring.Summary,,
1718,"They use a simple (small mlp), discrete, stochastic controller to control applications of updates among a finite number of different update procedures.",Structuring,Structuring.Summary,,
1719,"This controller is a function of heuristic features derived from the optimization problem, and is optimized using policy gradient either exactly in toy settings or in a online / truncated manor on larger problems.",Structuring,Structuring.Summary,,
1720,"They present results on 4 settings: quadratic regression, MLP classification, GAN, and multi-task MNT.",Structuring,Structuring.Summary,,
1721,They show promising performance on a number of tasks as well as show the controllers ability to generalize to novel tasks.,Structuring,Structuring.Summary,,
1722,This is an interesting method and tackles a impactful problem.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1723,"The setup and formulation (using PG to meta-optimize a hyper parameter controller) is not extremely novel (there have been similar work learning hyper parameter controllers), but the structure, the problem domain, and applications are.",Evaluative,Evaluative,U-Neutral,Originality
1724,"The experimental results are through, and provide compelling proof that this method works as well as exploration as to why the method works (analyzing output softmax).",Evaluative,Evaluative,P-Positive,Soundness/Correctness
1725,"Additionally the ""transfer to different models"" experiment is compelling.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
1726,Comments vaguely in order of importance:,Structuring,Structuring.Heading,,
1727,1. I am a little surprised that this training strategy works.,Social,Social,,
1728,"In the online setting for larger scale problems, your gradients are highly correlated and highly biased.",Evaluative,Evaluative,U-Neutral,Substance
1729,"As far as I can tell, you are performing something akin to truncated back back prop through time with policy gradients.",Other,Other,,
1730,The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.,Evaluative,Evaluative,N-Negative,Substance
1731,"As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).",Request,Request.Clarification,N-Negative,Substance
1732,Some comment as to this bias -- or even suggesting that it might exist would be useful.,Request,Request.Explanation,U-Neutral,Substance
1733,"As of now, it is implied that the gradient estimator is unbiased.",Evaluative,Evaluative,U-Neutral,Substance
1735,"Second, even ignoring this bias, the resulting gradients are heavily correlated.",,,,
1736,Algorithm 1 shows no sign of performing batched updates on \phi or anything to remove these corrections.,Evaluative,Evaluative,U-Neutral,Substance
1737,"Despite these concerns, your results seem solid.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
1738,"Nevertheless, further understanding as to this would be useful.",Request,Request.Explanation,U-Neutral,Substance
1739,3. The structure of the meta-training loop was unclear to me.,Request,Request.Clarification,N-Negative,Substance
1740,Algorithm 1 states S=1 for all tasks while the body -- the overhead section -- you suggest multiple trainings are required ( S>1?).,Fact,Fact,,
1741,"4. If the appendix is correct and learning is done entirely online, I believe the initialization of the meta-parameters would matter greatly -- if the default task performed poorly with a uniform distribution for sampling losses, performance would be horrible.",Evaluative,Evaluative,U-Neutral,Substance
1742,This seems like a limitation of the method if this is the case.,Evaluative,Evaluative,N-Negative,Substance
1743,5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:,Evaluative,Evaluative,N-Negative,Clarity
1744,5.1/Figure 1: I think there is an overloaded use of lambda?,Request,Request.Clarification,U-Neutral,Clarity
1745,"My understanding as written that lambda is both used in the grid search (table 1) to find the best loss l_1 and then used a second location, as a modification of l_2 and completely separate from the grid search?",,,,
1746,"6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.",Request,Request.Clarification,N-Negative,Clarity
1747,"It seems you performing controller optimization (optimizing phi), on the validation set loss, while also reporting scores on this validation set.",Request,Request.Clarification,U-Neutral,Clarity
1748,This should most likely instead be a 3rd dataset.,Request,Request.Clarification,U-Neutral,Clarity
1749,"You have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.",Request,Request.Clarification,N-Negative,Clarity
1750,"Given the low meta-parameter count of the I don't think this represents a huge risk, and baselines also suffer from this issue (hyper parameter search on validation set) so I expect results to be similar.",Other,Other,,
1751,"7. Page 4: ""When ever applicable, the final reward $$ is clipped to a given range to avoid exploding or vanishing gradients"".",Fact,Fact,,
1752,It is unclear to me how this will avoid these.,Request,Request.Clarification,N-Negative,Clarity
1753,"In particular, the ""exploding"" will come from the \nabla log p term, not from the reward (unless you have reason to believe the rewards will grow exponentially).",Request,Request.Clarification,U-Neutral,Clarity
1754,"Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.",Evaluative,Evaluative,N-Negative,Clarity
1755,"This clipping will also introduce bias, this is not discussed, and will probably lower variance.",Evaluative,Evaluative,N-Negative,Clarity
1756,"This is a trade off made in a number of RL papers so it seems reasonable, but not for this reason.",Evaluative,Evaluative,U-Neutral,Clarity
1757,"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.",Evaluative,Evaluative,N-Negative,Clarity
1758,"While not a published paper, some early gan work [2] does contains a dynamic schedule but you are correct that this family of methods are not commonplace in modern gan research.",Evaluative,Evaluative,P-Positive,Substance
1759,"9. Related work: While not exactly the same setting, I think [1] is worth looking at.",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
1760,"This is quite similar causing me pause at this comment: ""first framework that tries to learn the optimization schedule in a data-driven way"".",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
1761,"Like this work, they also lean a controller over hyper-parameters (in there case learning rate), with RL, using hand designed features.",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
1762,10. There seem to be a fair number of heuristic choices throughout.,Evaluative,Evaluative,U-Neutral,Meaningful Comparison
1763,Why is IS squared in the reward for GAN training for example? Why is the scaling term required on all rewards?,Request,Request.Explanation,U-Neutral,Substance
1764,Having some guiding idea or theory for these choices or rational would be appreciated.,Request,Request.Explanation,U-Neutral,Substance
1765,11. Why is PPO introduced?,Request,Request.Explanation,U-Neutral,Substance
1766,"In algorithm 1, it is unclear how PPO would fit into this?",Request,Request.Explanation,U-Neutral,Substance
1767,More details or an alternative algorithm in the appendix would be useful.,Request,Request.Explanation,U-Neutral,Substance
1768,Why wasn't PPO used on all larger scale models? Does the training / performance of the meta-optimizer (policy gradient  vs ppo) matter?,Request,Request.Explanation,U-Neutral,Substance
1769,I would expect it would.,Other,Other,,
1770,"This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.",Evaluative,Evaluative,N-Negative,Clarity
1771,"12. ""It is worth noting that all GAN K:1 baselines perform worse than the rest and are skipped in Figure 2, echoing statements (Arjovsky, Gulrajani, Deng) that more updates of G than D might be preferable in GAN training.",Fact,Fact,,
1772,""" I disagree with this statement.",Evaluative,Evaluative,N-Negative,Substance
1773,"The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.",Evaluative,Evaluative,N-Negative,Substance
1774,"Arjovsky does discuss issues with training D to convergence, but I don't believe there is any exploration into multiple G steps per D step as a solution.",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
1775,"13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.",Evaluative,Evaluative,N-Negative,Replicability
1776,14: Claims in paper seem a little bold / overstating.,Evaluative,Evaluative,U-Neutral,Motivation/Impact
1777,"The inception gain is marginal to previous methods, and trains slower than other baselines.",Evaluative,Evaluative,U-Neutral,Substance
1778,"This is also true of MNT section -- there, the best baseline model is not even given equal training time!",Evaluative,Evaluative,U-Neutral,Substance
1779,"There are highly positive points here, such as requiring less hyperparameter search / model evaluations to find performant models.",Evaluative,Evaluative,P-Positive,Substance
1780,15. Figure 4a. Consider reformatting data (maybe histogram of differences? Or scatter plot).,Request,Request.Edit,U-Neutral,Clarity
1781,Current representation is difficult to read / parse.,Evaluative,Evaluative,N-Negative,Clarity
1783,"page 2, ""objective term. on GANs, the AutoLoss: Capital o is needed.",Request,Request.Typo,U-Neutral,Clarity
1784,Page 3: Parameter Learning heading the period is not bolded.,Request,Request.Typo,U-Neutral,Other
1785,[1] Learning step size controllers for robust neural network training.,Request,Request.Typo,U-Neutral,Other
1786,Christian Daniel et. al.,,,,
1787,[2]http://torch.ch/blog/2015/11/13/gan.html,Request,Request.Typo,U-Neutral,Other
1788,"[3] Understanding Short-Horizon Bias in Stochastic Meta-Optimization, Wu et.al.",Request,Request.Typo,U-Neutral,Other
1789,"Given the positives, and in-spite of the negatives, I would recommend to accept this paper as it discusses an interesting and novel approach when controlling multiple loss values.",Structuring,Structuring.Summary,,
1822,"The paper analyzes the strategy that a visual question answering model (FiLM) uses to verify statements containing the quantifier ""most"" (""most of the dots are red"").",Structuring,Structuring.Summary,,
1823,"It finds that the model is sensitive to the ratio of objects that satisfy the predicate (that are red) to objects that do not; as the ratio decreases (e.g. 10 red dots compared to 9 blue dots), the model's performance decreases too.",Structuring,Structuring.Summary,,
1824,This is consistent with human behavior.,Fact,Fact,,
1826,* The introduction lays out an ambitious program of comparing humans to deep neural networks.,Evaluative,Evaluative,P-Positive,Motivation/Impact
1827,* The experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and non-red items.,Evaluative,Evaluative,U-Neutral,Soundness/Correctness
1829,"* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.",Request,Request.Edit,N-Negative,Substance
1830,"While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.",Request,Request.Edit,N-Negative,Meaningful Comparison
1831,"I would like to see more discussion of whether it is at all plausible for this model to acquire the pairing strategy, compared to alternative VQA models (e.g., using relation networks).",Request,Request.Edit,U-Neutral,Meaningful Comparison
1832,* I found it difficult to follow the theoretical motivation for performing the work.,Evaluative,Evaluative,N-Negative,Motivation/Impact
1833,"The goal seems to be to test whether the network is performing the task in way that ""if not human-like, at least is cognitively plausible"".",Evaluative,Evaluative,U-Neutral,Substance
1834,I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.,Request,Request.Clarification,N-Negative,Clarity
1835,"Later in the same paragraph, the authors argue that ""in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable"".",Fact,Fact,,
1836,"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).",Evaluative,Evaluative,N-Negative,Substance
1837,"In general, I don't understand why we would want a visual question answering system that returns approximate answers --",Evaluative,Evaluative,N-Negative,Clarity
1838,isn't it better to have it count exactly how many red dots there are compared to non-red dots?,Request,Request.Experiment,U-Neutral,Soundness/Correctness
1839,"* The authors assume that explicit counting is not ""likely to be learned by the 'one-glance' feed-forward-style neural network"" evaluated in the paper. What is this statement based on? Why would a ""one-glance"" network have trouble counting objects?",Request,Request.Clarification,U-Neutral,Substance
1840,(What is a “one-glance network”?),Request,Request.Explanation,U-Neutral,Substance
1841,"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".",Evaluative,Evaluative,N-Negative,Clarity
1842,"What is ""pattern matching"" and how does it differ from ""higher-level concepts""?",Request,Request.Clarification,U-Neutral,Substance
1843,* Why would the pairing strategy in a neural network be affected by the clustering of the objects?,Request,Request.Explanation,U-Neutral,Substance
1844,"I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.",Evaluative,Evaluative,N-Negative,Substance
1846,"* Is the definition of ""most"" really a central piece of evidence for ""the apparent importance of a cardinality concept to human cognition""? Our ability to count seems sufficient to me. Perhaps I'm not understanding what the authors have in mind here.",Request,Request.Clarification,U-Neutral,Clarity
1847,"* Please use the terms ""interpretation"" and ""verification"" consistently.",Request,Request.Edit,U-Neutral,Clarity
1848,"* ""One over the other strategy"" -> ""one strategy over the other"".",Request,Request.Typo,U-Neutral,Clarity
1849,"* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.",Evaluative,Evaluative,N-Negative,Other
1850,This paper proposes an algorithm for auxiliary learning.,Structuring,Structuring.Summary,,
1851,"Given a target prediction task to be learned on training data, the auxiliary learning utilizes external training data to improve learning.",Structuring,Structuring.Summary,,
1852,"The authors focus on a setup where both target and external training data come from the same distribution but differ in class labels, where each class in the target data is a set of finer-grained classes in the auxiliary data.",Structuring,Structuring.Summary,,
1853,The authors propose a heuristic for learning from both data sets through minimization of a joint loss function.,Structuring,Structuring.Summary,,
1854,The experimental results show that the proposed methods works well on this particular setup on CIFAR data set.,Structuring,Structuring.Summary,,
1856,+ a new auxiliary learning algorithm,Evaluative,Evaluative,P-Positive,Originality
1857,+ positive results on CIFAR data set,Evaluative,Evaluative,P-Positive,Substance
1859,- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space,Evaluative,Evaluative,N-Negative,Originality
1860,- there is no attempt to provide a theoretical insight into the performance of the algorithm,Evaluative,Evaluative,N-Negative,Substance
1861,"- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance",Evaluative,Evaluative,N-Negative,Replicability
1862,"- experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario",Evaluative,Evaluative,N-Negative,Replicability
1863,"- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as ""(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set""??",Evaluative,Evaluative,N-Negative,Clarity
1864,The paper design a low variance gradient for distributions associated with continuous or discrete random variables.,Structuring,Structuring.Summary,,
1865,The gradient is designed in the way to approximate the  property of reparameterization gradient.,Structuring,Structuring.Summary,,
1866,The paper is comprehensive and includes mathematical details.,Structuring,Structuring.Summary,,
1867,I have following comments/questions,Structuring,Structuring.Heading,,
1868,1. What is the \kappa in “variable-nabla” stands for? What is the gradient w.r.t. \kappa?,Evaluative,Evaluative,U-Neutral,Clarity
1869,"2. In Eq(8), does the outer expectation w.r.t . y_{-v} be approximated by one sample? If so, it is using the local expectation method.",Evaluative,Evaluative,U-Neutral,Clarity
1870,How does that differs from Titsias & Lazaro-Gredilla(2015) both mathematically and experimentally?,Evaluative,Evaluative,U-Neutral,Clarity
1871,"3. Assume y_v is M-way categorical distribution, Eq(8) evaluates f by 2*V*M times which can be computationally expensive.",Evaluative,Evaluative,U-Neutral,Clarity
1872,What is the computation complexity of GO? How to explain the fast speed shown in the experiments?,Evaluative,Evaluative,U-Neutral,Clarity
1873,4. A most simple way to reduce the variance of REINFORCE gradient is to take multiple Monte-Carlo samples at the cost of more computation with multiple function f evaluations.,Fact,Fact,,
1874,"Assume GO gradient needs to evaluate f N times, how does the performance compared with the REINFORCE gradient with N Monte-Carlo samples?",Evaluative,Evaluative,U-Neutral,Clarity
1875,"5. In the discrete VAE experiment, upon brief checking the results in Grathwohl(2017), it shows validation ELBO for MNIST as (114.32,111.12), OMNIGLOT as (122.11,128.20) from which two cases are better than GO.",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
1876,Does the hyper parameter setting favor the GO gradient in the reported experiments?,Evaluative,Evaluative,U-Neutral,Soundness/Correctness
1877,Error bar may also be needed for comparison.,Request,Request.Experiment,U-Neutral,Meaningful Comparison
1878,What about the performance of GO gradient in the 2 stochastic layer setting in Grathwohl(2017)?,Evaluative,Evaluative,U-Neutral,Clarity
1879,"6. The paper claims GO has less parameters than REBAR/RELAX. But in Figure 9, GO has more severe overfitting. How to explain this contradicts between the model complexity and overfitting?",Evaluative,Evaluative,U-Neutral,Clarity
1951,Overview:,Structuring,Structuring.Heading,,
1952,This paper proposes modifications to the original Differentiable Neural Computer architecture in three ways.,Structuring,Structuring.Summary,,
1953,First by introducing a masked content-based addressing which dynamically induces a key-value separation,Structuring,Structuring.Summary,,
1955,"Second, by modifying the de-allocation system by also multiplying the memory contents by a retention vector before an update",Structuring,Structuring.Summary,,
1957,"Finally, the authors propose a modification in the link distribution, through renormalization.",Structuring,Structuring.Summary,,
1958,They provide some theoretical motivation and empirical evidence that it helps avoiding memory aliasing.,Structuring,Structuring.Summary,,
1959,"The authors test their approach in the some algorithm task from the DNC paper (Copy, Associative Recall and Key-Value Retrieval), and also in the bAbi dataset.",Structuring,Structuring.Summary,,
1960,"Strengths: Overall I think the paper is well-written, and proposes simple adaptions to the DNC architecture which are theoretically grounded and could be effective for improving general performance.",Evaluative,Evaluative,P-Positive,Motivation/Impact
1961,"Although the experimental results seem promising when comparing the modified architecture to the original DNC, in my opinion there are a few fundamental problems in the empirical session (see weakness discussion bellow).",Evaluative,Evaluative,N-Negative|P-Positive,Substance|Soundness/Correctness
1962,Weaknesses: Not all model modifications are studied in all the algorithmic tasks.,Evaluative,Evaluative,N-Negative,Substance
1963,"For example, in the associative recall and key-value retrieval only DNC and DNC + masking are studied.",Structuring,Structuring.Quote,,
1964,"For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.",Request,Request.Experiment,N-Negative,Meaningful Comparison
1965,"Moreover, the sparse DNC (Rae el at., 2016) is already a much better performant in this task.",Fact,Fact,,
1966,"(mean error DNC: 16.7 \pm 7.6, DNC-MD (this paper) 9.5 \pm 1.6, sparse DNC 6.4 \pm 2.5).",,,,
1967,"Although the authors mention in the conclusion that it's future work to merge their proposed changes into the sparse DNC, it is hard to know how relevant the improvements are, knowing that there are much better baselines for this task.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
1968,"It would also be good if besides the mean error rates, they reported best runs chosen by performance on the validation task, and number of the tasks solve (with < 5% error) as it is standard in this dataset.",Request,Request.Experiment,U-Neutral,Substance
1969,Smaller Notes.,Structuring,Structuring.Heading,,
1970,"1) In the abstract, I find the message for motivating the masking from the sentence  ""content based look-up results... which is not present in the key and need to be retrieved.""  hard to understand by itself. When I first read the abstract, I couldn't understand what the authors wanted to communicate with it. Later in 3.1 it became clear.",Evaluative,Evaluative,N-Negative,Clarity
1971,"2) page 3, beta in that equation is not defined",,,,
1972,3) First paragraph in page 5 uses definition of acronyms DNC-MS and DNC-MDS before they are defined.,Evaluative,Evaluative,N-Negative,Clarity
1973,"4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?",Request,Request.Clarification,N-Negative,Clarity
1974,"5)In session 3.1-3.3, for completeness. I think it would be helpful to explicitly compare the equations from the original DNC paper with the new proposed ones.",Request,Request.Experiment,U-Neutral,Substance
1975,--------------,Structuring,Structuring.Heading,,
1976,Post rebuttal update: I think the authors have addressed my main concern points and I am updating my score accordingly.,Structuring,Structuring.Summary,,
1977,"The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques.",Structuring,Structuring.Summary,,
1978,"Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process.",Structuring,Structuring.Summary,,
1979,"Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors).",Structuring,Structuring.Summary,,
1981,"- The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
1983,- The idea is a simple extension of existing work.,Evaluative,Evaluative,N-Negative,Originality
1984,"- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.",Evaluative,Evaluative,N-Negative,Substance
2007,Summary: This paper is about models for solving basic math problems.,Structuring,Structuring.Summary,,
2008,The main contribution is a synthetically generated dataset that includes a variety of types and difficulties of math problems; it is both larger and more varied than previous datasets of this type.,Structuring,Structuring.Summary,,
2009,"The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems.",Structuring,Structuring.Summary,,
2010,"The results are then analyzed and insights are derived explaining where neural models seemingly cope well with math tasks, and where they fall down.",Structuring,Structuring.Summary,,
2011,Strengths: I am happy to see the proposal of a very large dataset with a lot of different axes for measuring and examining the performance of models.,Evaluative,Evaluative,P-Positive,Motivation/Impact
2012,"There are challenging desiderata involved in building the training+tests sets, and the authors have an interesting and involved methodology to accomplish these.",Evaluative,Evaluative,P-Positive,Motivation/Impact
2013,"The paper is very clearly written. I'm not aware of a comparable work, so the novelty here seems good.",Evaluative,Evaluative,P-Positive,Originality
2014,"Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).",Evaluative,Evaluative,N-Negative,Substance
2015,It would have been useful to compare the general models here with some specific math problem-focused ones as well.,Request,Request.Experiment,N-Negative,Meaningful Comparison
2016,Some details weren't clear to me.,Evaluative,Evaluative,N-Negative,Clarity
2017,More in the comments below.,Structuring,Structuring.Heading,,
2018,"Verdict: I thought this was generally an interesting paper that has some very nice benefits, but also has some weaknesses that could be resolved. I view it as borderline, but I'm willing to change my mind based on the discussion.",Structuring,Structuring.Summary,,
2020,- One area that could stand to be improved is prior work.,Request,Request.Experiment,U-Neutral,Meaningful Comparison
2021,I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems.,Request,Request.Experiment,U-Neutral,Meaningful Comparison
2022,"Since this is the core contribution, this should also be the main comparison.",Fact,Fact,,
2023,"For example, EMLNP 2017 paper ""Deep Neural Solver for Math Word Problems"" mentions a size 60K problem dataset.",Fact,Fact,,
2024,A more extensive discussion will help convince the readers that the proposed dataset is indeed the largest and most diverse.,Fact,Fact,,
2025,"- The authors note that previous datasets are often specific to one type of problem (i.e., single variable equation solving). Why not then combine multiple types of extant problem sets?",Request,Request.Explanation,U-Neutral,Substance
2026,- The authors divide dataset construction into crowdsourcing and synthetic.,Structuring,Structuring.Quote,,
2027,"This seems incomplete to me: there are tens of thousands (probably more) of exercises and problems available in workbooks for elementary, middle, and high school students.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2028,"These are solved, and only require very limited validation.",Fact,Fact,,
2029,They are also categorized by difficulty and area.,Fact,Fact,,
2030,"Presumably the cost here would be to physically scan some of these workbooks, but this seems like a very limited investment. Why not build datasets based on workbooks, problem solving books, etc?",Request,Request.Explanation,U-Neutral,Substance
2031,- How do are the difficulty levels synthetically determined?,Request,Request.Explanation,U-Neutral,Soundness/Correctness
2032,"- When generating the questions, the authors ""first sample the answer"". What's the distribution you use on the answer? This seems like it dramatically affects the resulting questions, so I'm curious how it's selected.",Request,Request.Explanation,U-Neutral,Soundness/Correctness
2033,- The general methodology of generating questions and ensuring that no question is too rare or too frequent and the test set is sufficiently different---these are important questions and I commend the authors for providing a strong methodology.,Request,Request.Edit,U-Neutral,Soundness/Correctness
2034,- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).,Evaluative,Evaluative,N-Negative,Substance
2035,"This is certainly a scientific decision, i.e., the authors are determining which models to use in order to determine the possible insights they will derive.",Fact,Fact,,
2036,But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful.,Request,Request.Clarification,U-Neutral,Soundness/Correctness
2037,"In fact, assuming that such methods outperform general-purpose models, we could investigate why and where this is the case (in fact the proposed dataset is very useful for this).",Fact,Fact,,
2038,"On the other hand, if these specialized approaches largely fail to outperform general-purpose models, we would have the opposite insights---that these models' benefits are dataset-specific and thus limited.",Fact,Fact,,
2039,- Really would be good to do real-world tests in a more extensive way.,Request,Request.Experiment,U-Neutral,Substance
2040,"A 40-question exam for 16 year olds is probably far too challenging for the current state of general recurrent models. Can you add some additional grades here, and more questions?",Request,Request.Edit,U-Neutral,Substance
2041,"- For the number of thinkings steps, how does it scale up as you increase it from 0 to 16? Is there a clear relationships here?",Request,Request.Explanation,U-Neutral,Clarity
2042,"- The 1+1+...+1 example is pretty intriguing, and could be a nice ""default"" question!",Fact,Fact,,
2043,"- Minor typo: in the abstract: ""test spits"" should be ""test splits""",Request,Request.Typo,U-Neutral,Clarity
2073,"edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns.",Evaluative,Evaluative,P-Positive,Substance
2074,I think the paper is much more convincing now. I am happy to increase my rating to clear accept.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
2075,"I also agree with the introduction of the Chi vector, and with the use of the term of ""photometric BA"", since it was used before, even if it is unfortunate in my opinion.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
2076,"I thank the authors to replace reprojection by alignment, which is much clearer.",Social,Social,,
2077,---------------,Other,Other,,
2078,This paper presents a method for dense Structure-from-Motion using Deep Learning:,Structuring,Structuring.Summary,,
2079,The input is a set of images; the output is the camera poses and the depth maps for all the images.,Structuring,Structuring.Summary,,
2080,The approach is inspired by Levenberg-Marquardt optimization (LM): A pipeline extracting image features computes the Jacobian of an error function.,Structuring,Structuring.Summary,,
2081,This Jacobian is used to update an estimate of the camera poses.,Structuring,Structuring.Summary,,
2082,"As in LM optimization, this update is done based on a factor lambda, weighting a gradient descent step and a Gauss-Newton step.",Structuring,Structuring.Summary,,
2083,"In LM optimization, this lambda evolves with the improvement of the estimate.",Structuring,Structuring.Summary,,
2084,Here lambda is also predicted using a network based on the feature difference.,Structuring,Structuring.Summary,,
2085,"If I understand correctly, what is learned is how to compute image features that provide good updates, how to predict the depth maps from the features, and how to predict lambda.",Structuring,Structuring.Summary,,
2086,The method is compared against DeMoN and other baselines with good results.,Evaluative,Evaluative,P-Positive,Substance
2087,"I like the fact that the method is based on LM optimization, which is the standard method in 'geometric bundle adjustment', while related works consider Gauss-Newton-like optimization steps.",Evaluative,Evaluative,P-Positive,Meaningful Comparison
2088,The key was to include a network to predict lambda as well.,Fact,Fact,,
2089,"However, I have several concerns:",Structuring,Structuring.Heading,,
2090,* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
2091,The image features learned with the proposed method are re-used in an approach using a fixed lambda.,Structuring,Structuring.Quote,,
2092,"If I understand correctly, there are 2 things wrong with that:",Structuring,Structuring.Heading,,
2093,"- for GN optimization, lambda should be set to 0 - not a constant value.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2094,Several constant values should also have been tried.,Evaluative,Evaluative,N-Negative,Substance
2095,"- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2096,"Thus, the advantage of using a LM optimization scheme is not very convincing.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2097,"Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.",Evaluative,Evaluative,N-Negative,Substance
2098,"* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.",Evaluative,Evaluative,N-Negative,Substance
2099,Less critical concerns that still should be taken into account if the paper is accepted:,Structuring,Structuring.Heading,,
2100,"- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2101,- the name 'Bundle Adjustment' is actually not adapted to the proposed method.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
2102,"'Bundle Adjustment' in 'geometric computer vision' comes from the optimization of several rays to intersect at the same 3D point, which is done by minimizing the reprojection errors.",Fact,Fact,,
2103,Here the objective function is based on image feature differences.,Fact,Fact,,
2104,I thus find the name misleading.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
2105,The end of Section 3 also encourages the reader to think that the proposed method is based on the reprojection error.,Structuring,Structuring.Quote,,
2106,The proposed method is more about dense alignment for multiple images.,Structuring,Structuring.Summary,,
2107,More minor points:,Structuring,Structuring.Heading,,
2108,1st paragraph:,Structuring,Structuring.Heading,,
2109,Marquet -> Marquardt,Request,Request.Typo,N-Negative,Soundness/Correctness
2110,title of Section 3: revisitED,Request,Request.Typo,N-Negative,Soundness/Correctness
2111,1st paragraph of Section 3: audience -> reader,Request,Request.Edit,N-Negative,Soundness/Correctness
2112,caption of Fig 1: extractS,Request,Request.Typo,N-Negative,Soundness/Correctness
2113,Eq (2) cannot have Delta Chi on the two sides.,Request,Request.Edit,N-Negative,Soundness/Correctness
2114,"Typically, the left side should be \hat{\Delta \Chi}",Fact,Fact,,
2115,before Eq (3): the 'photometric ..' -> a 'photometric ..',Request,Request.Typo,N-Negative,Soundness/Correctness
2116,1st paragraph of Section 4.3: difficulties -> reason,Request,Request.Edit,N-Negative,Soundness/Correctness
2117,typo in absolute in caption of Fig 4,Request,Request.Typo,N-Negative,Soundness/Correctness
2118,Eq (6): Is B the same for all scenes?,Request,Request.Clarification,U-Neutral,Clarity
2119,It would be interesting to visualize it.,Request,Request.Experiment,U-Neutral,Substance
2120,Section 4.5: applies -,Request,Request.Typo,N-Negative,Soundness/Correctness
2121,> apply,,,,
2122,The paper presents a an interesting novel approach to train neural networks with so called peer regularization which aims to provide robustness to adversarial attacks.,Structuring,Structuring.Summary,,
2123,The idea is to add a graph neural network to a spatial CNN.,Structuring,Structuring.Summary,,
2124,A graph is defined over similar training samples which are found using a Monte Carlo approximation.,Structuring,Structuring.Summary,,
2125,The regularization using graphs reminds me of recent work at ICML on semi-supervised learning (Kamnitsas et al. (2018) Semi-supervised learning via compact latent space clustering) which is using a graph to approximate cluster density which acts as a regularizer for training on labelled data.,Other,Other,,
2126,The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.,Evaluative,Evaluative,N-Negative,Replicability
2127,"Memory and computation limitations are mentioned, but not sufficently discussed.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2128,It would be good to add further details on practical limitations.,Request,Request.Edit,U-Neutral,Soundness/Correctness
2129,"Experiments are limited to benchmark data using MNIST, CIFAR-10, CIFAR-100.",Structuring,Structuring.Quote,,
2130,Comprehensive evaluation has been carried out with insightful experiments and good comparison to state-of-the-art.,Evaluative,Evaluative,P-Positive,Substance
2131,Both white- and black-box adversarial attacks are explored with promising results for the proposed approach.,Evaluative,Evaluative,P-Positive,Substance
2132,"However, it is difficult to draw conclusions for real-world problems of larger scale.",Evaluative,Evaluative,N-Negative,Other
2133,"The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2134,"It is stated that future work will aim at scaling PeerNets to benchmarks like ImageNet, but it is unclear how this could be done.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2135,Is there any hope this could be applied to problems like 3D imaging data or videos?,Request,Request.Explanation,U-Neutral,Soundness/Correctness
2157,"In this paper, the authors consider CNN models from the lens of kernel methods.",Structuring,Structuring.Summary,,
2158,"They build upon past work that showed that such models can be seen to lie in appropriate RKHS, and derive upper and lower bounds for the kernel norm.",Structuring,Structuring.Summary,,
2159,"These bounds can be used as regularizers that help train more robust neural networks, especially in the context of euclidean perturbations of the inputs, and training GANs.",Structuring,Structuring.Summary,,
2160,They show that the bounds can also be used to recover existing special cases such as spectral norm penalizations and gradient regularization.,Structuring,Structuring.Summary,,
2161,"They derive generalization bounds from the point of view of adversarial learning, and report experiments to buttress their claims.",Structuring,Structuring.Summary,,
2162,"Overall, the paper is a little confusing.",Evaluative,Evaluative,N-Negative,Clarity
2163,"A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.",Evaluative,Evaluative,N-Negative,Originality
2164,"It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).",Evaluative,Evaluative,N-Negative,Originality
2165,"It might be nice to carefully delineate the authors' work from the former, and present their contributions.",Request,Request.Explanation,N-Negative,Originality
2166,"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2167,"Page 4 last paragraph: ""One advantage ... complex architectures in practice"" : True, but the tightness of the bounds *do* depend on ""f"" (specifically the RKHS norm).",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2168,"It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?",Request,Request.Explanation,N-Negative,Soundness/Correctness
2169,eqn (8): use something else to denote the function 'U'.,Request,Request.Edit,N-Negative,Clarity
2170,You used 'U' before to denote the set.,Request,Request.Edit,N-Negative,Clarity
2171,eqn (12): does \tilde{O} hide polylog factors? please clarify.,Request,Request.Explanation,N-Negative,Replicability
2186,"This paper addresses a novel variant of AutoML, to automatically learn and generate optimization schedules for iterative alternate optimization problems.",Structuring,Structuring.Summary,,
2187,"The problem is formulated as a RL problem, and comprehensive experiments on four various applications have demonstrated that the optimization schedule produced can guide the task model to achieve better quality of convergence, more sample-efficient, and the trained controller is transferable between datasets and models.",Structuring,Structuring.Summary,,
2188,"Overall, the writing is quite clear, the problem is interesting and important, and the results are promising.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
2189,Some suggestions:,Structuring,Structuring.Heading,,
2190,"1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?",Request,Request.Explanation,U-Neutral,Substance
2191,More discussions on these questions can be very helpful to further understand the proposed method.,Request,Request.Explanation,U-Neutral,Substance
2192,"2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.",Request,Request.Clarification,U-Neutral,Replicability
2193,3. Any plan for open source ?,Request,Request.Clarification,U-Neutral,Replicability
2194,This paper describes the method for performing self-training where the unlabeled datapoints are iteratively added to the training set only if their predictions by the classifier are confident enough.,Structuring,Structuring.Summary,,
2195,The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection.,Fact,Fact,,
2196,"On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.",Fact,Fact,,
2197,"The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.",Evaluative,Evaluative,N-Negative,Clarity
2198,"Also, the writing can be improved by making the writing more concise and formal (examples of informal: ""spoil the network"", ""model is spoiled"", ""problem of increased classes"", ""many recent researches have been conducted"", ""lots of things to consider for training"", ""supervised learning was trained"" etc.).",Request,Request.Edit,N-Negative,Clarity
2199,The contributions of the method could also be underlined more clearly in the abstract and introduction.,Request,Request.Edit,N-Negative,Clarity
2200,The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.,Evaluative,Evaluative,N-Negative,Clarity
2201,The idea of selective sampling for self-training is promising and the investigated questions are interesting.,Evaluative,Evaluative,P-Positive,Originality
2202,"As far as I understand, the main contribution of this paper is the use of separate ""selection network"" to estimate the confidence of predictions by ""classification network"".",Evaluative,Evaluative,U-Neutral,Originality
2203,"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.",Evaluative,Evaluative,N-Negative,Originality
2204,"For example, imagine the case of binary classification.",Other,Other,,
2205,"If the selection network predicts 0 in come cases, it can be used to improve the result of ""classification network"" by flipping the corresponding label.",Fact,Fact,,
2206,How can you interpret such a thought experiment?,Request,Request.Explanation,U-Neutral,Substance
2207,"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.",Evaluative,Evaluative,N-Negative,Substance
2208,"Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of ""classification network"" is greater than some threshold?",Request,Request.Explanation,U-Neutral,Substance
2209,"Finally, could you show a plot of top-1 prediction of ""classification network"" vs score of ""selection network"" and elaborate on that?",Request,Request.Experiment,U-Neutral,Substance
2210,"Then, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes.",Fact,Fact,,
2211,"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2212,"Ablation study shows that the use of the ""selection network"" strategy does not improve the results without these heuristics.",Evaluative,Evaluative,N-Negative,Substance
2213,"It would be interesting to see how these heuristics would do without ""selection network"", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG.",Request,Request.Experiment,U-Neutral,Substance
2214,"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.",Evaluative,Evaluative,N-Negative,Originality
2215,It is very valuable that the experimental results include many recently proposed methods.,Evaluative,Evaluative,P-Positive,Meaningful Comparison
2216,"Besides, the settings are described in details that could help for the reproducibility of the results.",Evaluative,Evaluative,P-Positive,Replicability
2217,"However, I have a few concerns about the results.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2218,"First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).",Evaluative,Evaluative,N-Negative,Originality
2219,"Besides, as the base classifier is different for various baselines, it is hard to compare the methods.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
2220,"Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).",Evaluative,Evaluative,N-Negative,Clarity
2221,How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice?,Request,Request.Explanation,U-Neutral,Substance
2222,Another important parameters is the number of iterations of the algorithm.,Request,Request.Clarification,U-Neutral,Substance
2223,How was it chosen?,Request,Request.Clarification,U-Neutral,Substance
2224,"Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes?",Request,Request.Explanation,U-Neutral,Substance
2225,What would happen if you use random class splits or split animal classes (like in a more realistic scenario)?,Request,Request.Explanation,U-Neutral,Substance
2226,"To conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.",Structuring,Structuring.Summary,,
2227,Some questions and comments:,Structuring,Structuring.Heading,,
2228,- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?,Request,Request.Explanation,U-Neutral,Substance
2229,"- In the training procedure of ""selection network"" of Sections 3.1, do you use the same datapoints to train a ""classification network"" and ""selection network""? If it is the case, how do you insure that the ""classification network"" does not learn to fit the data perfectly and thus all labels s_i are 1?",Request,Request.Explanation,U-Neutral,Substance
2230,"- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?",Request,Request.Explanation,U-Neutral,Substance
2231,"- What do you mean in section 3.3 by ""if one class dominates the dataset, the model tends to overfit""?",Request,Request.Explanation,U-Neutral,Substance
2232,- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.,Request,Request.Edit,U-Neutral,Other
2233,- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?,Request,Request.Explanation,U-Neutral,Clarity
2234,- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?,Request,Request.Experiment,U-Neutral,Substance
2235,"- Can you explain the sentence ""To prevent data being added suddenly, no data was added until 5 iterations""?",Request,Request.Explanation,U-Neutral,Substance
2236,- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?,Request,Request.Explanation,U-Neutral,Substance
2237,The paper proposes a method to disentangle latent variables for certain factors of interest in an image by considering the original input image and a transformation of the image where information about the factors of interest is removed.,Fact,Fact,,
2238,The generative process is then modeled by having two latent variables --  the first responsible for generating the transformed image whereas both latent variables are responsible for generating the original input image.,Fact,Fact,,
2239,"This inductive bias naturally enforces that the second latent variable will not model the information which the first needs to reconstruct the transformed image, due to the VAE objective penalizing redundancy in information present in the latents.",Fact,Fact,,
2240,"The paper demonstrates this in one setting where the transformation is random shuffling of image patches, which should remove the global information of the original input image.",Fact,Fact,,
2241,The methodology of the paper was concise and easy to follow.,Evaluative,Evaluative,P-Positive,Clarity
2242,The simple inductive bias presented in the paper for disentangling local and global information is very interesting.,Evaluative,Evaluative,P-Positive,Originality
2243,"It is not obvious that shuffling image patches at a particular scale would lead to complete loss of global information, but the paper does show results on SVHN and CIFAR10 for which global information is sufficiently disentangled.",Evaluative,Evaluative,N-Negative|P-Positive,Clarity|Substance
2244,The results for digit identity clustering were great for showing the correlation between their learnt global information and label information.,Evaluative,Evaluative,P-Positive,Substance
2245,"The paper introduced their model as a general purpose strategy for placing desired information in latent variables using auxiliary tasks, but focus was directed to the global vs local line of analysis.",Fact,Fact,,
2246,"While giving examples for what kind of information can be removed, the authors mentioned that color to gray-scale might be one possibility.",Fact,Fact,,
2247,It would have been interesting to see this and other possibilities explored in the paper.,Request,Request.Experiment,N-Negative,Substance
2248,I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.,Request,Request.Experiment,N-Negative,Substance
2249,It is mentioned in the paper that having a single inference network for the posterior as opposed to the factorized one is conceivable.,Fact,Fact,,
2250,I would be curious to see an analysis of how that works out as compared to the separate encoders case.,Request,Request.Experiment,N-Negative,Substance
2251,"Overall, the paper has a novel idea which is well motivated and executed in terms of experiments.",Evaluative,Evaluative,P-Positive,Substance|Originality
2299,The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.,Structuring,Structuring.Summary,,
2300,"There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly.",Evaluative,Evaluative,N-Negative,Originality
2301,"However, the paper does cover a setup that I am not aware that was studied before.",Evaluative,Evaluative,P-Positive,Motivation/Impact
2302,"The paper is written clearly, and the experiments seem solid.",Evaluative,Evaluative,P-Positive,Clarity
2304,-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly.,Request,Request.Explanation,U-Neutral,Soundness/Correctness
2305,-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2),Evaluative,Evaluative,N-Negative,Clarity
2306,-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization,Request,Request.Edit,N-Negative,Soundness/Correctness
2345,"This paper is about using ""neural stethoscopes"", small complementary neural networks that are added to a main network which with their auxilary loss functions can measure suitability of features or guide the learning process.",Structuring,Structuring.Summary,,
2346,"The idea is incremental to multi-task learning and enable, in a single framework, to validate intermediate features for additional related tasks.",Structuring,Structuring.Summary,,
2347,Moreover it can promote or suppress the correlation of such features to the tasks related to the main one.,Structuring,Structuring.Summary,,
2348,The framework is applied to the task of visual stability prediction of block towers.,Structuring,Structuring.Summary,,
2349,"The paper builds upon Groth et al. 2018, adding the concept of local stability as correlated secondary task, used with the proposed neural stethoscopes.",Structuring,Structuring.Summary,,
2350,"Experiments with an extension of ShapeStacks (Groth et al. 2018) dataset where the local stability is added to the global stability class, show that it is possibile increase the performance using the additional task.",Structuring,Structuring.Summary,,
2351,"Moreover, it is shown that neural stethoscopes can suppress nuisance information when using a biased training dataset where the local and global stability are purposely inversely correlated.",Structuring,Structuring.Summary,,
2353,"+ A very nice paper, well written and easy to read. Figures are helpful and the structure is clear.",Evaluative,Evaluative,P-Positive,Clarity
2354,+ The concept of neural stethoscope is interesting and simplify the concepts behind multitask learning.,Evaluative,Evaluative,P-Positive,Motivation/Impact
2355,"+ Experiments are convincing, interesting and there is some novelty in vision stability prediction.",Evaluative,Evaluative,P-Positive,Substance
2357,"- The novelty is limited related to multitask learning, thus it is an incremental paper.",Evaluative,Evaluative,N-Negative,Motivation/Impact
2372,This paper proposed a framework to incorporate GAN into MAP inference process for general image restoration.,Structuring,Structuring.Summary,,
2373,"First, the motivation of the proposed framework is not convincing for me.",Evaluative,Evaluative,N-Negative,Motivation/Impact
2374,"That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.",Evaluative,Evaluative,N-Negative,Motivation/Impact
2375,"However, in real world scenarios, it is actually challenging to obtain exact degradation information.",Evaluative,Evaluative,N-Negative,Motivation/Impact
2376,Thus we may only apply the proposed model on a few tasks with exactly known F.,Evaluative,Evaluative,N-Negative,Motivation/Impact
2377,"Second, due to the norm based constraints, authors actually need to optimize a highly nonconvex optimization problem.",Evaluative,Evaluative,N-Negative,Substance
2378,"Moreover, due to the trace based loss function, the computational cost will also be very high.",Evaluative,Evaluative,N-Negative,Substance
2379,"Please notice that standard MAP based methods only need to solve a simple convex optimization model (e.g., TV) and these methods can also be applied for different restoration tasks.",Evaluative,Evaluative,U-Neutral,Substance
2380,"Actually, we only need to specify particular fidelity terms for different tasks.",Fact,Fact,,
2381,"Moreover, very recent works have also successfully incorporate both generative and discriminative network architectures (e.g., [1,2]) into the optimization process.",Evaluative,Evaluative,U-Neutral,Meaningful Comparison
2382,"Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.",Evaluative,Evaluative,N-Negative,Originality
2383,"Finally, the experimental part is also too weak to evaluate the proposed method.",Evaluative,Evaluative,N-Negative,Substance
2384,"As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.",Evaluative,Evaluative,N-Negative,Meaningful Comparison
2385,Some works actually also incorporate generative and/or discriminative networks into MAP inference process for these tasks.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
2386,Thus I believe authors must compare their method with these state-of-the-art approaches.,Evaluative,Evaluative,N-Negative,Meaningful Comparison
2387,"Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.",Request,Request.Experiment,N-Negative,Substance
2388,"This is because the digitals images in MNIST do not have rich texture and detail structures, thus are not very challenging for standard image restoration methods.",Request,Request.Experiment,U-Neutral,Substance
2389,"[1]. Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang: Learning Deep CNN Denoiser Prior for Image Restoration. CVPR 2017: 2808-2817",Other,Other,,
2390,"[2]. Jiawei Zhang, Jin-shan Pan, Wei-Sheng Lai, Rynson W. H. Lau, Ming-Hsuan Yang: Learning Fully Convolutional Networks for Iterative Non-blind Deconvolution. CVPR 2017: 6969-6977",Other,Other,,
2391,This paper provides a new dynamic perspective on deep neural network.,Structuring,Structuring.Summary,,
2392,"Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers.",Structuring,Structuring.Summary,,
2393,"Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system.",Structuring,Structuring.Summary,,
2394,Local performance around the fixed point is explored.,Structuring,Structuring.Summary,,
2395,Extensions are provided to include the batch normalization.,Structuring,Structuring.Summary,,
2396,I believe this paper may stimulate some interesting ideas for other researchers.,Evaluative,Evaluative,P-Positive,Motivation/Impact
2397,Two technical questions:,Structuring,Structuring.Heading,,
2398,"1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point.",Fact,Fact,,
2399,How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?,Request,Request.Explanation,N-Negative,Clarity
2400,"This somewhat conflicts the commonsense of ""the deeper the better?""",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2401,"2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold.",Request,Request.Edit,N-Negative,Soundness/Correctness
2402,This paper investigates the effect of the batch normalization in DNN learning.,Structuring,Structuring.Summary,,
2403,The mean field theory in statistical mechanics was employed to analyze the,Structuring,Structuring.Summary,,
2404,progress of variance matrices between layers.,,,,
2405,"As the results, the batch normalization itself is found to be the cause of gradient explosion.",Structuring,Structuring.Summary,,
2406,"Moreover, the authors pointed out that near-linear activation function can improve such gradient explosion.",Structuring,Structuring.Summary,,
2407,Some numerical studies were reported to confirm theoretical findings.,Structuring,Structuring.Summary,,
2408,The detailed analysis of the training of DNN with the batch normalization is quite interesting.,Evaluative,Evaluative,P-Positive,Substance
2409,There are some minor comments below.,Structuring,Structuring.Heading,,
2410,"- in page 3, 2line above eq(2): what is delta in the variance of the multivariate normal distribution?",Request,Request.Clarification,U-Neutral,Clarity
2411,- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.,Evaluative,Evaluative,N-Negative,Clarity
2412,"- The randomized weight is not very practical. Though it may be the standard approach of mean field,",Request,Request.Edit,N-Negative,Soundness/Correctness
2413,some comments would be helpful to the readers.,,,,
2414,This paper develops a framework for evaluating the ability of neural models on answering free-form mathematical problems.,Structuring,Structuring.Summary,,
2415,"The contributions are i) a publicly available dataset, and ii) an evaluation of two existing model families, recurrent networks and the Transformer.",Structuring,Structuring.Summary,,
2416,I think that this paper makes a good contribution by establishing a benchmark and providing some preliminary results.,Evaluative,Evaluative,P-Positive,Motivation/Impact
2417,"I am biased because I once did exactly the same thing as this paper, although at a much smaller scale; I am thus happy to see such a public dataset.",Social,Social,,
2418,The paper is a reasonable dataset/analysis paper.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
2419,Whether to accept it or not depends on what standard ICLR has towards such papers (ones that do not propose a new model/new theory).,Fact,Fact,,
2420,I think that the dataset generation process is well-thought-out.,Evaluative,Evaluative,P-Positive,Soundness/Correctness
2421,"There are a large variety of modules, and trying to not generate either trivial or impossible problems is a plus in my opinion.",Evaluative,Evaluative,P-Positive,Motivation/Impact
2422,"The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.",Evaluative,Evaluative,N-Negative,Substance
2423,"I think the authors should move a portion of the big bar plot (too low resolution, btw) into the main text and discuss it thoroughly.",Request,Request.Edit,U-Neutral,Substance
2424,"Details on how to generate the dataset, however, can be moved into the appendix.",Request,Request.Edit,U-Neutral,Clarity
2425,"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?",Request,Request.Experiment,N-Negative,Substance
2426,One other thing I want to see is a test set with multiple different difficulty levels.,Request,Request.Experiment,U-Neutral,Substance
2427,"The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? And what about the transfer between these tasks (e.g., if a network learns to solve equations with both x and y and also factorise a polynomial with x, can it generalize to the unseen case of factorising a polynomial with both x and y)?",Request,Request.Explanation,U-Neutral,Soundness/Correctness
2428,"Also, is there an option for ""unsolvable""?",Request,Request.Clarification,U-Neutral,Soundness/Correctness
2429,"For example, the answer being a special ""this is impossible"" character for ""factorise x^2 - 5"" (if your training set does not use \sqrt, of course).",Fact,Fact,,
2430,"This is an interesting paper with a new approach to learn a sparse, positive (and hence interpretable) semantic space that maximizes human similarity judgements, by training to specifically maximize the prediction of human similarity judgements.",Structuring,Structuring.Summary,,
2431,The authors have collected the dataset themselves and have rating of sets of 3 objects from 1854 unique objects.,Structuring,Structuring.Summary,,
2432,They end up with a space (SPoSE) with relatively low dimensionality with respect to usual word embeddings (49 dimension) but perhaps not surprising when considering the small size of the words to embed.,Structuring,Structuring.Summary,,
2433,The authors run a set of experiment to show the usefulness of SPoSE.,Structuring,Structuring.Summary,,
2434,"The most interesting one is the prediction of its dimensions by the CSLB features, which reveals a nice clustering in the different SPoSE dimensions.",Structuring,Structuring.Summary,,
2435,Perhaps the results would be a little more convincing if additional common word embeddings were also tested.,Evaluative,Evaluative,N-Negative,Substance
2436,"Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.",Evaluative,Evaluative,N-Negative,Substance
2437,A good extension of this work would be to combine a text-derived embedding  or the synsets to interpolate the SPoSE dimensions for missing words in the original set.,Request,Request.Experiment,U-Neutral,Substance
2438,Or perhaps the object similarity ratings could be used in a semi-supervised setting to inform the learning of a co-occurence word embedding.,Request,Request.Experiment,U-Neutral,Substance
2439,This will allow the model to better describe a larger set of words.,Request,Request.Experiment,U-Neutral,Substance
2440,Another possible extension is to test this larger set of words on a non-behavioral NLP task to show possible improvements that the behavioral data and the interpretable space give.,Request,Request.Experiment,U-Neutral,Substance
2532,The authors proposed a Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.,Structuring,Structuring.Summary,,
2533,The authors define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.,Structuring,Structuring.Summary,,
2534,It replaces the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective.,Structuring,Structuring.Summary,,
2535,"By further conditioning our system on several different instruments, the proposed method can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.",Structuring,Structuring.Summary,,
2536,"Some detailed comments are listed as follow,",Structuring,Structuring.Heading,,
2537,1 The implementation steps of the proposed method (MoVE) are not clear.,Evaluative,Evaluative,N-Negative,Clarity
2538,"Some details are missing, which is hardly reproduced by the other researchers.",Evaluative,Evaluative,N-Negative,Replicability
2539,2 The experimental settings are not reasonable.,Evaluative,Evaluative,N-Negative,Substance
2540,The current experimental settings are not matched with the practice environment.,Evaluative,Evaluative,N-Negative,Substance
2541,3 The proposed method can transfer the positive knowledge.,Structuring,Structuring.Heading,,
2542,"However, some negative knowledge information can be also transferred.",Structuring,Structuring.Heading,,
2543,So how to avoid the negative transferring?,Evaluative,Evaluative,U-Neutral,Soundness/Correctness
2544,"4 For the model, the optimization details or inferring details are missing, which are important for the proposed model.",Evaluative,Evaluative,N-Negative,Substance
2563,Verifying the properties of neural networks can be very difficult.,Structuring,Structuring.Summary,,
2564,Instead of,Structuring,Structuring.Summary,,
2565,"finding a formal proof for a property that gives a True/False answer, this",Structuring,Structuring.Summary,,
2566,paper proposes to take a sufficiently large number of samples around the input,Structuring,Structuring.Summary,,
2567,point point and estimate the probability that a violation can be found.,Structuring,Structuring.Summary,,
2568,Naive,Structuring,Structuring.Summary,,
2569,Monte-Carlo (MC) sampling is not effective especially when the dimension is,Structuring,Structuring.Summary,,
2570,"high, so the author proposes to use adaptive multi-level splitting (AMLS) as a",Structuring,Structuring.Summary,,
2571,sampling scheme.,Structuring,Structuring.Summary,,
2572,This is a good application of AMLS method.,Evaluative,Evaluative,P-Positive,Motivation/Impact
2573,Experiments show that AMLS can make a good estimate (similar quality as naive,Evaluative,Evaluative,P-Positive,Motivation/Impact
2574,"MC with a large number of samples) while using much less samples than MC, on",Evaluative,Evaluative,P-Positive,Motivation/Impact
2575,both small and relatively larger models,Evaluative,Evaluative,P-Positive,Motivation/Impact
2577,"Additionally, the authors conduct",Evaluative,Evaluative,P-Positive,Originality
2578,sensitivity analysis and run the proposed algorithm with many different,Evaluative,Evaluative,P-Positive,Originality
2579,"parameters (M, N, pho, etc), which is good to see.",Evaluative,Evaluative,P-Positive,Originality
2580,I have some concerns on this paper:,Evaluative,Evaluative,N-Negative,Soundness/Correctness
2581,I have doubts on applying the proposed method to higher dimensional inputs.,Evaluative,Evaluative,N-Negative,Clarity
2582,In,Evaluative,Evaluative,N-Negative,Clarity
2583,"section 6.3, the authors show an experiments in this case, but only on a dense",Evaluative,Evaluative,N-Negative,Clarity
2584,"ReLU network with 2 hidden layers, and it is unknown if it works in general.",Evaluative,Evaluative,N-Negative,Clarity
2585,How does the number of required samples increases when the dimension of input,Evaluative,Evaluative,U-Neutral,Substance
2586,(x) increases?,Evaluative,Evaluative,U-Neutral,Substance
2587,"Formally, if there exists a violation (counter-example) for a certain property,",Evaluative,Evaluative,U-Neutral,Substance
2588,"and given a failure probability p, what is the upper bound of number of samples",Evaluative,Evaluative,U-Neutral,Substance
2589,"(in terms of input dimension, and other factors) required so that the",Evaluative,Evaluative,U-Neutral,Substance
2590,probability we cannot detect this violation with probability less than p?,Evaluative,Evaluative,U-Neutral,Substance
2591,"Without such a guarantee, the proposed method is not very useful because we",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2592,have no idea how confident the sampling based result is.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
2593,Verification needs,Evaluative,Evaluative,N-Negative,Substance
2594,"something that is either deterministic, or a probabilistic result with a small",Evaluative,Evaluative,N-Negative,Substance
2595,"and bounded failure rate, otherwise it is not really a verification method.",Evaluative,Evaluative,N-Negative,Substance
2596,The experiments of this paper lack comparisons to certified verification,Request,Request.Experiment,N-Negative,Substance
2597,methods. There are some scalable property verification methods that can give a,Request,Request.Experiment,N-Negative,Substance
2598,lower bound on the input perturbation (see [1][2][3]),Request,Request.Experiment,N-Negative,Substance
2600,These methods can,Evaluative,Evaluative,P-Positive,Substance
2601,"guarantee that when epsilon is smaller than a threshold, no violations can be",Evaluative,Evaluative,P-Positive,Substance
2602,found.,Evaluative,Evaluative,P-Positive,Substance
2603,"On the other hand, adversarial attacks give an upper bound of input",Fact,Fact,,
2604,perturbation by providing a counter-example (violation).,Fact,Fact,,
2605,The authors should,Request,Request.Experiment,U-Neutral,Meaningful Comparison
2606,compare the sampling based method with these lower and upper bounds.,Request,Request.Experiment,U-Neutral,Meaningful Comparison
2607,For,Request,Request.Experiment,U-Neutral,Meaningful Comparison
2608,"example, what is log(I) for epsilon larger than upper bound?",Request,Request.Experiment,U-Neutral,Meaningful Comparison
2609,"Additionally, in section 6.4, the results in Figure 2 also does not look very",Request,Request.Experiment,N-Negative,Clarity
2610,positive - it unlikely to be true that an undefended network is predominantly,Request,Request.Experiment,N-Negative,Clarity
2611,"robust to perturbation of size epsilon = 0.1. Without any adversarial training,",Request,Request.Experiment,N-Negative,Clarity
2612,adversarial examples (or counter-examples for property verification) with L_inf,Request,Request.Experiment,N-Negative,Clarity
2613,distortion less than 0.1 (at least on some images) should be able to find.,Request,Request.Experiment,,Clarity
2614,It,Request,Request.Experiment,U-Neutral,Substance
2615,is better to conduct strong adversarial attacks after each epoch and see what,Request,Request.Experiment,U-Neutral,Substance
2616,are the epsilons of adversarial examples.,Request,Request.Experiment,U-Neutral,Substance
2617,Ideas on further improvement:,Request,Request.Experiment,U-Neutral,Substance
2618,The proposed method can become more useful if it is not a point-wise method.,Request,Request.Experiment,U-Neutral,Motivation/Impact
2619,"If given a point, current formal verification method can tell if a property is",Request,Request.Experiment,U-Neutral,Motivation/Impact
2620,hold or not.,Request,Request.Experiment,U-Neutral,Motivation/Impact
2621,"However, most formal verification method cannot deal with a input",Fact,Fact,,
2622,"drawn from a distribution randomly (for example, an unseen test example).",Fact,Fact,,
2623,This,Request,Request.Experiment,U-Neutral,Motivation/Impact
2624,is the place where we really need a probabilistic verification method.,Request,Request.Experiment,U-Neutral,Motivation/Impact
2625,The,Evaluative,Evaluative,N-Negative,Motivation/Impact
2626,setting in the current paper is not ideal because a probabilistic estimate of,Evaluative,Evaluative,N-Negative,Motivation/Impact
2627,"violation of a single point is not very useful, especially without a guarantee",Evaluative,Evaluative,N-Negative,Motivation/Impact
2628,of failure rates.,Evaluative,Evaluative,N-Negative,Motivation/Impact
2629,"For finding counter-examples for a property, using gradient based methods might",Request,Request.Experiment,U-Neutral,Substance
2630,be a better way.,Request,Request.Experiment,U-Neutral,Substance
2631,The authors can consider adding Hamiltonian Monte Carlo,Request,Request.Edit,U-Neutral,Other
2632,to,Request,Request.Edit,U-Neutral,Other
2633,this framework,Request,Request.Edit,U-Neutral,Other
2634,(See [4]).,Other,Other,,
2635,References:,Other,Other,,
2636,"There are some papers from the same group of authors, and I merged them to one.",Other,Other,,
2637,"Some of these papers are very recent, and should be helpful for the authors",Other,Other,,
2638,to further improve their work.,Other,Other,,
2639,"[1] ""AI2: Safety and Robustness Certification of Neural Networks with Abstract",Other,Other,,
2640,"Interpretation"", IEEE S&P 2018 by Timon Gehr, Matthew Mirman, Dana",Other,Other,,
2641,"Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin Vechev",Other,Other,,
2642,"(see also ""Differentiable Abstract Interpretation for Provably Robust Neural",Other,Other,,
2643,"Networks"", ICML 2018.",Other,Other,,
2644,"by Matthew Mirman, Timon Gehr, Martin Vechev.  They also",Other,Other,,
2645,"have a new NIPS 2018 paper ""Fast and Effective Robustness Certification"" but is",Other,Other,,
2646,not on arxiv yet),Other,Other,,
2647,"[2] ""Efficient Neural Network Robustness Certification with General Activation",Other,Other,,
2648,"Functions""",Other,Other,,
2649,", NIPS 2018. by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui",Other,Other,,
2650,"Hsieh, Luca Daniel.",Other,Other,,
2651,"(see also ""Towards Fast Computation of Certified Robustness for ReLU Networks"",",Other,Other,,
2652,"ICML 2018 by Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh,",Other,Other,,
2653,"Duane Boning, Inderjit S. Dhillon, Luca Danie.)",Other,Other,,
2654,[3] Provable defenses against adversarial examples via the convex outer,Other,Other,,
2655,"adversarial polytope, NIPS 2018. by Eric Wong, J. Zico Kolter.",Other,Other,,
2656,"(see also ""Scaling provable adversarial defenses"", NIPS 2018 by the same authors)",Other,Other,,
2657,"[4] ""Stochastic gradient hamiltonian monte carlo."" ICML 2014. by Tianqi Chen,",Other,Other,,
2658,"Emily Fox, and Carlos Guestrin.",Other,Other,,
2659,============================================,Other,Other,,
2660,"After discussions with the authors, they agree to revise the paper according to our discussions and my primary concerns of this paper have been resolved. Thus I increased my rating.",Social,Social,,
2691,"The authors study the learning dynamics of deep neural networks, which is of fundamental importance but lacks understanding.",Evaluative,Evaluative,N-Negative|P-Positive,Motivation/Impact|Clarity
2692,"The authors study several dynamics like activation independence, gradient starvation, which gives new insights.",Evaluative,Evaluative,N-Negative,Motivation/Impact
2693,"However, the assumption is too strong.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2694,There are two main results in the paper:,Structuring,Structuring.Heading,,
2695,"1) Through learning, the neurons activates of one class.",Structuring,Structuring.Summary,,
2696,"2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape.",Structuring,Structuring.Summary,,
2697,"However, there are two strong assumptions: 1. the two data are perfectly separable by linear classifier.",Structuring,Structuring.Summary,,
2698,"2.  H2 assumes ""at the beginning of training data points from different classes do not activate the same neurons"".",Structuring,Structuring.Summary,,
2699,"This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.",Evaluative,Evaluative,N-Negative,Substance
2700,It sounds to me this assumption implicitly suggests that the algorithm is already ALMOST CONVERGENT.,Fact,Fact,,
2701,"If this assumption cannot be weakened, I don't think the paper can be accepted.",Social,Social,,
2740,"In their abstract, the authors claim to provide state-of-the-art perplexity on Penn Treebank, which is not true.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2741,"As the authors state, their notion of ""state-of-the-art"" excludes exactly that earlier work, which does provide state-of-the-art perplexity on Penn Treebank (Yang et al. 2017), as stated in Sec. 4.1.",Fact,Fact,,
2742,"The question is, why one would exlude the mixture-of-softmax approach here?",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2743,This is clearly misleading.,Evaluative,Evaluative,N-Negative,Soundness/Correctness
2744,The authors introduce the idea of past decoding for the purpose of regularization.,Fact,Fact,,
2745,"It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2746,The results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on Penn Treebank.,Fact,Fact,,
2747,"Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2748,"The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.",Evaluative,Evaluative,N-Negative,Substance
2749,"Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.",Evaluative,Evaluative,N-Negative,Substance
2750,"It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.",Evaluative,Evaluative,N-Negative,Soundness/Correctness
2751,"The authors make use of the theory of functional gradient, based on optimal transport, to develop a method that can promote the entropy of the generator distribution without directly estimating the entropy itself.",Structuring,Structuring.Summary,,
2752,Theoretical results are provided as well as necessary experiments to support their technique's outperformance in some data sets.,Evaluative,Evaluative,P-Positive,Substance
2753,"I found that this is an interesting paper, both original ideal and numerical results.",Evaluative,Evaluative,P-Positive,Substance|Originality
2762,"This paper proposes LEMONADE, a random search procedure for neural network architectures (specifically neural networks, not general hyperparameter optimization) that handles multiple objectives.",Structuring,Structuring.Summary,,
2763,"Notably, this method is significantly more efficient more efficient than previous works on neural architecture search.",Evaluative,Evaluative,P-Positive,Soundness/Correctness
2764,The emphasis in this paper is very strange.,Evaluative,Evaluative,N-Negative,Other
2765,"It devotes a lot of space to things that are not important, while glossing over the details of its own core contribution.",,,,
2766,"For example, Section 3 spends nearly a full page building up to a definition of an epsilon-approximate network morphism, but this definition is never used.",,,,
2767,I don't feel like my understanding of the paper would have suffered if all Section 3 had been replaced by its final paragraph.,,,,
2768,Meanwhile the actual method used in the paper is hidden in Appendices A.1.1-A.2.,Evaluative,Evaluative,N-Negative,Other
2769,"Some of the experiments (eg. comparisons involving ShakeShake and ScheduledDropPath, Section 5.2) could also be moved to the appendix in order to make room for a description of LEMONADE in the main paper.",Evaluative,Evaluative,N-Negative,Other
2770,"That said, those complaints are just about presentation and not about the method, which seems quite good once you take the time to dig it out of the appendix.",Evaluative,Evaluative,P-Positive,Substance
2771,I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?,Evaluative,Evaluative,N-Negative,Clarity
2772,Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?,Request,Request.Clarification,N-Negative,Meaningful Comparison
2773,"It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.",Evaluative,Evaluative,N-Negative,Substance
2774,How could scaling be handled?,Request,Request.Explanation,U-Neutral,Substance
2778,Following the suggested rubric:,Structuring,Structuring.Summary,,
2779,1. Briefly establish your personal expertise in the field of the paper.,Other,Other,,
2780,2. Concisely summarize the contributions of the paper.,Other,Other,,
2781,3. Evaluate the quality and composition of the work.,Other,Other,,
2782,"4. Place the work in context of prior work, and evaluate this work's novelty.",Other,Other,,
2783,5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.,Other,Other,,
2784,6. Provide a summary judgment if the work is significant and of interest to the community.,Other,Other,,
2785,1.  I work at the intersection of machine learning and biological vision,Other,Other,,
2786,and have worked on modeling word representations.,Other,Other,,
2787,2. This paper develops a new representation system for object,Structuring,Structuring.Summary,,
2788,representations from training on data collected from odd-one-out human,,,,
2789,judgements of images.,,,,
2790,The vector representation for objects is,Structuring,Structuring.Summary,,
2791,designed to be sparse and low dimensional (and ends up being about,,,,
2792,49D),,,,
2794,Similarity is measured by dot products in the space and,Structuring,Structuring.Summary,,
2795,probabilities of which pair of items will be paired are modeled as the,,,,
2796,exponential of the similarity.,,,,
2797,"3,5",Structuring,Structuring.Heading,,
2798,The resulting embedding	does a good job	of predicting human similarity,Evaluative,Evaluative,P-Positive,Soundness/Correctness
2799,judgements,,,,
2800,and,,,,
2801,seems to cover similar features to those named by,,,,
2802,humans.,,,,
2803,They also explain typicality judgements and cluster semantic,Evaluative,Evaluative,P-Positive,Soundness/Correctness
2804,categories well.,,,,
2805,The creation of the upper limit based on noise between,Evaluative,Evaluative,P-Positive,Soundness/Correctness
2806,and within subjects was a nice addition.,,,,
2807,4. Some relevant related work is discussed and this seems like a novel,Evaluative,Evaluative,P-Positive,Originality
2808,and interesting contribution.,,,,
2809,The authors might also want to compare,Request,Request.Experiment,U-Neutral,Substance
2810,to similar work that looked at similarities among triplets (Similarity,,,,
2811,Comparisons for Interactive Fine-Grained Categorization,,,,
2812,http://ttic.uchicago.edu/~smaji/papers/similarity-cvpr14.pdf;,,,,
2813,Conditional Similarity Networks https://arxiv.org/abs/1603.07810 ).,,,,
2814,"6. While this paper is not especially surprising or ground breaking, the",Evaluative,Evaluative,P-Positive,Substance
2815,number and quality of the comparisons make it a worthwhile,,,,
2816,contribution and the resulting embeddings are worth further exploration,,,,
2817,and could be very useful for future research.,,,,
2832,This paper develops a mean field theory for batch normalization (BN) in fully-connected networks with randomly initialized weights.,Structuring,Structuring.Summary,,
2833,There are a number of interesting predictions made in this paper on the basis of this analysis.,Structuring,Structuring.Summary,,
2834,The main technical results of the paper are Theorems 5-8 which compute the statistics of the covariance of the activations and the gradients.,Structuring,Structuring.Summary,,
2836,1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?,Request,Request.Explanation,N-Negative,Soundness/Correctness
2837,"2. In a similar vein, there a number of highly technical results in the paper and it would be great if the authors provide an intuitive explanation of their theorems.",Request,Request.Clarification,U-Neutral,Clarity
2838,3. Can the statistics of activations be controlled using activation functions or operations which break the symmetry?,Request,Request.Explanation,U-Neutral,Clarity
2839,"For instance, are BSB1 fixed points good for training neural networks?",Request,Request.Explanation,U-Neutral,Clarity
2840,"4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.",Request,Request.Experiment,N-Negative,Substance
2841,"For instance, when the authors observe that the structure of the fixed point is such that activations are of identical norm equally spread apart in terms of angle, this is quite far from practice.",Fact,Fact,,
2842,It would be good to mention this in the introduction or the conclusions.,Request,Request.Edit,U-Neutral,Soundness/Correctness
